{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EIP4_Phase1_Assignment_4.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bharathbolla/EIP4/blob/assignment_4/EIP4_Phase1_Assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqW3FC9jBTyE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "58715392-d50c-4d2f-e125-0efbd34dddd5"
      },
      "source": [
        "from keras import backend as K\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "% matplotlib inline\n",
        "np.random.seed(2017) \n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, SeparableConv2D\n",
        "from keras.layers import Activation, Flatten, Dense, Dropout\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PBo82ipB1sf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1cfd76f7-5eee-4f70-a6a0-315061e3d806"
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "(train_features, train_labels), (test_features, test_labels) = cifar10.load_data()\n",
        "num_train, img_channels, img_rows, img_cols =  train_features.shape\n",
        "num_test, _, _, _ =  test_features.shape\n",
        "num_classes = len(np.unique(train_labels))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 14s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkcoQkuyB_ie",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8c7d966e-09bb-4c46-8a65-0598eb02610d"
      },
      "source": [
        "import keras\n",
        "#print(tf.__version__)\n",
        "print(keras.__version__)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrHURqn0CX6N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "7d32a66d-bd0e-41a4-b679-f64d308b4d51"
      },
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7410 sha256=3916b8adecdafc0b1fea50ce007726b1d796a54af162ca54fa5d87df6029f5eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd5Y5ocUC-5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBtuJBvZDIdd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "a1718dbb-b5bd-4d39-ab1d-e2d7aaf12023"
      },
      "source": [
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "\n",
        "printm()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 12.5 GB  | Proc size: 616.0 MB\n",
            "GPU RAM Free: 7611MB | Used: 0MB | Util   0% | Total 7611MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAC9dg-hQAma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHulD68ADJme",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "d87fe41c-4692-41ad-8358-caf7bb1b7888"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Dec  8 10:28:32 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8     7W /  75W |      0MiB /  7611MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSove7mhDamu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "# Subtracting pixel mean improves accuracy\n",
        "subtract_pixel_mean = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Xv8h3A0En5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training parameters\n",
        "batch_size = 128  # orig paper trained all networks with batch_size=128\n",
        "epochs = 200\n",
        "data_augmentation = True\n",
        "num_classes = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvpjtRkpQCgo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_model_history(model_history):\n",
        "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
        "    # summarize history for accuracy\n",
        "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
        "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
        "    axs[0].legend(['train', 'val'], loc='best')\n",
        "    # summarize history for loss\n",
        "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
        "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
        "    axs[1].legend(['train', 'val'], loc='best')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQmN1D5qQGvy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(test_x, test_y, model):\n",
        "    result = model.predict(test_x)\n",
        "    predicted_class = np.argmax(result, axis=1)\n",
        "    true_class = np.argmax(test_y, axis=1)\n",
        "    num_correct = np.sum(predicted_class == true_class) \n",
        "    accuracy = float(num_correct)/result.shape[0]\n",
        "    return (accuracy * 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxQiVnXOF7sv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "19f154b7-b69c-4e1d-bc5b-cd1bcd101ee0"
      },
      "source": [
        "n = 3\n",
        "\n",
        "# Model version\n",
        "# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n",
        "version = 1\n",
        "\n",
        "# Computed depth from supplied model parameter n\n",
        "if version == 1:\n",
        "    depth = n * 6 + 2\n",
        "elif version == 2:\n",
        "    depth = n * 9 + 2\n",
        "\n",
        "# Model name, depth and version\n",
        "model_type = 'ResNet%dv%d' % (depth, version)\n",
        "\n",
        "# Load the CIFAR10 data.\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Input image dimensions.\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# Normalize data.\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# If subtract pixel mean is enabled\n",
        "if subtract_pixel_mean:\n",
        "    x_train_mean = np.mean(x_train, axis=0)\n",
        "    x_train -= x_train_mean\n",
        "    x_test -= x_train_mean\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "print('y_train shape:', y_train.shape)\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "y_train shape: (50000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lax4NLr-F-lO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaBnGmadGipP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_v1(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 1 Model builder [a]\n",
        "\n",
        "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "    Last ReLU is after the shortcut connection.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filters is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same number of filters.\n",
        "    Features maps sizes:\n",
        "    stage 0: 32x32, 16\n",
        "    stage 1: 16x16, 32\n",
        "    stage 2:  8x8,  64\n",
        "    The Number of parameters is approx the same as Table 6 of [a]:\n",
        "    ResNet20 0.27M\n",
        "    ResNet32 0.46M\n",
        "    ResNet44 0.66M\n",
        "    ResNet56 0.85M\n",
        "    ResNet110 1.7M\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # Start model definition.\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v1 does not use BN after last shortcut connection-ReLU\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUvNRdKjGoeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_v2(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 2 Model builder [b]\n",
        "\n",
        "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
        "    bottleneck layer\n",
        "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
        "    Second and onwards shortcut connection is identity.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filter maps is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same filter map sizes.\n",
        "    Features maps sizes:\n",
        "    conv1  : 32x32,  16\n",
        "    stage 0: 32x32,  64\n",
        "    stage 1: 16x16, 128\n",
        "    stage 2:  8x8,  256\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
        "    # Start model definition.\n",
        "    num_filters_in = 16\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "\n",
        "    # Instantiate the stack of residual units\n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                if res_block == 0:  # first layer and first stage\n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "                if res_block == 0:  # first layer but not first stage\n",
        "                    strides = 2    # downsample\n",
        "\n",
        "            # bottleneck residual unit\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if res_block == 0:\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzhjc_2uGz7f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2f232604-8990-448e-caa2-ffbb76772248"
      },
      "source": [
        "\n",
        "def lr_schedule(epoch, lr):\n",
        "  return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "if version == 2:\n",
        "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
        "else:\n",
        "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "print(model_type)\n",
        "\n",
        "# Prepare model model saving directory.\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(zoom_range=0.0, \n",
        "                             horizontal_flip=False)\n",
        "\n",
        "\n",
        "# train the model\n",
        "start = time.time()\n",
        "# Train the model\n",
        "model_info = model.fit_generator(datagen.flow(x_train, y_train, batch_size = 128),\n",
        "                                 samples_per_epoch = x_train.shape[0], nb_epoch = 50, \n",
        "                                 callbacks=[LearningRateScheduler(lr_schedule, verbose=1)], #Added Rohan's Learning rate scheduler\n",
        "                                 validation_data = (x_test, y_test), verbose=1)\n",
        "\n",
        "                                            \n",
        "end = time.time()\n",
        "print (\"Model took %0.2f seconds to train\"%(end - start))\n",
        "# plot model history\n",
        "plot_model_history(model_info)\n",
        "# compute test accuracy\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 32, 32, 16)   448         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 32, 32, 16)   64          conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 32, 32, 16)   0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 32, 32, 16)   2320        activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 32, 32, 16)   64          conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 32, 32, 16)   0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 32, 32, 16)   2320        activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 32, 32, 16)   64          conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, 32, 32, 16)   0           activation_39[0][0]              \n",
            "                                                                 batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 32, 32, 16)   0           add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 32, 32, 16)   2320        activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 32, 32, 16)   64          conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 32, 32, 16)   0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 32, 32, 16)   2320        activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 32, 32, 16)   64          conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, 32, 32, 16)   0           activation_41[0][0]              \n",
            "                                                                 batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 32, 32, 16)   0           add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 32, 32, 16)   2320        activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 32, 32, 16)   64          conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 32, 32, 16)   0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 32, 32, 16)   2320        activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 32, 32, 16)   64          conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, 32, 32, 16)   0           activation_43[0][0]              \n",
            "                                                                 batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 32, 32, 16)   0           add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 16, 16, 32)   4640        activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 16, 16, 32)   128         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 16, 16, 32)   0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 16, 16, 32)   9248        activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 16, 16, 32)   544         activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 16, 16, 32)   128         conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, 16, 16, 32)   0           conv2d_52[0][0]                  \n",
            "                                                                 batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 16, 16, 32)   0           add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 16, 16, 32)   9248        activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 16, 16, 32)   128         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 16, 16, 32)   0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 16, 16, 32)   9248        activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 16, 16, 32)   128         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_23 (Add)                    (None, 16, 16, 32)   0           activation_47[0][0]              \n",
            "                                                                 batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 16, 16, 32)   0           add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 16, 16, 32)   9248        activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 16, 16, 32)   128         conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 16, 16, 32)   0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 16, 16, 32)   9248        activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 16, 16, 32)   128         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_24 (Add)                    (None, 16, 16, 32)   0           activation_49[0][0]              \n",
            "                                                                 batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 16, 16, 32)   0           add_24[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 8, 8, 64)     18496       activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 8, 8, 64)     256         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 8, 8, 64)     0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 8, 8, 64)     36928       activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 8, 8, 64)     2112        activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 8, 8, 64)     256         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_25 (Add)                    (None, 8, 8, 64)     0           conv2d_59[0][0]                  \n",
            "                                                                 batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 8, 8, 64)     0           add_25[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 8, 8, 64)     36928       activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 8, 8, 64)     256         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 8, 8, 64)     0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 8, 8, 64)     36928       activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 8, 8, 64)     256         conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_26 (Add)                    (None, 8, 8, 64)     0           activation_53[0][0]              \n",
            "                                                                 batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 8, 8, 64)     0           add_26[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 8, 8, 64)     36928       activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 8, 8, 64)     256         conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 8, 8, 64)     0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 8, 8, 64)     36928       activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 8, 8, 64)     256         conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_27 (Add)                    (None, 8, 8, 64)     0           activation_55[0][0]              \n",
            "                                                                 batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 8, 8, 64)     0           add_27[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 1, 1, 64)     0           activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 64)           0           average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 10)           650         flatten_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 274,442\n",
            "Trainable params: 273,066\n",
            "Non-trainable params: 1,376\n",
            "__________________________________________________________________________________________________\n",
            "ResNet20v1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., callbacks=[<keras.ca..., validation_data=(array([[[..., verbose=1, steps_per_epoch=390, epochs=50)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "390/390 [==============================] - 28s 73ms/step - loss: 1.5552 - acc: 0.5004 - val_loss: 2.0566 - val_acc: 0.4202\n",
            "Epoch 2/50\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 1.0694 - acc: 0.6758 - val_loss: 1.9904 - val_acc: 0.4951\n",
            "Epoch 3/50\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "390/390 [==============================] - 23s 58ms/step - loss: 0.8825 - acc: 0.7455 - val_loss: 1.1574 - val_acc: 0.6599\n",
            "Epoch 4/50\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "390/390 [==============================] - 23s 58ms/step - loss: 0.7663 - acc: 0.7864 - val_loss: 1.0114 - val_acc: 0.7136\n",
            "Epoch 5/50\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.6754 - acc: 0.8200 - val_loss: 0.9431 - val_acc: 0.7378\n",
            "Epoch 6/50\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "390/390 [==============================] - 23s 58ms/step - loss: 0.5986 - acc: 0.8467 - val_loss: 0.9216 - val_acc: 0.7416\n",
            "Epoch 7/50\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5347 - acc: 0.8695 - val_loss: 0.8473 - val_acc: 0.7752\n",
            "Epoch 8/50\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.4732 - acc: 0.8905 - val_loss: 0.9301 - val_acc: 0.7534\n",
            "Epoch 9/50\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.4136 - acc: 0.9116 - val_loss: 1.0326 - val_acc: 0.7512\n",
            "Epoch 10/50\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "390/390 [==============================] - 22s 58ms/step - loss: 0.3652 - acc: 0.9289 - val_loss: 0.9636 - val_acc: 0.7716\n",
            "Epoch 11/50\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.3111 - acc: 0.9491 - val_loss: 1.0102 - val_acc: 0.7739\n",
            "Epoch 12/50\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.2754 - acc: 0.9600 - val_loss: 1.1181 - val_acc: 0.7602\n",
            "Epoch 13/50\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.2488 - acc: 0.9692 - val_loss: 1.1143 - val_acc: 0.7617\n",
            "Epoch 14/50\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.2233 - acc: 0.9775 - val_loss: 1.1574 - val_acc: 0.7656\n",
            "Epoch 15/50\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.2078 - acc: 0.9816 - val_loss: 1.0330 - val_acc: 0.7866\n",
            "Epoch 16/50\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.1989 - acc: 0.9834 - val_loss: 1.2515 - val_acc: 0.7627\n",
            "Epoch 17/50\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.1870 - acc: 0.9876 - val_loss: 1.1966 - val_acc: 0.7745\n",
            "Epoch 18/50\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.1772 - acc: 0.9900 - val_loss: 1.2579 - val_acc: 0.7738\n",
            "Epoch 19/50\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.1696 - acc: 0.9916 - val_loss: 1.3058 - val_acc: 0.7690\n",
            "Epoch 20/50\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "390/390 [==============================] - 22s 58ms/step - loss: 0.1694 - acc: 0.9910 - val_loss: 1.1596 - val_acc: 0.7911\n",
            "Epoch 21/50\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0004065041.\n",
            "390/390 [==============================] - 22s 58ms/step - loss: 0.1602 - acc: 0.9929 - val_loss: 1.2487 - val_acc: 0.7795\n",
            "Epoch 22/50\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.000389661.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.1560 - acc: 0.9938 - val_loss: 1.2261 - val_acc: 0.7903\n",
            "Epoch 23/50\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0003741581.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.1563 - acc: 0.9929 - val_loss: 1.2778 - val_acc: 0.7859\n",
            "Epoch 24/50\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0003598417.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.1516 - acc: 0.9940 - val_loss: 1.4137 - val_acc: 0.7754\n",
            "Epoch 25/50\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0003465804.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.1419 - acc: 0.9964 - val_loss: 1.3098 - val_acc: 0.7844\n",
            "Epoch 26/50\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0003342618.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.1459 - acc: 0.9938 - val_loss: 1.4408 - val_acc: 0.7623\n",
            "Epoch 27/50\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0003227889.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.1450 - acc: 0.9940 - val_loss: 1.2938 - val_acc: 0.7892\n",
            "Epoch 28/50\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0003120774.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.1354 - acc: 0.9971 - val_loss: 1.3338 - val_acc: 0.7856\n",
            "Epoch 29/50\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.000302054.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.1351 - acc: 0.9961 - val_loss: 1.4462 - val_acc: 0.7673\n",
            "Epoch 30/50\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0002926544.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.1341 - acc: 0.9956 - val_loss: 1.2816 - val_acc: 0.7897\n",
            "Epoch 31/50\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0002838221.\n",
            "390/390 [==============================] - 22s 58ms/step - loss: 0.1349 - acc: 0.9947 - val_loss: 1.2622 - val_acc: 0.7916\n",
            "Epoch 32/50\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0002755074.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.1289 - acc: 0.9967 - val_loss: 1.2816 - val_acc: 0.8022\n",
            "Epoch 33/50\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.000267666.\n",
            "390/390 [==============================] - 22s 58ms/step - loss: 0.1259 - acc: 0.9972 - val_loss: 1.3611 - val_acc: 0.7871\n",
            "Epoch 34/50\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0002602585.\n",
            "390/390 [==============================] - 22s 58ms/step - loss: 0.1253 - acc: 0.9969 - val_loss: 1.3866 - val_acc: 0.7903\n",
            "Epoch 35/50\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.00025325.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.1248 - acc: 0.9967 - val_loss: 1.3213 - val_acc: 0.7938\n",
            "Epoch 36/50\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0002466091.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.1235 - acc: 0.9964 - val_loss: 1.3283 - val_acc: 0.7886\n",
            "Epoch 37/50\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0002403076.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.1153 - acc: 0.9991 - val_loss: 1.3141 - val_acc: 0.7942\n",
            "Epoch 38/50\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0002343201.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.1130 - acc: 0.9992 - val_loss: 1.4095 - val_acc: 0.7860\n",
            "Epoch 39/50\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0002286237.\n",
            "390/390 [==============================] - 22s 58ms/step - loss: 0.1176 - acc: 0.9969 - val_loss: 1.5035 - val_acc: 0.7782\n",
            "Epoch 40/50\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0002231977.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.1226 - acc: 0.9946 - val_loss: 1.4729 - val_acc: 0.7776\n",
            "Epoch 41/50\n",
            "\n",
            "Epoch 00041: LearningRateScheduler setting learning rate to 0.0002180233.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.1157 - acc: 0.9971 - val_loss: 1.3819 - val_acc: 0.7890\n",
            "Epoch 42/50\n",
            "\n",
            "Epoch 00042: LearningRateScheduler setting learning rate to 0.0002130833.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.1096 - acc: 0.9988 - val_loss: 1.4492 - val_acc: 0.7852\n",
            "Epoch 43/50\n",
            "\n",
            "Epoch 00043: LearningRateScheduler setting learning rate to 0.0002083623.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.1081 - acc: 0.9988 - val_loss: 1.3245 - val_acc: 0.7960\n",
            "Epoch 44/50\n",
            "\n",
            "Epoch 00044: LearningRateScheduler setting learning rate to 0.0002038459.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.1057 - acc: 0.9991 - val_loss: 1.4102 - val_acc: 0.7903\n",
            "Epoch 45/50\n",
            "\n",
            "Epoch 00045: LearningRateScheduler setting learning rate to 0.0001995211.\n",
            "390/390 [==============================] - 22s 58ms/step - loss: 0.1076 - acc: 0.9978 - val_loss: 1.6202 - val_acc: 0.7627\n",
            "Epoch 46/50\n",
            "\n",
            "Epoch 00046: LearningRateScheduler setting learning rate to 0.0001953761.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.1114 - acc: 0.9965 - val_loss: 1.7300 - val_acc: 0.7564\n",
            "Epoch 47/50\n",
            "\n",
            "Epoch 00047: LearningRateScheduler setting learning rate to 0.0001913998.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.1079 - acc: 0.9970 - val_loss: 1.3602 - val_acc: 0.7912\n",
            "Epoch 48/50\n",
            "\n",
            "Epoch 00048: LearningRateScheduler setting learning rate to 0.0001875821.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.1022 - acc: 0.9991 - val_loss: 1.3654 - val_acc: 0.7974\n",
            "Epoch 49/50\n",
            "\n",
            "Epoch 00049: LearningRateScheduler setting learning rate to 0.0001839137.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.1039 - acc: 0.9979 - val_loss: 1.4711 - val_acc: 0.7866\n",
            "Epoch 50/50\n",
            "\n",
            "Epoch 00050: LearningRateScheduler setting learning rate to 0.000180386.\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.1015 - acc: 0.9985 - val_loss: 1.3943 - val_acc: 0.7937\n",
            "Model took 1124.85 seconds to train\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3gAAAFNCAYAAABSRs15AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzdd3iUVfbA8e+dtJmENBJaCB2ULk2w\nix1EBRUVxN792XYtq7ura1ldXVfdXddesCLIYl3FLsUCKEgvIoQWakhI7zP398eZQAgpk2QmM0nO\n53nmmcnMOzNnQpj3Pe8991xjrUUppZRSSimlVPPnCHYASimllFJKKaX8QxM8pZRSSimllGohNMFT\nSimllFJKqRZCEzyllFJKKaWUaiE0wVNKKaWUUkqpFkITPKWUUkoppZRqITTBU6qRjDHdjTHWGBPu\nw7ZXGGO+b4q4lFJKqeZK961KNZwmeKpVMcZsNsaUGmOSq9y/1Lsj6R6cyA6KpY0xJt8Y81mwY1FK\nKaXqEsr71vokikq1FJrgqdZoEzC54gdjzCAgOnjhHOJ8oAQ4zRjTsSnfWHeASimlGijU961KtRqa\n4KnW6C3gsko/Xw68WXkDY0y8MeZNY0yGMWaLMeZeY4zD+1iYMeYJY8xeY0waMK6a575qjNlpjNlu\njHnYGBNWj/guB14AVgCXVHntLsaY971xZRpjnqn02LXGmLXGmDxjzBpjzDDv/dYY07vSdq8bYx72\n3h5tjEk3xtxtjNkFvGaMSTTGfOJ9j33e26mVnt/WGPOaMWaH9/EPvfevMsacXWm7CO/vaGg9PrtS\nSqnmKdT3rYcwxkQZY/7l3Z/t8N6O8j6W7N3/ZRtjsowx31WK9W5vDHnGmF+NMac0Jg6l/E0TPNUa\nLQTijDH9vDuHScDbVbb5DxAP9ARORHZaV3ofuxY4CxgKjAAmVnnu60A50Nu7zenANb4EZozpBowG\npnkvl1V6LAz4BNgCdAc6AzO8j10APODdPg44B8j05T2BjkBboBtwHfK98Jr3565AEfBMpe3fQs7K\nDgDaA//03v8mByekZwI7rbVLfYxDKaVU8xWy+9Za/Bk4ChgCHAGMBO71PnYHkA60AzoAfwKsMeZw\n4GbgSGttLHAGsLmRcSjlV5rgqdaq4kzjacBaYHvFA5V2TH+01uZZazcDTwKXeje5EPiXtXabtTYL\neLTSczsgic3vrLUF1to9SAI0yce4LgVWWGvXIMnbgEojYCOBFOAu72sXW2srJpVfAzxurf3Zig3W\n2i0+vqcHuN9aW2KtLbLWZlpr37PWFlpr84BHkB0xxphOwFjgBmvtPmttmbV2nvd13gbONMbEVfos\nb/kYg1JKqeYvVPetNZkCPGSt3WOtzQAerBRPGdAJ6Obd131nrbWAG4gC+htjIqy1m621GxsZh1J+\npfNtVGv1FjAf6EGVEhIgGYhARsoqbEFGzECSrG1VHqvQzfvcncaYivscVbavzWXAywDW2u3GmHlI\nmctSoAuwxVpbXs3zugAN3cFkWGuLK34wxkQjO84xQKL37ljvzrkLkGWt3Vf1Ray1O4wxPwDnG2M+\nQBLB2xoYk1JKqeYnVPetNUmpJp4U7+1/IJUxX3rf8yVr7WPW2g3GmN95HxtgjPkCuN1au6ORsSjl\nNzqCp1ol7+jWJuSM4PtVHt6LnLnrVum+rhw4E7kTSXQqP1ZhG9IgJdlam+C9xFlrB9QVkzHmGKAP\n8EdjzC7vnLhRwMXe5ifbgK41NELZBvSq4aULOXiie9XGLbbKz3cAhwOjrLVxwAkVIXrfp60xJqGG\n93oDKdO8AFhgrd1ew3ZKKaVamFDct9ZhRzXx7PB+ljxr7R3W2p7ItIfbK+baWWvfsdYe532uBf7e\nyDiU8itN8FRrdjVwsrW2oPKd1lo3MBN4xBgT650XdzsH5hLMBG41xqQaYxKBeyo9dyfwJfCkMSbO\nGOMwxvQyxpzoQzyXA18B/ZH5AEOAgYALGQ37CdkBPmaMiTHGOI0xx3qf+wpwpzFmuBG9vXEDLEOS\nxDBjzBi85Za1iEXm3WUbY9oC91f5fJ8Bz3mbsUQYY06o9NwPgWHIyF3Vs7dKKaVavlDbt1aI8u43\nKy4OYDpwrzGmnZElHv5SEY8x5izvvtQAOUhppscYc7gx5mRvM5ZiZH/pqefvSKmA0gRPtVrW2o3W\n2sU1PHwLUACkAd8D7wBTvY+9DHwBLAd+4dCzlJcBkcAaYB8wC6njr5ExxonMP/iPtXZXpcsmpOTl\ncu/O8WxkgvlWZPL3Rd7P8l9krtw7QB6SaLX1vvxt3udlI/MNPqwtFuBfSFK5F5k0/3mVxy9FzsKu\nA/YAv6t4wFpbBLyHlOdU/b0opZRq4UJp31pFPpKMVVxOBh4GFiNdq1d63/dh7/Z9gK+9z1sAPGet\nnYPMv3sM2UfuQpqN/bEecSgVcEbmiyqllH8YY/4CHGatvaTOjZVSSimllF9pkxWllN94Szqv5kAX\nMqWUUkop1YS0RFMp5RfGmGuRifCfWWvnBzsepZRSSqnWSEs0lVJKKaWUUqqF0BE8pZRSSimllGoh\nNMFTSimllFJKqRai2TVZSU5Ott27dw92GEoppZrAkiVL9lpr2wU7juZC95FKKdU61LZ/bHYJXvfu\n3Vm8uKblVZRSSrUkxpgtwY6hOdF9pFJKtQ617R+1RFMppZRSSimlWghN8JRSSimllFKqhdAETyml\nlFJKKaVaiGY3B686ZWVlpKenU1xcHOxQAsrpdJKamkpERESwQ1FKKaWUUipo9Pi/Zi0iwUtPTyc2\nNpbu3btjjAl2OAFhrSUzM5P09HR69OgR7HCUUkoppZQKGj3+r1mLKNEsLi4mKSmpxf7jAhhjSEpK\navFnKZRSSimllKqLHv/XLGAJnjFmqjFmjzFmVQ2PG2PM08aYDcaYFcaYYY18v8Y8vVloDZ9RKaWU\nUkopX7SGY+OGfMZAjuC9Doyp5fGxQB/v5Trg+QDGElDZ2dk899xz9X7emWeeSXZ2dgAiUkoppZRS\nSgVKKB//ByzBs9bOB7Jq2WQ88KYVC4EEY0ynQMUTSDX9A5eXl9f6vNmzZ5OQkBCosJRSSimllFIB\nEMrH/8FsstIZ2Fbp53TvfTuDE07D3XPPPWzcuJEhQ4YQERGB0+kkMTGRdevWsX79eiZMmMC2bdso\nLi7mtttu47rrrgOge/fuLF68mPz8fMaOHctxxx3Hjz/+SOfOnfnoo49wuVxB/mRKNX85hWVsyMhj\na1YhZW4LVu63WKz3R4eBjvEuurWNpnOii4iwwJz7KnN72JNXwu7cYsrKPQc9VrkEw+2xlLk93ovc\nLvd4KCu3uCLDaBsTSWJ0JElt5Doy3L/x5hWXkb6viG1ZhRSVuYl3RZAQHUmCK4KE6AjinBE4HAeX\njFhrKSn3UFzmprjMQ0m5m25JMX6NSzWB9V9AdDKkDg92JEopFdJC+fi/WXTRNMZch5Rx0rVr1yBH\nc6jHHnuMVatWsWzZMubOncu4ceNYtWrV/m43U6dOpW3bthQVFXHkkUdy/vnnk5SUdNBr/Pbbb0yf\nPp2XX36ZCy+8kPfee49LLrkkGB9HBVlhaTnbsoqIc4XTMc7Z6PryfQWlrNmZy5oduRSWuolzhRPn\njCDOFUGss+J2ONGR4USGO4gMcxARZhr8voWl5ezILiJ9XxFZBaWUlHsoKXNTUu6htNwjP5e7KfdY\nwh2G8DAHEd7r8DBDhEOuwxwGh5HrMGNwOAxhDghzOAh3yP0Hrh2EOQzlHg9pGQVs2JMvl4x8MvJK\n6hW/w0BKgouubaPplhRN17YxdE+KpltSDN2SoomJqvlrs7C0nM17C0nbm8+mjAJ25BSzJ7eYXbnF\n7M4tJrOgFGsb9GutVWxUOIkxkSTGRJIUE0nbStcV90WGO/YnXlWvs/JLJaHbV0j6viJyispqfT9j\nIN4VgTM8jOJy9/6krrJwh2HD3870/4dVgVNeCp/fAyX5cO23kNAl2BEppVTICuXj/2AmeNuBynuP\nVO99h7DWvgS8BDBixIhaD48e/N9q1uzI9VeMAPRPieP+swf4vP3IkSMPamX69NNP88EHHwCwbds2\nfvvtt0P+gXv06MGQIUMAGD58OJs3b2584CqgPB5Lfmk5hSVunBEOYqLC6xz5cXssecVlZBeWsa9Q\nDqq3ZBawObOQLZkFbMksZE+lhCQ6MoweyTH0ateGnu1i6NmuDT2TY4h1huOx4LEWay1uj9x2eyzp\n+wpZsyOXNTtzWb0jl5059e+8agxEhjmIDHcQFe7AGRFGm6hwuTjDD9yOCqfcY9mRXcT27CJ2ZBex\nr7D25MBhwBkRRpgxlHusjEy5/Zv1xEaF07tDG0Yf1o7e7dvQu30buifHEBnmwJgDo2XG+1nL3fIZ\ntmYVHnT5cvVuMgtKD3rtdrFR9PAme50TXezNL2HT3gLSMgoO+V0nxUTSPs5Jx7goBqfG0yHO6b1E\n4QwP279d5U9vLYQ5DJHhkrhGeBPuCG8CXFTqJqugVC6FpWTle6+99+3OLWbtzlwyC0oprTJKWBNn\nhIPUxGhSE10M65pIaqKL1MRourR1ERMVTnZhGTlFpd6/2zJyCkvJLiqjuMyNMyKs0sWBM/zAbWtt\nq5gA32KER8Kkd+DV02H6JLjqc4iKDXZUSilVJz3+P1gwE7yPgZuNMTOAUUCOtbbZlWdWJybmQFnS\n3Llz+frrr1mwYAHR0dGMHj262lanUVFR+2+HhYVRVFTUJLG2Ftba/UlVRJgkLFHeg9HKo1WFpeXs\nzClmZ3YxO3KK2JVTzM6cInbnlpBTVEZecRl5xeXkFZeTX3JojbUzwkGbqAjinJIIRUeGUVTqJrtI\nkrrc4rJqR3A6xEXRLSmGEw9rR/fkGFITXeQWlbExo4C0vQX8snUf/1uxw+fRH4eBXu3aMKpHW/qn\nxDEgJZ5+neKIc4aTX1JOblE5ucVl5BZJTDlFZRSWuin1jrKVug+MtpW6PRSVuskvKaegpJysglK2\nZhWS7/0dOIwhJcFJ5wQXQ7okkJLgIjXRRUqCi+Q2UTgjHESFhxHlTRbDq0mCrTc5LfeWJpa7LW5r\n8Xjk2u2xeDx4b3twe6DcI9uVeyqe68FhDD2SY2gfG1XvxKJL22hG9Uw65P78knJJwvcWsjmzYH9C\nPm99BnvySoh1htOzXRuO7plEj2RJwnskx9AjOQZXZFg179Q0rLUUepPBzIJSytweb/Il/x77/10i\n5N9FEzEFQPt+cMFrMO0CeO9amDQNHMH7O1ZKqeYilI7/A5bgGWOmA6OBZGNMOnA/EAFgrX0BmA2c\nCWwACoEr/fG+9cm0/SU2Npa8vLxqH8vJySExMZHo6GjWrVvHwoULmzi6ls9ay77Csv0H39uyitiT\nV8ye3BL25JWQ4b2UuqsfzXAYiAoPI8xhqk3akttE0iHOSbwrguTkGGKdUtoY65RELjoynJJy9/6k\nTxLAsv0JUUJ0JN2TY0hwRRBfaR5TQnQEnROi6do22qdEoLjMzebMAjZlFFBU5sZhDMaAwxjvRUam\nOsY76dsxFmdE9a+ZEB1JQnRk/X7JAWaMITzMEB5GjXEHS5uocAakxDMgJf6Qx0rK3d5RwdBLjowx\nxESFExMVTpe20cEORzUnvU+FsY/D7Dvhq7/AGY8EOyKllKqVHv8fLGAJnrV2ch2PW+CmQL1/U0pK\nSuLYY49l4MCBuFwuOnTosP+xMWPG8MILL9CvXz8OP/xwjjrqqCBG2vzlFJWxKC2T5enZB8oa9xaS\nVyUxS4yOoH2sk/ZxUfRsFyO3Y6NIjImgzG0Pmhcm84dkTli72ChS4l10jHeSEu+ifVxUyCQczogw\n+naMo2/HuGCHoryiwkPjb0OFJmNMF+BNoANSifuStfbfVbYxwL+RE56FwBXW2l+8j10O3Ovd9GFr\n7RtNFTsjr4W9v8GCZyC5Dwy/osneWimlmoNQPv43NhAz/gNoxIgRdvHixQfdt3btWvr16xekiJpW\na/qsIOVxP2/KYkFaJgs2ZrJ6Rw4e7xylLokuuiUd3ACjW5KUN4ZKUqaUahxjzBJr7Yhgx9EQ3qV/\nOllrfzHGxAJLgAnW2jWVtjkTuAVJ8EYB/7bWjjLGtAUWAyOQ5HAJMNxau6+296xuH9lg7nKYfhGk\nzYVL3oeeJ/rndZVSyg9a0zFxdZ+1tv1js+iiqVq2ghLvvLecov1z33ZmF/Pr7jxWbs/B7bFEhjkY\n0jWBW07uw9G9khjaNUFHT5RSIc07r3yn93aeMWYtshzQmkqb7V8TFlhojKlYE3Y08JW1NgvAGPMV\nMAaY3mQfICwcJk6FV8+AmZfCNd9Ccu8me3ullFINowmealJFpW6Wbt3Hok1ZLNqUydqdedW2ZG8X\nG0X3pGhuPLEXR/dKYljXxKA2rFBKqcYwxnQHhgKLqjxU05qwNd3ftJzxcPEMePkUeOcCuOYbiG7b\n5GEopZTynSZ4KqCKy9wsSMvkp01Z/LQpixXp2ZS5LQ4j7WfPGtyJzokuUuJddIp3kpLgokOc0+8L\nNyulVLAYY9oA7wG/s9b6t483TbBWbGJ3WT7hjbPgy3thwnP+fw+llFJ+owmeCojiMjdvL9zCC/PS\n2JtfQrjDMDg1nquP68monm0Z3i2ROGdEsMNUSqmAMsZEIMndNGvt+9VsUtOasNuRMs3K98+t7j3q\ns1Zsg3UdBd2OkcYrSimlQpomeMqvisvcvLNoK8/P20hGXgnH9EriiQsGM7JHW6Ij9c9NKdV6eDtk\nvgqstdY+VcNm1a4Ja4z5AvibMSbRu93pwB8DHnRtXImQuyOoISillKqbHnErvyguczPjp608N3cj\ne/JKOKpnW56ZPLTahaOVUqqVOBa4FFhpjFnmve9PQFeofU1Ya22WMeavwM/e5z1U0XAlaJwJUJQd\n1BCUUkrVTRO8IGjTpg35+fnBDsMvyt0e3l28jf98s4FducWM7NGWf08aytG9NLFTSrVu1trvAVPH\nNjWuCWutnQpMDUBoDeNKgKJ9YC2YWj+WUkqpKpry+F8TPNUg1lq+WbuHRz9by8aMAoZ3S+TJC4/g\nmF5JGN3xK6VUy+NMAE8ZlBVCZEywo1FKKVUDTfD84J577qFLly7cdJOchH3ggQcIDw9nzpw57Nu3\nj7KyMh5++GHGjx8f5Ej9Y2V6Do/MXsPCtCx6Jsfw4qXDOb1/B03slFKqJXMlyHVRtiZ4SqlWL5SP\n/7UXvR9cdNFFzJw5c//PM2fO5PLLL+eDDz7gl19+Yc6cOdxxxx1IJU7zlb6vkN/NWMrZz3zP+t35\nPDR+AF/8/gTOGNBRkzullGrpnN4Er1jn4SmlVCgf/7e8EbzP7oFdK/37mh0HwdjHanx46NCh7Nmz\nhx07dpCRkUFiYiIdO3bk97//PfPnz8fhcLB9+3Z2795Nx44d/RtbEygpd/PMtxt4cX4aBvi/0b24\nYXQvXeZAKaVak8ojeEopFUr0+P8gLS/BC5ILLriAWbNmsWvXLi666CKmTZtGRkYGS5YsISIigu7d\nu1NcXBzsMOttRXo2d/53Oet35zNhSAp/GNOXlARXsMNSSinV1FzeFRuK9gU3DqWUChGhevzf8hK8\nWjLtQLrooou49tpr2bt3L/PmzWPmzJm0b9+eiIgI5syZw5YtW4ISV0OVlLt5+pvfeGFeGu3aRPHa\nFUdyUt/2wQ5LKaVUsGiJplIqVOnx/0FaXoIXJAMGDCAvL4/OnTvTqVMnpkyZwtlnn82gQYMYMWIE\nffv2DXaIPqs8anfB8FTuPas/8S4tx1RKqVZNSzSVUuogoXr8rwmeH61ceaD2Nzk5mQULFlS7Xaiu\ngaejdkoppWoUGQvGoSN4SilVSSge/2uCpwAoLnMz5ZVFLNmyT0ftlFJKHcrhAGe8juAppVSI0wRP\nYa3lnvdWsGTLPv49aQjjh3QOdkhKKaVCkTNBR/CUUirE6Tp4ihfmpfHhsh3cefphmtwppZSqmStR\nu2gqpVSIazEJXnNfRNwXgfiMX6/ZzeNfrOOswZ246aTefn99pZRSLYgrQUs0lVIhQ4//q9ciEjyn\n00lmZmaL/ke21pKZmYnT6fTba67fncdtM5YyMCWef0w8AmOM315bKaVUC6QlmkqpEKHH/zVrEXPw\nUlNTSU9PJyMjI9ihBJTT6SQ1NdUvr7WvoJRr3lhMdFQ4L102HFdkmF9eVymlVAumI3hKqRChx/81\naxEJXkREBD169Ah2GM1GmdvDjdOWsCu3mBnXHUWneFewQ1JKKdUcVIzgWQta9aGUCiI9/q9ZiyjR\nVPXz4P9WszAti8fOG8SwronBDkcppVRz4UoATzmUFgQ7EqWUUjXQBK+VeWvhFt5euJXrT+jJecP8\nU+6plFKqlXB5TwpqJ02llApZmuC1Ip+s2MH9H63i5L7t+cOYvsEORymlVHPjTJBrbbSilFIhSxO8\nVuKbtbv53YxlDO+WyDMXDyXMoXMnlFJK1ZPLm+BpoxWllApZmuC1Aj9u2MuN036hX6c4Xr3iSKIj\nW0RvHaWUUk1NR/CUUirkaYLXwi3Zso9r3lxMj6QY3rxqJHHOiGCHpJRSqrnSETyllAp5muC1YKu2\n53DFaz/RPjaKt64ZSWJMZLBDUkop1ZzpCJ5SSoU8rdVroTbsyeOyqT8R54xg2rVH0T7WGeyQlFKq\nVTHGTAXOAvZYawdW8/hdwBTvj+FAP6CdtTbLGLMZyAPcQLm1dkTTRF2HqFgwYdpFUymlQpiO4LVA\nWzMLmfLKIsIchrevGUXnBF3IXCmlguB1YExND1pr/2GtHWKtHQL8EZhnrc2qtMlJ3sdDI7kDWdzc\nlaAlmkopFcI0wWthsgtLueTVRZSUe3j76lH0SI4JdkhKKdUqWWvnA1l1bigmA9MDGI7/OBO0RFMp\npUKYJngtiNtjuXXGMnblFDP1iiM5vGNssENSSilVB2NMNDLS916luy3wpTFmiTHmuuBEVgMdwVNK\nqZAW0ATPGDPGGPOrMWaDMeaeah7vZoz5xhizwhgz1xiTGsh4Wrp/frWe+eszeOCcAQzrmhjscJRS\nSvnmbOCHKuWZx1lrhwFjgZuMMSfU9GRjzHXGmMXGmMUZGRmBjlVH8JRSKsQFLMEzxoQBzyI7p/7A\nZGNM/yqbPQG8aa0dDDwEPBqoeFq6L1fv4pk5G7hoRBcmj+wS7HCUUkr5bhJVyjOttdu913uAD4CR\nNT3ZWvuStXaEtXZEu3btAhoo4B3B0yYrSqkWoqw42BH4XSBH8EYCG6y1adbaUmAGML7KNv2Bb723\n51TzuPLBxox8bp+5nMGp8Tw4fgDGmGCHpJRSygfGmHjgROCjSvfFGGNiK24DpwOrghNhNZxaoqmU\naiG2/QyPpsK2n4IdiV8FMsHrDGyr9HO6977KlgPneW+fC8QaY5ICGFOLk19Szg1vLSEy3MHzlwzH\nGREW7JCUUkoBxpjpwALgcGNMujHmamPMDcaYGyptdi7wpbW2oNJ9HYDvjTHLgZ+AT621nzdd5HVw\nJUJxDlgb7EiUUqpxvv0reMogfXGwI/GrYK+DdyfwjDHmCmA+sB1Z8+cg3gnm1wF07dq1KeMLadZa\n/jBrORsz8nn7al0OQSmlQom1drIP27yOLKdQ+b404IjAROUHrgSwbijJA2dcsKNRSqmG2fw9bJon\ntzM3BDcWPwtkgrcdqDwZLNV7337W2h14R/CMMW2A8621h9R9WGtfAl4CGDFihJ4y9HppfhqzV+7i\nT2f25ZjeycEORymlVGvgTJDr4mxN8JRSzZO1MOdv0KYjxLSDzN+CHZFfBbJE82egjzGmhzEmEplE\n/nHlDYwxycaYihj+CEwNYDwtyg8b9vL3z9cxblAnrj2+Z7DDUUop1Vq4vAmezsNTSjVXm+bBlh/g\n+Dug40DY27JG8AKW4Flry4GbgS+AtcBMa+1qY8xDxphzvJuNBn41xqxH5hw8Eqh4WpLt2UXcMn0p\nvdq14fGJg7WpilJKqaZTMYKnnTSVUs1RxehdXGcYdhkk9YK8HVCSH+zI/Cagc/CstbOB2VXu+0ul\n27OAWYGMoaUpLnPzf28vobTcw4uXDicmKtjTKJVSSrUqrkolmkop1dxs/Aa2LYJxT0GEE5L6yP1Z\nG6FT6E5/ro+ALnSu/O/B/61meXoOT154BD3btQl2OEoppVobV6Jca4mmUqq5qRi9i+8CQy+V+5K9\nCd7eljMPTxO8ZuTdn7cy/adt/N/oXpwxoGOww1FKKdUaOXUETynVTK3/ArYvgRPugvBIua9tT8BA\n5saghuZPmuA1EyvSs7nvo9Uc1zuZO04/PNjhKKWUaq0iY8ARriN4SqnmxVqY8wgkdochFx+4P8Il\nI3otqJOmJnjNQFZBKTe+/Qvt2kTx9OShhDm0qYpSSqkgMUZG8XQETynVnKz7FHatgBPvhrCIgx9L\n7q0lmqrpuD2W22YsJSOvhOcvGUbbmMhgh6SUUqq1cyVoF02lVPPh8cDcR6FtLxh04aGPJ/WWEk3b\nMpbb1gQvxD311a9899teHho/gMGpCcEORymllJIRPC3RVEo1F2s/ht2rYPQ9EFZNB/qkPlCaB/m7\nmz62ANAEL4R9uXoXz87ZyKQjuzBpZNdgh6OUUkoJl5ZoKqWaCY9bRu+SD4OB51e/TXJvuW4hZZqa\n4IWorZmF3DFzOYNT43ngnAHBDkcppZQ6wJWoI3hKqeYhbQ5krJO5d46w6rdJ8iZ4mRuaLq4A0gQv\nBJW5Pdw6YykYePbiYTgjavhjVEoppYJBm6wopZqLjXMgLAr6jqt5m7hUCHdpgqcC519fr2fZtmwe\nO28wXdpGBzscpZRS6mCuBCjOkcYFSikVytLmQtdRshxCTRwOSOqlJZoqMH7csJfn5m7kohFdGDe4\nU7DDUUoppQ7lTADrgZLcYEeilFI1y98jzVV6jq5726ReOoKn/C+roJTfvbuMHskx3H9O/2CHo5RS\nSlXP5e3qrGWaSqlQtmm+XPccXfe2SX1g32YoLw1gQE1DE7wQYa3lD7OWk11Yxn8mDyU6spoWrkop\npVQocHoTPG20opQKZWlz5Puq05C6t03uA9YtSV4zpwleiHhzwRa+XruHe8b2ZUBKfLDDUUoppWrm\nSpRrHcFTSoUqa2HjXOhxQqtxXZYAACAASURBVM3dMytrQZ00NcELAWt35vLI7LWc3Lc9Vx7bPdjh\nKKWUaqXK3R7yisvq3tClI3hKqRCXlQa56b6VZ4LMwQPIbP6NVjTBC7KiUje3TF9KvCuCf0wcjDEm\n2CEppZRqpU55ah73friq7g33l2juC2xASinVUGlz5LrnaN+2dyVCdHKL6KSpE72C7K+frmFjRj5v\nXTWKpDZRwQ5HKaVUK9YhzsnO7OK6N9QmK0qpULdxDsR3hbY9fX9Och/I3Bi4mJqIjuAF0Xe/ZfDO\noq1cd0JPjuuTHOxwlFJKtXIp8U62ZxfVvWFENDgitERTKRWaPG7Y9B30PBHqUx2X1FtLNFXDFZe5\nuffDVfRIjuH3px4W7HCUUkopUhJc7M4txu2xtW9ojHexc03wlFIhaMcyKMnxvTyzQlJvKMho9iev\nNMELkufmbGBLZiEPTxiIM8KHzj5KKaWaFWPMVGPMHmNMtZPajDGjjTE5xphl3stfKj02xhjzqzFm\ngzHmnqaKuVOCi3KPJSOvpO6NnQnN/iBIKdVCVcy/63Fi/Z6X3Eeum3mZpiZ4QbBhTz7Pz9vIhCEp\nHNtbSzOVUqqFeh0YU8c231lrh3gvDwEYY8KAZ4GxQH9gsjGmf0Aj9eqc4ARgR44PZZquRB3BU0qF\nprS50GEQtGlXv+clVSR4zbtMUxO8Jmat5b4PV+GKCOPP45pkf62UUioIrLXzgawGPHUksMFam2at\nLQVmAOP9GlwNOsW7ANjhyzw8V4J20VSqNduzDlbOAnd5sCM5WGkhbFsk8+/qK7E7mLBmvxaeJnhN\n7IOl21mQlsndY/vSLla7ZiqlVCt3tDFmuTHmM2PMAO99nYFtlbZJ994XcCneBM+nTppaoqlU61Ve\nAjMmw3tXw/PHwK+fycLioWDrAnCXQq+T6v/c8EhI7Nbsl0rQBK8JZReW8sinaxnaNYHJR3YNdjhK\nKaWC6xegm7X2COA/wIcNeRFjzHXGmMXGmMUZGRmNCijOFU5MZJhvnTS1yYpSrdeCZ2Qh8RP+ANYN\n0yfB62fB9iXBjkzKM8MioevRDXt+Uh8dwVO++/vn68guKuORCYNwOHRBc6WUas2stbnW2nzv7dlA\nhDEmGdgOdKm0aar3vppe5yVr7Qhr7Yh27eo536QKYwydElzs9GUOnjMBinPB42nUeyqlmpmcdJj/\nBPQ9C07+M/zfQhj3JOz9FV4+Gf57pSR/wZI2F7qMgsiYhj0/qbc0WWnG322a4DWRJVuymP7TNq46\ntjv9U+KCHY5SSqkgM8Z0NEYWaDLGjET2yZnAz0AfY0wPY0wkMAn4uKniSklwsTPH18XOrbQiV0q1\nHl/eC9YDZ/xNfg6LgCOvgVuXyoje+s/hmZHw7cNNH1tBJuxa0bD5dxWSe0N5EeTWeF6tblsXQXHw\nvhs1wWsCZW4Pf3p/FSnxTn6na94ppVSrYIyZDiwADjfGpBtjrjbG3GCMucG7yURglTFmOfA0MMmK\ncuBm4AtgLTDTWru6qeJOiXf62GQlUa51Hp5SrUfaPFj9ARx3u8xVqywqVkb0bvkF+o+H+f+AdbOb\nNr5N8+S6ZwPm31VobCfN9CUw9XSYMUUWXA+C8KC8aysz9ftN/Lo7j5cuHU5MlP7KlVKqNbDWTq7j\n8WeAZ2p4bDbQxEdGIiXBxd78UorL3LWv0+pMkOuifUCPJolNKRVE7jL47A+Q0A2OvbXm7eI6wbkv\nwO7Vsn3PExteLllfaXMgKh46DWn4ayT1luvMjdDr5Po//5sHISwKNn8H3z8FJ9zV8FgaSEfwAmx3\nbjH/+vo3TuvfgdMHdAx2OEoppVStOsXLWni76irTdHkTPG20olq6vRtCp0NkMP30EmSsgzGPQYSr\n9m3DIuCspyBnG8x73Pf3yPgVfvh3w+a/WQsb50KP4yGsEQMqsR0hsk3DOmlunCOjiKc+AAMnwpxH\npVyziWmCF2AvzNtIqdvDfbrmnVJKqWagc4J3Lby6Gq3sH8HTBE+FiPw93uTAj2VxG76GZ4bDmo/8\n95rNUd5uSVZ6nwaHj/XtOd2OgSFTpOPmnrW+vcdb58JXf4Hfvqh/jPs2Qc5W6Dm6/s+tzBhvo5V6\nJnjWwjcPQVwqjLgKzvonxKfCe9c0+fekJngBtCevmHcWbeW8oZ3pmhQd7HCUUkqpOnWqSPDqWgtP\nR/BUqJn3uDc5+NI/r2etJDUAK//rn9dsrr76C7hLYOzfJQHy1WkPydy8T++ofRS0rBjenSIl3zHt\n4cdqq9drlzZXrnuOrv9zq0rqXf+lEtb+D3b8Aif9ESKc4IyDiVMhbwf877YmHQXWBC+AXvluE2Vu\nDzed1DvYoSillFI+qSjR3FlXoxUdwVOhpDgXlk+X20vf9s9rbvgGti+G+K7w21fyHq3RlgWwYgYc\nfTMk9arfc2OS4dQHYcsPB/59qrJWEqD0n2Xu3rG3wpbvYcfS+r1X2lyI63xgDl1jJPeB7G1Q5kPD\nKZBR428fhuTDYPCkA/enjoCT74U1H8IvbzY+Lh9pghcgmfklvLVgC+OHdKZ7chNNLFVKKaUayRkR\nRlJMJDvqmoMX4ZJGAjqCp/zN44GSvPo9Z9k7UJovozfrP4f8jMbFYC3Mewziu8C5z8vo1a9B6XsU\nXB43zL5LEqcT7mzYawy9FFJHyvIKhVmHPv7DvySBPOnP0n1z2GUQGQsLnq1fnJvmS/fM+oww1iSp\nN2B9X89v+QxZB/Dkew+d/3fMbfJ3+dndMsewCWiCFyCvfL+J4nK3jt4ppZRqdjol+LBUgjFSplm0\nr2mCUq3H90/BUwMgb5dv23s88NOLkkSMeQw85bByZuNi2PitjCgdfzt0PUbmVa16v3Gv2Rwtngq7\nV8IZjzS8E6bDIfPRirKlw2Rl62bD1w/CwPMPdJt0xsPwy+X3nZPu23vsWCrfRT1HNyzGqvZ30vSh\nTLO8BOY+CilDod85hz7ucMC5L8rvb9ZVUo4aYAFN8IwxY4wxvxpjNhhj7qnm8a7GmDnGmKXGmBXG\nmDMDGU9T2VdQyps/buaswSn0bt8m2OEopZRS9ZIS72JnXU1WQMo0tURT+ZO7DH56GUpyYN7ffXvO\nxm9kpGXU9dC+H6QMg6XTGj7nyVp577hUGHKJHKAPmCBJX2s6oVGYJWWH3Y+H/hMa91odB8JRN8KS\n12HbT3Lf7tXw/rWQMgTGP3vwyNuo6+V60Qt1v7bHI3MEo+Kh9ymNi7NCRYLnSyfNxVOlW+gp99c8\nehjbESY8D7tXSawBFrAEzxgTBjwLjAX6A5ONMVVbSd6LLOA6FJgEPBeoeJrS1B82UVDq5paTdfRO\ntWJ5u2Hn8mBHoZRqgJQEV91NVkBG8LREU/nTuk8hfxd0HARL3pAlCuqy6EVo0+HA6MnQS2DPati5\nrGExpM2BbYtk9C48Uu4beB54yiS+1uLbh6VUduzj/il7HH0PxKbAJ7fL6Oz0SbIcwaR3Dl12IaGr\nlGsueaPuuY9LpsocvzMegei2jY8TIKqNxFrXCF5JHsx/AnqcAL3qWFz9sNPhqJtktPnXz/wTZw0C\nOYI3EthgrU2z1pYCM4DxVbaxQJz3djywI4DxNImcojJe/2EzYwd25LAOscEOR6mm5/HI2axnRsCL\nJ8ptpVSzkpLgJL+knNzisto31BE85W8/vyJNTabMgnAnfPvX2rfP3AgbvpK29PuTsfPluQ1ptmIt\nzP27zDkbesmB+1OGQWL34JZp7loJH98Kqz8I/HvtXAFLXoOR10IHPy31FRUrXTh3r4Tnj5VlLSa/\nA3Ep1W9/9M1Qklv7v2P2Nvjqfpl7V/nfyx+SetWd4C18Hgr3yuidL069HzoP9730tIECmeB1BrZV\n+jnde19lDwCXGGPSgdnALdW9kDHmOmPMYmPM4oyMRk6aDbDXf9hMXkk5N+vonWqN9v4Gr4+DT34v\nJRe9T5Hb8x7XRWKbo5x02Lc52FGoIOgUX7FUQh1lmjqCp/wpYz1s/g5GXCElbcfcLN0Hty+p+Tk/\nvQyOCBh+5YH7XAnQ9yxZ2qC+8502zYNtC+G430N41IH7jYEB50qnxoLM+r1mY1grpaFvToAXjoNf\n3pB11TbND+x7fnY3uBJl1M2f+p0NfU6XpGj8s5Ls1CR1OHQ9WpIod3n1cX7yO7k++9/+GWWsLLmP\nHNfUdPxSkAk/PC1/a6kjfHvN8Ci46ktJnAMo2E1WJgOvW2tTgTOBt4wxh8RkrX3JWjvCWjuiXbt2\nTR6kr/KKy3j1+zRO69+BASnxwQ5HqaZTXgrz/wHPHyNlMeOfhcs+hskz4IjJMOcR+OwPMrqnalaY\nJbX5c/7WZJ22DlFaIN3A3jgb/jkAnh4qMfnaKlq1CCkJFUsl1HFwrCN4yp8WT5Vkbehl8vPRN0N0\nMnz9QPUH2SX5sGyazI+L7XDwY0MvgeIc+LUeJZUVo3exKdLJsaoB54F1w9qPfX/NhnKXwfJ34YXj\nZfHvPWtklOh3K2V+2LuXSEIcCKveg60/wil/kSTPn4yB81+VJGfQxLq3P/pmWby8ut/58hmyEP2p\n90NiN//GCfJ7Ls6GwhoS+u+fgrIC6ZxZH1W7bAZAIN9hO9Cl0s+p3vsquxoYA2CtXWCMcQLJwJ4A\nxhUwby7YQm5xObee3CfYoSjVdNKXwMe3SGLXf4LU6lfsaMMiYPxzEJ0EC56RL8kJLxwoo/GFxy1n\n0LYvkfWIcndCzxOh7zip0a9NeYmc+Vz9gTx/4PkyydvfOyx/WPMRfHqn/I6sRyb4dxgk8z4Gniel\nQYFiLWxdKAdKqz+E0jxI6Aaj/wS56fDDv2XeyTnPQLeja3+twiw5w7xlAVz8rv/PqKomkVKx2Hld\njVZciVJC5XGDI6wJIlMtVmmBLHXQ/xxo4z2Z74yTzoqf3y3f5VUbaCyfLn9/I68/9PV6nChLHCyd\nJt/9vtg0XxKbM584ePSuQsdBctC/+n0YceWhj/vLktdh3j/k+zf5cPnuHXzhgZgungmvnALvXADX\nfCNrzflLaQF8eR90HCzLGwSCMw66jvJt28PHQtuecgwx4NwD+5S83fD5PdDlKDgyQKNhSd7j+bmP\nyfFM/m4pK83fLZfiHDjiYmnsE2ICmeD9DPQxxvRAErtJwMVVttkKnAK8bozpBziB0K7BrEFBSTmv\nfJfGSYe3Y1Cqjt41O4VZ0qlp6KWQ0KXu7StYC3vWSv24KyFw8QVbcY6U6+Vsl05RudsP3N66ANp0\nlEnSfccd+lyHQyY+t2kvI0FF++DCt2QCc1XWymvuXA7piyUp27FMEg6AqDhJFtd/Jl/snY6Q0oi+\nZ8kXrDFy1jNtriR1az+RTmzOBOgwUJKmhc9Ld66j/s9/k7EbI38PzL5TEryOg+GS9+R3teYjWDlL\nWkp/8yB0HiEHKUMu9u/f2rpP4Ys/w75NEBEjO9AhF0tZjMNbUDHgPPjfrfDaWCkrOeX+Q//9dq2S\nieMrZkJ5sXRdK84OzWRa1al9rJMwh/GtRBPkOyIU/j+p5mvVe/J9feQ1B98/4kpY+Bx87Z1nVfG9\nZK2UZ6YMrb48zuGQCpL5/5D9V3xq3THM+zvEdqo5sako0/zuSfnubtO+fp/RF+s+lUW/u4yCcU9K\nOaOjSnFbYjeYNB3eOAtmXCwVMxFO/7z/9/+EvB1wwWuhcdLGESb769l3yonIipOMs++UypLxzxz6\n+/GXjoNkRPnnl6UZTJv20synfT9ZjiEuReZ+hqCAJXjW2nJjzM3AF0AYMNVau9oY8xCw2Fr7MXAH\n8LIx5vdIw5UrrG2eE3XeXriFfYVl3HKKjt41Ox6PtOnd8DUsfEG+UAdfUPfzcnfCp7fLwqfGIROw\ne46WS5eR1Z/9CyRr4Ys/SYKUeqRcOg2ByOjGveb71x26npAJky+2uM5w9E1yhtVZx4mNY2+T5Ozj\nW6X87+KZkgDsXC6XXSvkuqIFtSNCvlyPmCQ1+p2Hy5lTh0Mm1a/7RBK4OY/IJbGHzPtLmyuvERUn\nid/A8+RMbnikJCHzH5cd/sIXYNR13hIgPx+Y5u6Q94iMkZ10l6MOnJGuYC2seFcS1dJCKYU55lY5\nSwiShI66HvZtkbPFq96DL/4Ia/8HV872z8hYaQF8dDPEtJOR1X5nV5949zoJblwgzQ4WvQi/fg7n\n/Bu6nyDJ9qIXZd5MuAsGXyRxdxjQ+PhU0IQ5DB3jnL6VaIL8n9METzXGz69Cu35ycqmy8CgpgXv/\nWvkurCjrS5srC0tPeKHm78MhF8t3/vLpB9ZYq8mm76QT49jHa0+WBpwn3+9rPvL/PKq83VIR03EQ\nXP5J7dUuXY6Ec1+A/14BH90E57/S+P1C1iaZUzboQuh6VONey5+GXCwdPRd4q0jWfCQlm6fcL/Pk\nAiWuE9y1ARzh1e8bQ5hpbvnUiBEj7OLFi4MdxkGKSt0c//i39OsUx1tX+zjkrOTLedvPcMKdwS3j\n+u4pGSE58W5vTItg4EQY90T1ow/WwtK34It7wV0Cx99xYNRo+xKpz4+Ihm7HSLI3ZErTHPis+Qhm\nXgYx7aHAW+VswuRAO/VIOcN52Jj6xbLwBSmNGX4l9Dheyl3iOsvk94ae2Vs3G2ZdKeWTeL9/wiKh\nfX8Zkes0WBLTDgN9OyOZt0uS7HWfStevnifKDrj3KTUn2bvXyE5/9YeShB15jSR6VZOwhlg5Cz69\nwztnzYK7VO5v21MSva6j5CDmuyfgty9lYd7xz0K7w+p+7cVTpWnNuS9K4ttYC5+XBPOqL3zfmW9d\nKElh5m+SGBZkyN/FkdfInBU//60bY5ZYa32cva78uY+c+PyPhDkM715fS1nur59Jq/Nrv629WYJS\ntdm+BF4+WUojq0uaPB548QSp5rjpZ0l8pk+W9dRuX1P7CdXXz5Kqk1t+qf1Y4/WzZDrAbcvr3vc8\nO0pOWF4527fP5wtrYdoFcrLsunnQvq9vz/vuSfjmITjhD3DynxsXw4wpsHEO3LK45s6WwfLNQ3K8\nds038p0T1wmu+bZJ5rOFqtr2j633t+JHHy/fzt78Um46STtn+mzPOvkiKc2Xg+rhVwQnji0L5KzQ\ngHNh9B/lC/L7f8K8x6T08NwXZG2TCvs2S+lE2lzodiyc8x9powvyxVqcA5t/kDV00ubCl/fC9/+S\ntsADz/ctkd2zVuY8DZwIfU717XOU5MFn98hZv2vnyshY+mJI/1nmra2YCYtflXlVV3916GT06uxY\nBl/dB33OgLP+6b8kvO+ZcMWnkpC26ysJXbu+B0au6iu2o5RI1KdMokN/uOB1OHGtnIn94d9Sojvs\nMkn0GjJZuzBLErvV70tCfe6LUhK0Y5l0ZNu6SBK65e/I9hHRMOYxGHmd78nysCtkPsmX98m8hLpG\nTWtTXipnarsdW78ztV2Pghu+lwR11yo5s3r4ma16J9tSpSS4WLatjgYq+0fwtNGKaoSfp0qJ+OCL\nqn/c4YBTH4Bp58v8tD6nycmF4++ou1pmyBT48AbZp3c75tDH3WUyx2rzdzDm776dWBxwHsx9VCp5\n4jrVvb0vfn5FlnsY+w/fkzuA426XRd7nPy4nEodMPvjxkjzI3ioXZ7yciKnud7bhG6mMOeX+0Evu\nQPaVPzwNb46H8iK49H3d79RCfzN+MG3RVg7r0IZRPbQ8xSfFufDuFFnUsuMg+PxPkkS17dm0cRRk\nwqyrpFHH2U9LAhMWDifeBb1PltLEN86REsST74Vf3oSvH5Ttxj0Jw686tO7bGS8JTN8z5eedKyQh\nfO9q6fY07smak4fcHVJquOwdabKx7lO4fp5vv5c5j0LeTrjwTfkMMclw+Bi5gDRA2Py9nPGcdj5c\nMVsmOdekJE9G2aKTYcLz/h9hTR3he0vhQGrfDyZOleT+h3/B4tekTGjQBXDc73yfOP3b11IiU7hX\n/laO/f2BHU/XUXI5FjlDm5UGO5ZKEljfRNLhgDP/IWe65z4GYx6t3/MrWzFD5lmM/0/9nxvhrH/X\nMNXsdEpw8tmqIjwei8NRw3eAq1KJplINUZgFq2bJfLna9ku9T5G5vfP+Lh0ljcO3E3v9z4HZd8la\nalUTvKw0eO9aORE6ZIrvJwoHngdz/yZLOBx1o2/PqU3Gejlx1+uU+pd9GgPj/inl/B/fAjt+kcqW\niqSuKOvg7cNdsk/qfrwce6UMlfs/v0emOhx9U+M/TyDEdpR98/J34Pg75fhR1SjYyyQ0eyvSs1mR\nnsOUUd0w2i2ubh4PfHij1Hlf8LrUjDvC4YMbJAlpyjg+uF4OyC9849CdSufhcP18mdy94Bn4Rx9p\n89/taPi/hVKO5suk3k6D4Zqv5azglh/huaPgx2cOXs+lOEcSx6eHSUvkUTdKeYYxUltfXlL7e+xc\nIaNPw6+QmvzqOMKkdPGiN2WEcMbFNb+utfDJ7TJaef4rEJNU9+ds7pL7SJnkbctk/tjaj+XfavrF\nUpJYlC2/r6ol7aUFUjI57Xw50L32W5nnUdNZRWNkxHfQxIa3dO48TP4uF70Iu1c37DU8bhlZ7jhY\nDiiUqkZKvIsyt2VvQS3fQRVl7LoWnmqo5dOlMdORV9e+nTFw6oOy317ymswZjq+6vHI1ImNkGYXV\nH8qyCiDf5cumyxIEe3+Dia/BhOd87/Cc3Ee6HPtj0fPyUnj/GjnpPf7Zhp1QDY+Ei96SKRm/vAkZ\n6+RE74AJ8jub+JqUNk56R44V8jNkTvWrp8Hfu8PLJ8He9VJV0tT9A+rjlL/AyffBiX8IdiQhT0fw\nGumdRVtxRYRx7jAfvmQU/PBPKQE442/Q/Ti5b9wTMnn6h39JuUVT+PFpKYU48wmZ91WdyBgpTTxs\nDMx/Qg6qj5hc/y9fRxgcdYN0mPz0Dvjyz7L46rgnpYRy3uNyhm3QBTIqUtEOf8Lzkoh9eR+c+Xj1\nr+3xSKMXV6KsA1OX3qfKsgUfXCcjlBOnHloeuGyaNFU56c/Q/dj6fdbmLj5VRsVOuEsSqEUvVFlD\nychOONwpJZZlhTJycfTNstPxVxezupx8nxysfHpnwxqurPkIsjbCBW/oMgaqRhVLJezMLqZ9bA1/\n21qi2XKkzZXmVJ2HNd17Witzi1NH+jYikzoc+o+X77BR1SyNUJOhl8rc+TUfSvOtT2+XxlVdj4Hz\nXqpf9+wKA8+VeWHZ2xr2/ArzHpMGYxe+1bhyT1ciXDdXbtf2vV7R7bpgr1T2bJov14MuhMPOaPj7\nN4W4TtK3QdVJE7xGyC0u46NlOzjniBTinA2cP1ST8lL4+GaZh3XY6f597eoU7ZMRm4RugWsIsuEb\nme828HxpeVth0AVSjjjnUeh9mox61Wbvb3LWLCxCkrCIaDnorrgdnSQNO2oaYdu6UL6U+084tB1z\ndQ47wz9fegldZF2w1R/AZ3fLGjYgHR5Pe/BAmUSFvuPgqJtg4bOSaPUff+hr/vKGJInnvuh7O/oj\nLpImLF/eC5+1k5K/ip1Bxq9SytLjhKZLtkNRdFs46Y9wzC1yQqIwS5K58mJpnlJx7XHLYro9jm/6\n+E59QJYuWDFT/k19Za1MVE/qI2fAlapBp3hJ6nZkF3FElxqW5ohwygkPHcFrvqyVuchzHpETjLcs\nDVzb+ao2zYPMDbIP89WZT8BhYw/ttlmbLiPlO+/H/8hC5rnb4aR74fjbG94wbMB5ciyx+gM49taG\nvcaWH+X7eOglUkraWPU5YVcxwjdgQuPfV4UcTfAa4aOl2ykqc3PxqDoWW26I5e9IC/V1s2UeVkUj\nj8bat0XaAGdtktrzrDRZ/6pi/kR0Mkya5v/2uPu2yDy0dn2lMUnlLyFjZKRs60IZVbpubvUjIdZK\nWcbnf5IJtrWJTpZJ2H1Og14nH0h+CrO88+66wDlPN/3ohTFSu9/rJJko3nGQlMjVFMepD0iDjo9u\nkXK6tj0OPJafIesCdT++5onpNTnmFqnRX/CMNFw54S5JWP57hSTJ570cGuvfBFtUG/90qwyEoZdK\ngv9VRcOVWuauVLbha9i9UkqB9N9Y1aLz/sXOfVgqQUfwmqfyUjlRtHy6dC7evUoW+q6osAm0n18F\nV1s54eqrNu0PbSRSF2OkIdQ3D0oSe9UXNU9p8FXbHnJidvX7DUvwinPg/eulXH/MY42LRakqNMFr\nIGst0xZtZWDnOAb7e2Fzd5m0vW0/QBpnzLwcrvlKRqkaI2O9jBqV5Mrk5Pgu8gU14Fxp5BHbSc7g\nvX4WnP1vGDrFP5+nrAhmXirlhBe9LSNtVUW3lcUqp02UuvAzHjn48YJMmTz866ey0OmE52XOU2kh\nlBUcfJ27XQ5i138uOy0TJmuR9TlNktuCDLj6y8Z1IGwsVyIc9/u6twuPlNr5F4+XpidXfXGgPv6r\n++TzjnuyYYnqaX+VhVq/fViWVtixVCauT3lPJjOr0OZwyJns/Q1X/ubb8757EuJSpRxHqVokREfg\njHD4tti5juA1P4VZ8O6lsOV7GP0nOfH35OHSjKQpErzcHVK9c/RNTVPePuoG2fcOPN/3E2J1GXAu\nfPUX6Z7d7Rjf98X5GbJQd2667NejYv0Tj1JemuA10C9b97FuVx6PnjfI/81VVrwrnY8u/q98WUyb\nKCV95zzd8NcsyoYZk2W9sevmSQljdZOJe53sXTTz/+Rg/7SHGneW31qZd7ZzOUx+t/aRyD6nSQer\nBc/KvLeKsreN38IHN8o8tdMfkfLOivKRCBdQTROQIyZJ+Vz6YmlN/9sXcuYO5KC4ajlkKEvsJvPm\n3p0CX90PYx+TBVmXT5cyynaHN+x1HQ4ZxSnMlE6fWFmM3NelGVTwdR4mE+YXvSAnZOpaXHzLj9Iq\nfOzjvjcTUA1mjJkKnAXssdYOrObxKcDdgAHygButtcu9j2323ucGyoOxFqAxhpQEFztz6kjwdASv\n+clKkzXXsrdKxcZg7wmfgedLx+exj/svCaqOxy3ljdYt89ubQmS0/99r4ESY/yS8fiYkHyY/D5pY\n/bFOcQ6s/UQ6hqbNoSy7WQAAIABJREFUk89+0p+lfFQpP9MEr4GmLdxKm6hwzjnCz2uFuMuloUen\nIZLwGCMH8d89KWeHGlIu5nHDe9fIHLvL/wcpQ2reNrotXPIefPEnKd/bu146KdZ3tCt7q5Rcbvha\nEtYT7z7Qsr82pz8sE70/vFFKNb//p8SRfDhM+W/d8/Mqc4QdaFF/yn1ytjBzY9OVnvhTv7Oku+ai\n56WsZO7fZXmH4xs52Tg8UpZWmDZRupmefJ9/4lVN55S/SOOA2XfJ+oK1nXD67ikpXx56adPF17q9\nDjwDvFnD45uAE621+4wxY4GXgFGVHj/JWrs3sCHWLiXexfbsOko0XYmQk940AanG27pQGnhZC5d9\ndPDSAUMvlakQq98P3Pq05aXSxXr1+zI9oKmXSPKn+M5w61JY+xGsfE/Wxpv7NzmGGzRR1gjdtQJW\nzoLfvgJ3iey7j71NHq/rpJxSDaQJXgNkF5byycqdXDgilZgoP/8KV82SOXGTph84UBv9J1kk+ZPf\nS8dHX9fmqvDtX6Vj5Linql/ks6qwCGm80b6fHDS+chpMnl7z6Ju7HPb+KiMDWxbIziPXu7OPioPh\nV8KJ9/gWa2QMnPsSTD0d/jVYyi5HXC2JX2S0b69Rk7iU0Fy801enPQTbFsGsqwErI7yN/Z2AzDO7\n8jPZ2TfVxHrlP/sbrtwmJ0NGXl/96NzO5fI9cPJ9/vm7UXWy1s43xnSv5fEfK/24EEgNdEz11Sne\nyfr1GbVv5EqQuVsq9K16X5Ylik+Vk6ZV9+udh0G7flKmGYgEr7QA3r1EKnNOe0gSneYuJkmqj0Zc\nBTnbJXFdOUsamX3pXS+0TQcZPRw4UdaA1e7FKsA0wWuAWUvSKS33cPHIBq5jVROPWzpZdRgkTRMq\nhIXDxFfhheNkPt6138pBuS9WzpJRsOFX1r3GTFUjrpKuUzMvlbl7o26UEoOCPTJ3qyBDrgszAe/6\nYG06ylpxXW+TRi0dBtS/xLPLkbJcwE8vy+hhxaLhrV14JFzwGrx4ojRp8Wd3VWN0h9OcDb1Muml+\nea/8fx98kSza27FSVeB3T8kJF186x6pguBr4rNLPFvjSGGOBF621LwUjqJQEFxn5JZSWe4gMr+EE\nkJZoNr1FL0GH/vWrSFn3qTQ76zJK1kOrrmO2MVLu/eW90lXZlykABXtlPl+7w2rfrjAL3rkQti+B\nc56BYS2wkiC+s8xlPOYW2LtBTqq17yfN0LSplWpCmuDVk7WWdxZtZVjXBPqn+Lk+ffUH0i74wrcO\nPdiO7QjnvwpvjpeRvPNeqvuAfOdy+OhmaSU8toZ11OrS43hJKGdMkbKDiGiIaSdno9r2lB1Fm/aQ\n2EMSusTu/kkUjr8Djrtdk46qErvDbct1QrY6mMMBl30MaXNkraefXoaFz0mZ0NBLZM7pmo+ksY+r\nhnb3KmiMMSchCV7lo/XjrLXbjTHtga+MMeustfNreP51wHUAXbv6t6tzSoITa2F3bjFd2tYw8utK\ngNI8qeYI08OKgMvfA5/dJXPqL3zLt+kPW36UDtIpQ2HKrNpPEg++CL5+QEbxTv9r7a/rLoM3J0hn\n3tQj5WTygHMPrRLI3Qlvn+c9xnmzdSzRktxbLkoFQZ3fxMaYW4C3rbX7miCekLcgLZO0vQU8eUEN\ni2M3lMctC1637y+LcFan54lw0p+k02W3Y2qfLJyfIUlZdFv5Mm1MQ4W2PeGGH2QdMF9HDv1Bk7vq\n6QG6qk5Y+IGlQQoyYeV/Ydnb0qkNZK2yyutPqpBgjBkMvAKMtdZmVtxvrd3uvd5jjPkAGAlUm+B5\nR/deAhgxYoT1Z3wVi53vyC6qOcGrWOy8OEfK1VRgbfL+GbTpKOWOF75xYPHq6uxaBe9Mks7ZF/+3\n7v14m/bS6Gz5DJnjG1bLOr8Ln5fkbsTVsPk7adD2xR/hiMmS7LXvK3Pf35ogI3hTZsmxjFIqoHw5\n1dYB+NkY8wswFfjCWuvXHUhzMm3RVuJdEYwb3Mm/L7zmI5nHNvG12udBHX+nzHX77G6pZU8+TEZ1\nEroeaDPsLoP/Xi4llFd9Ll/WjeVwNG1yp5RquJgkOOoGuexcLgdq7fpCm3bBjkxVYozpCrwPXGqt\nXV/p/hjAYa3N894+HXgoGDF2iq9YC6+WTpoVJ52K9mmC1xQ2fyfl1tfPk06YMy+TY4fqFsret1lG\nziJj4NL3ff/3GXoJrPtEGoP8f3t3Hh9XXe9//PWZyb40TdukO20pXShbgVJAUDYVUDZRsRUQEEQU\n0Htd8XevorhcvV6Xq6JYkMUFEFC0Iojs4KXQFihLN+hG17TpkiZp9pnP748zaUObZdJm5kwy7+fj\ncR4zc86ZmU+ODcdPvt/v59PVMokdbwdFRaZ8IGjXA8FI4cLbg+3FW4IZRNtWQrwNLpsLo4/drx9Z\nRHqnxwTP3f/TzL5OcIO5AviFmd0H/MbdV6Y6wExSXdfMPxdXcekJ4ynI7cO51PF4sPZu2BSYdn73\n50YiQUnj28+Cf/7HO4+VjgqSPY8HzbEvvLV/tQMQkb438qhgk7Qzs3uAU4FhZrYeuBHIBXD3W4Bv\nEPR5+WWi3U57O4ThwIOJfTnA3e7+j7T/AARTNAE2dldJc/cIntbhpcXq52DcScEMnUsfDKog3395\nsFb/sA/tOa++Gn73IWhrDv7YO7gX03cPeV/QH/WV33ee4LknZgdYUJStfcbN+JOCbddWWPQHeOnO\noHfrJQ/tf0sfEem1pCbLu7ubWRVQBbQB5cADZvaYu38llQFmkvtfWkdrzPn48X27xoFlDwU95y68\nLblFuMXD4LoFwQjdjjX7brUbgyIlR6qRsYhIWNx9dg/HrwL2qXrj7quAjMjKi/JyGFyU232z88Ly\n4FGFVlJv5wbYvnJP0bSCQUFroz98NKiwHI8F5feb64LEr3ZT0Aqht9W3ozlBW6Z5Nwdr/vaeCbTk\nL0GP2TP/K6jIubfiYUGFzHd9LvijswqMiKRVMmvwPg98AthKsE7gy+7eamYR4C0gKxK8eDwornLC\nwUM4pLIPpyq6B2vvhh4Ch1+Y/PvMgv/gllSqSaaIiKTMqLJCNu3sZgSvUCN4abPmueBx/Lv37Msv\nDda23f0x+POngvXyrz8AVa8HLY4OOr7zz+rJ0ZfA8z8Letm+6/o9+xtrgmUiI6fD8Z/u/jPMwJTc\niaRbMk2vhgAXuvuZ7n6/u7cCuHsc6KIayMDz4urtrN/RyOyZfTx6t/yRYIHyu7+kv3CJiEjGGTW4\noPsRvIIOa/AktVY/G4yYDj/8nfvzS+Di+4K2CXOvh9XPwPk3w+Qz9/+7KqbAmJnBNM2OpReeuCmY\nQXTu/+r/t4hkqGQSvEeA7e0vzGyQmR0P4O5LUxVYpnlh1TbM4LSpfVCwBKC1KWgI/tT3gnVzR3y0\nbz5XRESkD40sK+xhimZ7gqcRvJRb/VyQxHVWjC2vGGb/MRh5O+cnML3bGcLJOfpiqF4W9K4DWDc/\nKKBy/Gdg1PQD/3wRSYlk1uD9Cjimw+v6TvYNeAvWbOfQEYMYVNBNueDu7NoaJHTrXgy2ja9ArAUw\n+Mjt6h0kIiIZadTgQmqb2qhvbqMkv5N7VU4+5BRqimaq7VgDO9fCSZ/r+py8omDkrq8cdiE8ckPQ\nX3PkUfC3z8Og0UHLJhHJWMlkFdaxLYK7x80sq7KR1licV9bWcNGMThYS96R2I9x9UTAXHoLGpKOO\nDuatjz0h0ShcpctFRCQztVfS3FTTyKThpZ2fVDhYI3ip1t7/ruP6u1QrGASHXQCv/wlKhgcF4Wbf\nq7ZJIhkumURtlZl9jmDUDuCzwKrUhZR5Fm+spbE1xnEThvTuje7BXPhtK+G93wz6wYycvqdfnYiI\nSIbb3ex8Z1M3CV65RvBSbfVzQeuCdLcbOPoSePUeeOYHcOi5MOXs9H6/iPRaMmvwrgHeBWwA1gPH\nA1enMqhMs2B1sARx5vheJngv3QErHof33QQn/zscdIKSOxER6VdGlrX3wuuh0ErD9q6Py75q1sKD\n18Cm13o+1z0YwZvw7j0959Jl3ElBrYC8Ujj7v9P73SKyX5JpdL4FmJWGWDLW/DXbGTe0iMpBvUjO\ntq+CR/8TDj4NjtunzZGIiEi/MHxQARELpmh2acTh8NJdQSXN9r540rW3Hoc/XxVcr/otcOmfuz9/\n2wqor0rv9Mx2ZvDRO6GtBQaNSv/3i0iv9TiCZ2YFZnatmf3SzG5v39IRXCaIx52Fa7ZzXG9G7+Ix\n+MtnIZITLHZO91/bRESkT5nZRDPLTzw/1cw+Z2aDw44rHXKjESpLC9hQ000vvOkfh1gzvPGn9AXW\nH8Xj8PT3gybkpaPg2Ctg5ROweUn372tffzfhPamPsTOjjt7/fnoiknbJTNH8HTACOBN4BhgD1KUy\nqEyysrqeHQ2tHDe+F3+RnHczrJ0HH/hvKBuduuBERCRd/gTEzOwQYA4wFrg73JDSZ+TgAjbt7GYE\nb+T0oDfbK79PX1D9za5tQWL39H/BUbPgqsfhjG8EFUjn9VD5cvWzQfXKIQenJ1YR6deSSfAOcfev\nA7vc/S7ggwTr8LLCgjVB49akR/C2LIUnvw1Tz4EjP5bCyEREJI3i7t4GfAj4ubt/GRgZckxpM2pw\nIZt2djOCZxYU49j4CmxenL7A+ov1L8GcU2DNc3DOT+GCXwUtDYqGBL3mXr8P6jZ3/t54HNb8K5ie\nqRlBIpKEZKpotiYea8zscKAK6KNu35lvwZrtDCvJY8Kw4p5PjrXCg5+G/EFw7v/qP8QiIgNHq5nN\nBi4Dzk3s28/GqP3PqLICHl+yGXfHurq3HXER/PPr8Mof4KzvpTfAMNVXB/d+CNpFFJYHRWfan9dv\nhqf+C0pHwicfhdF7tRE+4bOw4Dcwfw6c8fV9P796KTRsDW96poj0O8kkeHPMrBz4T2AuUAJ08l+g\ngWn+6mD9XZc3tI6e/SFsehU+9gcoHpb64EREJF2uIKgq/V13X21mEwiWMGSFUYMLaW6Ls31XC0NL\n8js/qXgoTP0AvHZv0BooJy+dIYbnuf+BVU8HjcB3rA76ATbVgMf3nHPI++DCOcGI3d6GToSpH4SF\nv4F3fwHy9vqD8urngscJIRRYEZF+qdsEz8wiQK277wCeBbJq8vfGmkY21DRy5ckTej55w0vw7P/A\nUbPh0HNSH5yIiKSNuy8BPgeQ+KNnqbv/INyo0mdkWaIXXk1T1wkewNGXwpK/wpv/gGnnpSm6EO1c\nDwtvD4rMnP+LPfvjcWipC5K9tiYYOgki3ayKOfE6WPYQLLobZn7qncfWPBe0KRh8UEp+BBEZeLpd\ng+fuceAraYol4yxYE/T06XH9XWsTPPgZKB0BZ30/DZGJiEg6mdnTZjbIzIYALwO3mtmPw44rXUbv\nbnbeTaEVgImnB9Uhs6XYyjOJvnCnfPWd+yMRKCiD8nFBY/LukjsI+uSOPhZe+GVQibtdPBYkeGG0\nRxCRfiuZIiuPm9mXzGysmQ1p31IeWQZYsGY7xXlRDh1Z2v2Jb/4Dti6HD/4omHMvIiIDTZm71wIX\nAr919+OB94YcU9qMHJxEs3OASBSmz4YVj0HtpjREFqJtK4NE9tjLYfDYA/sss2AUb/sqWP7Inv1V\nr0PTTq2/E5FeSSbB+xhwLcEUzZcS28JUBpUpFqzewTHjysmJ9nCZlj8MhUOCOfYiIjIQ5ZjZSOAi\n4KGwg0m3ocV55OVEuq+k2W76xcH6s1fvSX1gYXrmBxDNg3d/qW8+79DzoOwgmNdhqmd7/zuN4IlI\nL/SY4Ln7hE62pNbimdlZZrbczFaY2Q2dHP+JmS1KbG+aWc3+/BCpUNPQwvLNdczsaXpmrBXefBQm\nnwnRZGrWiIhIP3QT8Ciw0t0XmNnBwFshx5Q2ZsaosgI29DSCB0HRkIPeFYxuuac+uDBsWQqv3QfH\nXw2lw/vmM6M5cMJngj66618K9q15Lli/NyhrOnKISB/oMcEzs090tiXxvihwM3A2MA2YbWbTOp7j\n7v/u7tPdfTrwc+DP+/dj9L2F7f3vJvSQ4K2dF1TLmvKBNEQlIiJhcPf73f1Id/9M4vUqd/9w2HGl\n0yGVpSxaW4Mnk7QdfQlsXwlrX0h9YGF46nuQVwIn/Vvffu4xl0J+Gcz7efAH5Lef1/RMEem1ZKZo\nHtdhezfwTSCZ0lgzgRWJm2ALcC9wfjfnzwYyZj7Hgre3kxs1po/tYU3d8kcgmh8sLBcRkQHJzMaY\n2YNmtiWx/cnMxoQdVzqdPrWSDTWNvLm5vueTp50fJECLBmCxlY2LYOlcOPHaztseHIj8Ujj2sqAS\n6ZK/Qku92iOISK8lM0Xz+g7bp4BjCHrh9WQ0sK7D6/WJffsws3HABODJJD43LRas3s4Ro8soyI12\nfZI7LPs7HHwq5CdzSUREpJ+6g6AX7KjE9rfEvqxx+tRKAJ5Ytrnnk/NL4LAPwRsPQnMSCWF/8uR3\nggbmJ342NZ9//DVgEXg4sbZP6+9EpJeSGcHb2y6CZKwvzQIecPdYZwfN7GozW2hmC6urq/v4q/fV\n1Brj9Q07e56euWUJ1LwdNHYVEZGBrMLd73D3tsR2J1ARdlDpNKKsgMNHD+KJpVuSe8PRl0LrLljy\nl9QGlk5rXwgqhJ70+aANQiqUjYbDLoTGHVB5GBQPS833iMiAlcwavL+Z2dzE9hCwHHgwic/eAHSs\nGzwmsa8zs+hmeqa7z3H3Ge4+o6Ii9ffTV9bW0BrzngusLHsYMJh8dspjEhGRUG0zs0vMLJrYLgG2\nhR1Uup0xdTgvr93B9l0tPZ88dmZQICSTeuK99Rj8fAZsXtz797oHo3fFlTDz6r6PraMTrw0eNT1T\nRPZDMiN4/wP8KLH9F/Aed9+nImYnFgCTzGyCmeURJHFz9z7JzKYC5cC8pKNOsfYG5zPG9ZDgLf87\njJnRdxW0REQkU32SoEVCFbAJ+AhweU9vMrPbE2v23ujiuJnZzxLVpl8zs2M6HLvMzN5KbJf1zY9x\nYM44tBJ3eGpZEqN4ZkGxlbXzYOuK1AfXk1grPPIV2PYW3DMbGrb37v2rnwmqWr77i5BXnJoY242a\nDh/7PZz8hdR+j4gMSMkkeGuBF939GXf/P4K/Yo7v6U3u3gZcR1BWeilwn7svNrObzKxjkZZZwL2e\nVFmu9FiwZjtThpdSVpTb9Um1G2HjKzBFo3ciIgOdu7/t7ue5e4W7V7r7BUAyVTTvBM7q5vjZwKTE\ndjXwKwAzGwLcCBxPULTsRjMrP4AfoU8cPqqMytJ8nkwmwQM4ahZYNDOKrbx0Z9BI/JQboG4T3H85\nxNqSe687PPFtGDQGZlyRyij3OPRc/QFZRPZLMgne/UC8w+tYYl+P3P1hd5/s7hPd/buJfd9w97kd\nzvlmkiOCadEWi/Py2zs4bkIP99HljwSPUz6Y+qBERCQT9Ti84u7PAt0NFZ0P/NYDLwCDEw3VzwQe\nc/ft7r4DeIzuE8W0iESM06dW8syb1bS0xXt+Q+kImHwWzL8NqpenPsCuNNfB09+HcSfDqTfAOT8J\nRuQe+0bP743H4NH/gA0L4ZQvQ05+6uMVETkAySR4OYk2BwAknuelLqRwLd1Ux66WGMf1tP5u+cMw\n5GComJKewEREJNNYH3xGVxWnk65EnW5nHDqc+ua23csZenT2DyC3IJgW2bgjtcF15fmfQ8NWeN9N\ne6aOzvw0vHAzLOqmQ1NTLdwzKzhv5tVB4RgRkQyXTIJX3XFKpZmdD2xNXUjhmp+4Yc3sroJmcx2s\nfjZobm59cX8XEZF+KCOWFqS70vRJhwwlLyfC40uTaJcAMHgsXPQ7qFkLD1yZ/LTIvlJXFSR4h30I\nxhy7Z/+Z3w1aEPzt87DhpX3ft301/Ob9sOIJ+OCP4QM/hEg3rZNERDJEMgneNcD/M7O1ZrYW+Crw\n6dSGFZ6Fa7YzpryQkWWFXZ+04nGItcBUTc8UERnIzKzOzGo72eoI+uEdqK4qTiddiTrdlaaL8nI4\naeJQnli6haSXz487ET74I1j5BDx+Y2oD3NvT3w8KrJyx13TMaC589K5gndu9l0Bdh4T17efhtjOg\nbiNc+mc47sr0xiwicgCSaXS+0t1PAKYB09z9Xe6eAeWw+p67s2DN9p6nZy57GIqGwtjj0xOYiIiE\nwt1L3X1QJ1upu+f0wVfMBT6RqKZ5ArDT3TcRFCh7v5mVJ4qrvD+xLyOcfuhw1m5vYGV1L5qYH3tZ\nMM1x3i+6nxbZl6rfhJd/CzM+GSyr2FvxUJh1NzTVwH2XQltz0NbhrvOCZuZXPQkHn5qeWEVE+kgy\nffC+Z2aD3b3e3esTN5vvpCO4dFu9dRdb61u6T/BirfDWo8GicU3VEBGRbpjZPQRtgKaY2Xozu9LM\nrjGzaxKnPAysAlYAtwKfBXD37cC3CVoOLQBuSuzLCGdMrQRIvul5uzO/t2da5PqFKYhsL098C3KL\n4JSvdH3OiCPggl/Cuhdhzmnw12th/Elw1eMw7JDUxygi0seSmaJ5trvXtL9IVPP6QOpCCs/ijbUA\nTB87uOuT1s6Dpp1qjyAiIj1y99nuPtLdc919jLv/xt1vcfdbEsfd3a9NVJs+wt0Xdnjv7e5+SGK7\nI7yfYl+jBhdy6MhBvU/worlw0W+D6pr3Xhy0HEqVtS/Asofg5M9D8bDuzz3sQ/DuL8GWxXDcVXDx\nA8EInohIP5RMghc1s901gc2sEBiQNYK31DUDMGpwQdcnLXsYcgpg4ulpikpERCTznDG1koVvb6em\noaXnkzsqGgKz74WW+iDJa23s++Dc4Z9fh9KRcMK1yb3n9P+E618O1gpGu+mDKyKS4ZJJ8P4APJGY\nVnIVQS+eu1IbVjiq65rJi0YoK+ziP+zusPzvwXz8vOJ0hiYiIpJRzji0krjDM2/uR+XO4dPgwjmw\n8WW47zLY+Epwj+0ryx6C9fPh1K9BXlFy7zGDoRP7LgYRkZAkU2TlB8B3gEOBKQSLvMelOK5QVNc1\nU1Gaj3XV+mDz4qDM85QBOUNVREQkaUeNGcywkjwe7+00zXZTPxisyVv9DMw5FW45GV64BRoOcKlh\nrBUe/yYMmwLTLz6wzxIR6YeSGcED2EzQ7+ejwOnA0pRFFKLq+maGlXYz+3T5w4Bp/Z2IiGS9SMQ4\nbUolzyzfQmssvn8fcuK18MXlQZ+5aC7846vwoylw/+VBS6J4rPefueA22LYC3vctiPZFoVMRkf6l\nywTPzCab2Y1mtgz4ObAWMHc/zd1/kbYI02hLbRMVJd0keMv+DmOOg5LK9AUlIiKSoc44tJLapjYW\nrtmx/x9SODjoM3f103DN/8GMK2HVM/D7D8Mt7w4KmyVr20p4/FtwyPuCatciIlmouxG8ZQSjdee4\n+8nu/nNgP/6U1n9srQ+maHZq5wbYtAimanqmiIgIwMmTKsiLRnhy2eaeT07GiMPh7O/DF5fBBbdA\n9TKYe31y6/PiMfjLZyAnD877ebCmTkQkC3WX4F0IbAKeMrNbzewMYMD+17ItFmfbrpauE7xNi4LH\n8e9JX1AiIiIZrCQ/h+MPHtL7dgk9ycmH6bPhvTfCkr8G0y57Mu/moJfd2T+EQSP7Nh4RkX6kywTP\n3f/i7rOAqcBTwL8BlWb2KzN7f7oCTJftu1pwp+sEr64qeBw0Kn1BiYiIZLj3HjqcVVt3saq6vu8/\n/MTrYdKZ8Oj/CyptdmXLMnjyOzDlg3DkRX0fh4hIP5JMFc1d7n63u58LjAFeAb6a8sjSrL0HXpdr\n8OqqwCJQXJHGqERERDLb6VODdelPLuvjUTyASAQ+dAsUVwaFVzpbjxdrC6Zm5hXDuT/V1EwRyXrJ\nVtEEwN13uPscdz8jVQGFpbo+keB1OYK3KbjBqCKXiIjIbmOHFDFleCmPvFGVmi8oGgIfvQN2roe/\nXrfverz/+2nQT++DP1IRNBERepngDWTViRG8yq4SvPrNUDo8jRGJiIj0DxcdN5aX3t7BwjUH2MOu\nK2Nnwhk3wtK5MH/Onv1Vb8DT34fDPgSHX5ia7xYR6WeU4CW0J3jdjuCVatG2iIjI3mbPHMuQ4jx+\n8dSK1H3JidcFrQ8e/Q/Y8DK0tcBfrgnaLHzgR6n7XhGRfkYJXkJ1XTOlBTkU5EY7P6GuCkpHpDco\nERGRfqAoL4crT57A08ureWNDL/rW9UYkAhf8KrgX3385PP5NqHodzvkpFA9NzXeKiPRDSvASquu6\n6YEXa4Vd1VCiBE9ERKQzl544jtKCHH7xZApH8YqGwEfugNoN8MLNcOTH4NBzUvd9IiL9kBK8hOq6\n5q4raNYnKoNpBE9ERKRTgwpyuezE8fxjcRVvba5L3ReNPQ4+8EMYcxyc/YPUfY+ISD+lBC+hur6b\nEbz2HnhagyciItKlT548gcLcKL98emVqv2jGJ+Gqx6GwPLXfIyLSDynBS+h2imbdpuBRI3giIiJd\nGlKcx8XHH8TcVzeydltD2OGIiGQlJXhAQ0sb9c1tVJYWdH5CffsInhI8ERGR7nzqPQcTNeNXz6R4\nFE9ERDqlBA/YWtcCdNcioQosAsUVaYxKRESk/xk+qICPzhjDAy+tY9POxrDDERHJOkrwgOr6JqCH\nHnglwyHSRQsFERER2e2aUyYSd5jz7KqwQxERyTpK8IAttYkm511V0azbHCR4IiIi0qOxQ4o4f/oo\n7pm/lq31zWGHIyKSVZTgEVTQhB6maKqCpoiISNI+e+ohNLfFuf1fq8MORUQkqyjBI6igGbGg+len\n6japwIqIiEgvHFJZwgcOH8lv573NzobWsMMREckaSvAIEryhJflEI7bvwVgrNGxVgiciIr1mZmeZ\n2XIzW2FmN3Ry/CdmtiixvWlmNR2OxTocm5veyPvGZ0+bSH1zG3fNWxN2KCIiWSMn7AAyQXVdc9fr\n7+o3B49K8EREpBfMLArcDLwPWA8sMLO57r6k/Rx3//cO518PHN3hIxrdfXq64k2Fw0aVccbUSm59\nbhWzZo7tuh0klYCyAAAgAElEQVSRiIj0GY3gEazBqxzUzfo70Bo8ERHprZnACndf5e4twL3A+d2c\nPxu4Jy2RpdH/++ChNLfG+d7fl4YdiohIVlCCRw8jeHWbgkdV0RQRkd4ZDazr8Hp9Yt8+zGwcMAF4\nssPuAjNbaGYvmNkFXX2JmV2dOG9hdXV1X8TdpyZWlPDpUw7mL4s28vyKrWGHIyIy4GV9ghePO1vr\nm7uvoAkawRMRkVSaBTzg7rEO+8a5+wzg48BPzWxiZ2909znuPsPdZ1RUVKQj1l679rRDOGhIEf/5\n1zdobov1/AYREdlvKU3welpcnjjnIjNbYmaLzezuVMbTmZrGVlpj3n2CZ1EoHpbewEREpL/bAIzt\n8HpMYl9nZrHX9Ex335B4XAU8zTvX5/UrBblRvnX+Yayq3sWcZ9T8XEQklVKW4HVYXH42MA2YbWbT\n9jpnEvA14CR3Pwz4t1TF05XquiR64JUMh0g0jVGJiMgAsACYZGYTzCyPIInbpxqmmU0FyoF5HfaV\nm1l+4vkw4CRgyd7v7U9Om1LJ2YeP4BdPrWDttoawwxERGbBSOYKXzOLyTwE3u/sOAHffksJ4OrU7\nweuyimYVlGr9nYiI9I67twHXAY8CS4H73H2xmd1kZud1OHUWcK+7e4d9hwILzexV4Cng+x2rb/ZX\n3zh3GjkR4xtz3+CdP66IiPSVVLZJ6Gxx+fF7nTMZwMz+D4gC33T3f6Qwpn1U1zcBPYzgDT4ojRGJ\niMhA4e4PAw/vte8be73+Zifvex44IqXBhWBkWSH//r7JfOfvS/nHG1WcfYTWt4uI9LWwi6zkAJOA\nUwnKQ99qZoP3PimVFcJ6nqK5ST3wRERE+sjl7xrPoSMH8a2/LaG+uS3scEREBpxUJnjJLC5fD8x1\n91Z3Xw28SZDwvUMqK4RV1zVTmBulJL+Twcy2FmjYBiVK8ERERPpCTjTCdy44nKraJn762JthhyMi\nMuCkMsFLZnH5XwhG79oXkU8G0lpeq7ouaJFgZvserN8cPGoET0REpM8cO66c2TPHcsfza1i6qTbs\ncEREBpSUJXhJLi5/FNhmZksIFpF/2d23pSqmzmypUw88ERGRdPvqWVMpK8zla39+nbZYPOxwREQG\njJSuwXP3h919srtPdPfvJvZ9w93nJp67u3/B3ae5+xHufm8q4+lMdV1z1xU06zYFj6qiKSIi0qcG\nF+Vx47nTWLSuhp89uSLscEREBoywi6yErrq+mxG83VM0NYInIiLS186fPpoPHzOGXzz5Fi+sSusE\nHhGRASurE7zmthg1Da3dV9C0KBQNS29gIiIiWeJb5x/GuKHF/PsfF7FjV0vY4YiI9HtZneBtqw9u\nJN2uwSsZDpGsvkwiIiIpU5Kfw89mHc3W+ma++qfX1ABdROQAZXXm0t4Dr7K7BE8VNEVERFLqiDFl\nfPWsqfxzyWZ+/+LasMMREenXlODRwwie1t+JiIik3CdPmsCpUyr49kNLWFal1gkiIvsruxO8+p4S\nvE2qoCkiIpIGkYjxPx89ikEFuVx/9ys0tsTCDklEpF/K6gRvS22Q4A0t7iTBa2uGxu0awRMREUmT\nYSX5/ORjR/HWlnq+/fclYYcjItIvZXWCV13fRHlRLnk5nVyG3S0StAZPREQkXd49qYJrTpnI3S+u\n5ZHXN4UdjohIv5PdCV5dNz3w6qqCR43giYiIpNUX3z+Zo8YO5it/eo03N9eFHY6ISL+iBK+nBK9E\na/BERETSKTca4eaPH01BbpQr7ljAltqmsEMSEek3sjvBq2+mokQjeCIiIplmTHkRt192HDsaWvjk\nXQvY1dwWdkgiIv1C1iZ47k51XTOVgwo6P6FuE0RyoGhoegMTERERIOiPd/PHj2HJxlquv+cV2mLx\nsEMSEcl4WZvg1Te30dQa734Er2Q4RLL2EomIiITutKmVfPuCw3ly2RZunLsYdw87JBGRjJYTdgBh\n6bHJeX2VKmiKiIhkgIuPH8e67Y3c8sxKxg4p4ppTJoYdkohIxsraBG9LTwleXRUMOTiNEYmIiEhX\nvnLmFDbUNPL9R5YxanAh5x01KuyQREQyUtbOP+xxBK9ukypoioiIZIhIxPjhR45k5vghfOm+V5m/\nenvYIYmIZCQleJ2twWtrhsYdqqApIiKSQQpyo8z5xLGMGVLIp367kLfUI09EZB/Zm+DVN5MbNcoK\nc/c9uLtFgtbgiYjI/jOzs8xsuZmtMLMbOjl+uZlVm9mixHZVh2OXmdlbie2y9EaeuQYX5XHXFTPJ\ny4nwidvns7GmMeyQREQySvYmeHXNDCvJJxKxfQ8qwRMRkQNkZlHgZuBsYBow28ymdXLqH919emK7\nLfHeIcCNwPHATOBGMytPU+gZb+yQIu66Yib1TW184vb51DS0hB2SiEjGyOoEr7K7CpqgBE9ERA7E\nTGCFu69y9xbgXuD8JN97JvCYu2939x3AY8BZKYqzX5o2ahC3XjaDtdsb+OSdC2hsiYUdkohIRsjq\nBK/bCpqgNXgiInIgRgPrOrxen9i3tw+b2Wtm9oCZje3le7PaCQcP5WezpvPKuhquvftlWtUIXUQk\nixO8+u4SvE0QyYXCIekNSkREss3fgPHufiTBKN1dvf0AM7vazBaa2cLq6uo+DzDTnXX4SL59ftAI\n/Wt/fl2N0EUk62VlgheLO9vqmzuvoAlQtzlokRDJyssjIiJ9YwMwtsPrMYl9u7n7NndvTry8DTg2\n2fd2+Iw57j7D3WdUVFT0SeD9zSUnjOPf3juJB15az38/ujzscEREQpWVGcy2Xc3EvYceeFp/JyIi\nB2YBMMnMJphZHjALmNvxBDPruBbgPGBp4vmjwPvNrDxRXOX9iX3Shc+fMYmLjz+IXz29kt/8a3XY\n4YiIhCYn7ADC0HOT8yoYOjGNEYmIyEDj7m1mdh1BYhYFbnf3xWZ2E7DQ3ecCnzOz84A2YDtweeK9\n283s2wRJIsBN7q7O3t0wM246/3C272rh2w8tYXBhLh8+dkzYYYmIpJ0SvM7UbYLxJ6UxIhERGYjc\n/WHg4b32faPD868BX+vivbcDt6c0wAEmGjF+Oms6dXcu5MsPvEpxfg5nHa4ZOSKSXbJyimZ7gldZ\nWrDvwdYmaKrRFE0REZF+KD8nyq8vPZajxg7mc/e8wnNvZV/hGRHJbtmZ4NUHCd6wzoqs1KtFgoiI\nSH9WnJ/DnZfP5OCKYq7+7Uu89PaOsEMSEUmb7Ezw6popzc+hMC+678H2HnglGsETERHpr8qKcvnd\nlcczfFA+V9wxnyUba8MOSUQkLbI2weu5ybkSPBERkf6sojSf3191PCX5OXzi9hdZVV0fdkgiIimX\nlQnelrpmhvWY4GmKpoiISH83pryI3111PO5wyW0vsqGmMeyQRERSKisTvK3djuBtgkguFA1Jb1Ai\nIiKSEhMrSvjtlTOpa27jktteZP2OhrBDEhFJmaxM8KrrmqnorMAKQP3mYHqmWXqDEhERkZQ5bFQZ\nd15xHFvrm/nQL5/n9fU7ww5JRCQlsi7Ba2yJUdfc1v0IntbfiYiIDDjHjhvCnz7zLvKiES769Tye\nWLo57JBERPpcShM8MzvLzJab2Qozu6GT45ebWbWZLUpsV6UyHoCt9e098LpZg1cyPNVhiIiISAgm\nDy/lwWvfxaThJXzqtwv53bw1YYckItKnUpbgmVkUuBk4G5gGzDazaZ2c+kd3n57YbktVPO22JJqc\nd1tFUwVWREREBqzK0gLuvfoETp86nK//dTHfeWgJ8biHHZaISJ9I5QjeTGCFu69y9xbgXuD8FH5f\nUqq7S/BaG6GpRlM0RUREBriivBx+femxXP6u8dz2r9V89g8v09gSCzssEZEDlsoEbzSwrsPr9Yl9\ne/uwmb1mZg+Y2djOPsjMrjazhWa2sLq6+oCCqm1sxayLBE8tEkRERLJGNGJ887zD+Po503h0SRWz\nb32BHbtawg5LROSAhF1k5W/AeHc/EngMuKuzk9x9jrvPcPcZFRUVB/SFFx03lre+c3bnVTR3J3ha\ngyciIpItrjx5Ar+6+FiWbKpl9q0v7J7tIyLSH6UywdsAdByRG5PYt5u7b3P39v+K3gYcm8J4dsuJ\nRrDO2iDUrA0eB3U20CgiIiID1VmHj+D2y45jzbZdzJozj821TWGHJCKyX1KZ4C0AJpnZBDPLA2YB\nczueYGYd50KeByxNYTw92/AS5BbB0EmhhiEiIiLpd/KkYdx1xUyqdjZx0a/nsaGmMeyQRER6LWUJ\nnru3AdcBjxIkbve5+2Izu8nMzkuc9jkzW2xmrwKfAy5PVTxJWT8fRh0D0ZxQwxAREZFwHH/wUH53\n1fFs39XCRbfMY+22hrBDEhHplZSuwXP3h919srtPdPfvJvZ9w93nJp5/zd0Pc/ej3P00d1+Wyni6\n1doIVa/D2ONCC0FERETCd8xB5dx91Qnsamnjol/PY2V1fdghiYgkLewiK5lj4yKIt8EYJXgiIiLZ\n7ogxZdzzqRNojcX52K9f4M3NdWGHJCKSFCV47dYvCB7HzAw3DhEREckIh44cxB8/fQIRg4/9eh5P\nLN0cdkgiIj1Sgtdu/XwoHw8lB9aGQURERAaOQypLue/TJ1JRms+Vdy3kurtfVhsFEcloSvAA3GHd\nAk3PFBERkX2MH1bMQ9e/my+8bzL/XLyZ9/74Ge5fuA53Dzs0EZF9KMED2Lke6qs0PVNEREQ6lZcT\n4XNnTOLhz5/MpMoSvvzAa1z6m/mqsikiGUcJHnRYfzcj3DhEREQko7VP2fz2BYezaF0N7//pM8x5\ndiUtbfGwQxMRAZTgBdYvgJwCGHFE2JGIiMgAYmZnmdlyM1thZjd0cvwLZrbEzF4zsyfMbFyHYzEz\nW5TY5qY3culOJGJcesI4HvvCezj5kGF87+FlnPLDp7jj/1bT2BILOzwRyXJK8ADWzYdRR0M0N+xI\nRERkgDCzKHAzcDYwDZhtZtP2Ou0VYIa7Hwk8APx3h2ON7j49sZ2XlqClV0aWFXLrJ2Zw5xXHMba8\niG/9bQkn/eBJbn5qBTsbW8MOT0SylBK8tmaoek0FVkREpK/NBFa4+yp3bwHuBc7veIK7P+Xu7Yu4\nXgDGpDlGOUBmxqlTKrnvmhO5/5oTOWpMGT98dDknf/9JfvCPZaq4KSJppwRv06sQa1GCJyIifW00\nsK7D6/WJfV25Enikw+sCM1toZi+Y2QWpCFD61nHjh3DHFTN56PqTec+UCm55ZiUn/+BJ/uPB11m9\ndVfY4YlIlsgJO4DQ7S6wogRPRETCYWaXADOAUzrsHufuG8zsYOBJM3vd3Vd28t6rgasBDjrooLTE\nK907fHQZN3/8GFZV1zPn2VXcv3A9d89fy5nTRvCp9xzMsePKww5RRAYwjeCtmw9lY2HQyLAjERGR\ngWUDMLbD6zGJfe9gZu8F/gM4z913z+dz9w2Jx1XA08DRnX2Ju89x9xnuPqOioqLvopcDdnBFCd//\n8JH864bT+OypE5m3ahsf/tXzfORXz/Po4iricfXRE5G+pwRv/UKN3omISCosACaZ2QQzywNmAe+o\nhmlmRwO/JkjutnTYX25m+Ynnw4CTgCVpi1z6VGVpAV8+cyrP33A6N547jaraJj79u5d474+f4SeP\nvclLb++gLaY2CyLSN7J7imbtRqhdD2OvCzsSEREZYNy9zcyuAx4FosDt7r7YzG4CFrr7XOCHQAlw\nv5kBrE1UzDwU+LWZxQn+GPt9d1eC188V5+dwxUkTuPSEcTzyRhV3Pb+Gnz35Fv/7xFsMKsjh5EnD\nOGVyBe+ZXMHIssKwwxWRfiq7EzytvxMRkRRy94eBh/fa940Oz9/bxfueB9ScdYDKiUY496hRnHvU\nKGoaWvjXiq08s7yaZ9+q5uHXqwCYVFnCkWMGc0hlye5tbHkhOVFNvhKR7mV3grduPkTzYcSRYUci\nIiIiWWhwUR7nHDmKc44chbvz5uZ6nn2zmudWbOVfK6r508vrd5+bF40wYVgxEyuLOWxUGdPHDubI\nMWWUFqiPr4jskd0J3vqFMPIoyMkLOxIRERHJcmbGlBGlTBlRyqfeczAAtU2trNxSz4ot9ayormfl\nlnoWb6zdPdJnBhMrSpg+dvDubfLwUvJyNNInkq2yN8Fra4GNr8DMT4UdiYiIiEinBhXkcvRB5Rx9\n0DtbK+xsaOXV9TUsWhdsTy7bwgMvBaN9ORFjYkUJk0eUMnVEKVOGB0nj6MGFRCIWxo8hImmUvQne\n5tch1gxjZoQdiYiIiEivlBXl8p5EQRYAd2f9jkZeWVfDsk21LK+q45W1O/jbqxt3v6c4L8rBFSUc\nNKSIg4YWMS7xeNCQIkaWFRJV8icyIGRvgreuvcDKzHDjEBERETlAZsbYIUWMHVLEeUeN2r2/rqmV\nNzfX8+bmOpZX1bF66y6WbKrln0uqaI3t6cOXF40wqDAHMMzAIPEYvB5clMekypJgG17CIZWljB9a\npKIvIhkoexO89QugdBSUjQ47EhEREZGUKC3I5dhx5Rw77p1TPGNxZ2NNI2u3N7B2ewNvb2ugtqkV\ndwDHnWDDiTtsq2/m5bU7mNthRDA3ahw8rISxQ4ooL8qlvDiPwUW5lBflUV6Uy+CiPIrzcvD2zyMY\naYwnviM3GmFIcR5Di/MpzIum8aqIDGxZnODNh7FqjyAiIiLZJxrZM+J3Ui/et6u5jZXV9by1uZ63\nttTz1uY61u9o4I0NrexoaKG5bf8athfmRhlaksfQ4jyGluQzfFA+kyqDNYSTR5QyrCR/vz5XJBtl\nZ4JXtxlq1sLMq8OORERERKTfKM7P4cgxgzlyzOBOjze2xNjR0MKOhhZqGlppaIntme5pwVTS4LXR\n0hZnx64Wtu5qZnt9C9t2Bdvm2iZeWbuDexrW7f7cYSV5TB5eyuThpYwpL6SuqY2dja3UNLRQ09jK\njoZWdja0UN/cRsSMnIgRjRo5kQjRSPA6PzfKxGHFTE4Unpk8opRRZQWYae2hDCzZmeCt1/o7ERER\nkb5WmBelMK+QUYMLD+hz3J2t9S0sr6pj+eY63qyqY9nmOu5buI6GlhgApQU5lBcF00LLCnMZN6SI\nkoIc3J22mBOLOzF32uJOLObsamlj3qpt/PmVDbu/pzQ/h8kjSplYUUxeTmT3msNIIulrf54TsSBR\njEbIbX+MBvviDvF48D1xD763Le64O+VFeYwoKwi2QQVUluZr3aKkXJYmePMhkhv0wBMRERGRjGJm\nVJTmU1Gaz8mThu3eH487tU2tlOTn7HeitLOxlbc217Gsqm538Zmnl1fvTsqCtYIQTywcjLcniYnE\nLfmfgcSaxnfuqyjJZ0RZAWWFuZQW5FCcl0NJQQ6l+TkU5wfPc6MRomZEIkGCGY0YUTMs8TySSDzb\nE9BI4tycSJB45kYj5OVEyE0konnRCGVFueTnaK1jNsjSBG8hjDwScgvCjkREREREkhSJGIOL8g7o\nM8oKc5kxfggzxg/p9Xs9key1xZzWeJxYzIMELBokYNHIngQMYEdDK1U7m9hc28SmnU1U1TZRtbOR\nqtpmahtb2bSziV3NbdQ3tVHf0rZPQtiXIgajywuZMKyECUOLmDCsmAkVJRw8rJjKQfnkRSM9Tldt\naGljW30L23e1sG1XM1vrW9hW38LW+uY9W13wujUWZ3R5EaMHFzKmvOMW7BtclKvpsSmSfQlerBU2\nvAzHXh52JCIiIiLSj5hZYoQMCul5NGxIcR5DivOYNmpQj+fG405ja4z65jZa2uLEExVHY4mpn+3T\nP9tHF+OJR3cnFt9zXkssTmtbnNaY0xqL0xKL09IWp7qumdVbd7F66y5efnsH9c1t7/j+aMQoyo1S\nkBelMDdKUV6UgtwocXe21QcJXVNr50V0ivKiDCvJZ1hJHgcNLeKYceXkRo2NNY2s39HAC6u27fN9\n+TkRRiamr44sK0w8FlBRkk9+bjD6mNdhJDI/J0J+TpSCvAhFeTkU5kZ77N3YFovT3BansTXGzsbW\n3Vtt+/OGVupb2ijKzWFQYQ6DCnIZVJjLoIKc4LEwl2Elef1u5DP7ErzNi6GtUQ3ORURERCRjRCJG\ncWKaZqq5O9X1zazZ2sCq6nq27WqhoaWNxpY4ja1tNLbEaGiJ0dgaI2LGIRUlQbJaksew4vx3PB9W\nmkdRXvcxuzu1jW2s29HA+h2NbKhp3D2quammkfmrt7O5tqlXU2AhSBKL8qIU5eWQnxOhuS1Oc1uM\nptY4Ta2xpD4vLxqhJdZ99dfyolyGDyqgclABw0vzGT6ogIrSfMyguTX4zuC74zS3xmiJxcmNRijM\njVKYFyTLhXk5FCUS5/ZekqmSfQlee4GVsSqwIiIiIiLZx8yoLC2gsrSAmRN6P1V1f76vrCiXsqIy\nDh9d1uk58bizdVcwxbN91LE18diceN4+GtfY0hYkoIlEtKElRnNbjPycKPm5EQpyohTkJkb8ciMU\n5kUZVBAU4xlUGDy2b3k5Edpiceqa2qhtaqW2sf2xlZrGVqrrmtlc28Tm2ma21DWxvKqW6rpm9s4d\nzYKEsyA3Sm40QmssTkNLjJZOWodcc8pEbjh7aiouNZCNCd70i2H44VA2NuxIRERERESEYASzPelM\nt5xohPLiPMqLk1vfGYs723Y1EzHbPXU0N2qdrimMxT0xOronGS0vzu3rH+Edsi/ByyuCcSeGHYWI\niIiIiPRD0UQymuy5pQW5lBakNqnrSI04REREREREBggleCIiIiIiIgNEShM8MzvLzJab2Qozu6Gb\n8z5sZm5mKm0pIiIiIiKyn1KW4JlZFLgZOBuYBsw2s2mdnFcKfB54MVWxiIiIiIiIZINUjuDNBFa4\n+yp3bwHuBc7v5LxvAz8AmlIYi4iIiIiIyICXygRvNLCuw+v1iX27mdkxwFh3/3sK4xAREREREckK\noRVZMbMI8GPgi0mce7WZLTSzhdXV1akPTkREREREpB9KZYK3AejYTXxMYl+7UuBw4GkzWwOcAMzt\nrNCKu89x9xnuPqOioiKFIYuIiIiIiPRfqUzwFgCTzGyCmeUBs4C57Qfdfae7D3P38e4+HngBOM/d\nF6YwJhERERERkQErZQmeu7cB1wGPAkuB+9x9sZndZGbnpep7RUREREREspW5e9gx9IqZVQNvJ3Hq\nMGBrF8fKgJ19fCxVn5uKY+m+Nv3lWHfXJYx4MunYQP83cyDvHejXJlW/T8ka5+6am5+kDL5H9pdj\n+3tdUhVPJh3L5n8zPR3P5mszEK5LGN/ZF/fIru+P7j4gN2BhN8fm9PWxVH1uio6l9dr0o2NdXpcM\njDVjrk2GxRnG7++Avjap+n3SFu6mf7d9e10y8OfImGszEI7p2gzsfzOZdm36YgutimbI/paCY6n6\n3FTFmimxZNKxnmRSrJl0bTIpzjB+f1PxmQPhmPRfmfTvKJP+3Q6U/w+g/9b1/lgyx/v6OwfCse5k\nWpyZdG0OWL+bopksM1vo7vtU5BRdm67ounRN16Zrujad03XJbPrfp3O6Ll3Ttemark3ndF26lupr\nM5BH8OaEHUAG07XpnK5L13RtuqZr0zldl8ym/306p+vSNV2brunadE7XpWspvTYDdgRPREREREQk\n2wzkETwREREREZGsMiATPDM7y8yWm9kKM7sh7HjCZGa3m9kWM3ujw74hZvaYmb2VeCwPM8YwmNlY\nM3vKzJaY2WIz+3xiv66NWYGZzTezVxPX5luJ/RPM7MXE79UfzSwv7FjDYGZRM3vFzB5KvNZ1Acxs\njZm9bmaLzGxhYl/W/z5lGt0f99D9sXO6P3ZN98fu6f7YuTDujwMuwTOzKHAzcDYwDZhtZtPCjSpU\ndwJn7bXvBuAJd58EPJF4nW3agC+6+zTgBODaxL8TXRtoBk5396OA6cBZZnYC8APgJ+5+CLADuDLE\nGMP0eWBph9e6Lnuc5u7TOywc1+9TBtH9cR93ovtjZ3R/7Jruj93T/bFrab0/DrgED5gJrHD3Ve7e\nAtwLnB9yTKFx92eB7XvtPh+4K/H8LuCCtAaVAdx9k7u/nHheR/AfpNHo2uCB+sTL3MTmwOnAA4n9\nWXltzGwM8EHgtsRrQ9elO1n/+5RhdH/sQPfHzun+2DXdH7um+2OvpfT3aSAmeKOBdR1er0/skz2G\nu/umxPMqYHiYwYTNzMYDRwMvomsD7J5msQjYAjwGrARq3L0tcUq2/l79FPgKEE+8HoquSzsH/mlm\nL5nZ1Yl9+n3KLLo/9kz/ZjvQ/XFfuj92SffHrqX9/pjTlx8m/Y+7u5llbSlVMysB/gT8m7vXBn9w\nCmTztXH3GDDdzAYDDwJTQw4pdGZ2DrDF3V8ys1PDjicDnezuG8ysEnjMzJZ1PJjNv0/SP2X7v1nd\nHzun++O+dH/sUdrvjwNxBG8DMLbD6zGJfbLHZjMbCZB43BJyPKEws1yCm9cf3P3Pid26Nh24ew3w\nFHAiMNjM2v8olI2/VycB55nZGoKpbacD/4uuCwDuviHxuIXg//TMRL9PmUb3x57p3yy6PyZD98d3\n0P2xG2HcHwdigrcAmJSo3JMHzALmhhxTppkLXJZ4fhnw1xBjCUVibvhvgKXu/uMOh3RtzCoSf5nE\nzAqB9xGswXgK+EjitKy7Nu7+NXcf4+7jCf678qS7X0yWXxcAMys2s9L258D7gTfQ71Om0f2xZ1n/\nb1b3x67p/tg53R+7Ftb9cUA2OjezDxDMBY4Ct7v7d0MOKTRmdg9wKjAM2AzcCPwFuA84CHgbuMjd\n915oPqCZ2cnAc8Dr7Jkv/v8I1hlk+7U5kmDBb5Tgj0D3uftNZnYwwV/mhgCvAJe4e3N4kYYnMQXl\nS+5+jq4LJK7Bg4mXOcDd7v5dMxtKlv8+ZRrdH/fQ/bFzuj92TffHnun++E5h3R8HZIInIiIiIiKS\njQbiFE0REREREZGspARPRERERERkgFCCJyIiIiIiMkAowRMRERERERkglOCJiIiIiIgMEErwRNLI\nzGJmtqvo4yYAAAH3SURBVKjDdkMffvZ4M3ujrz5PREQknXSPFOkbOT2fIiJ9qNHdp4cdhIiISAbS\nPVKkD2gETyQDmNkaM/tvM3vdzOab2SGJ/ePN7Ekze83MnjCzgxL7h5vZg2b2amJ7V+KjomZ2q5kt\nNrN/mllhaD+UiIhIH9A9UqR3lOCJpFfhXtNPPtbh2E53PwL4BfDTxL6fA3e5+5HAH4CfJfb/DHjG\n3Y8CjgEWJ/ZPAm5298OAGuDDKf55RERE+orukSJ9wNw97BhEsoaZ1bt7SSf71wCnu/sqM8sFqtx9\nqJltBUa6e2ti/yZ3H2Zm1cAYd2/u8BnjgcfcfVLi9VeBXHf/Tup/MhERkQOje6RI39AInkjm8C6e\n90Zzh+cxtM5WREQGBt0jRZKkBE8kc3ysw+O8xPPngVmJ5xcDzyWePwF8BsDMomZWlq4gRUREQqB7\npEiS9JcLkfQqNLNFHV7/w93by0CXm9lrBH9hnJ3Ydz1wh5l9GagGrkjs/zwwx8yuJPgr5GeATSmP\nXkREJHV0jxTpA1qDJ5IBEusLZrj71rBjERERySS6R4r0jqZoioiIiIiIDBAawRMRERERERkgNIIn\nIiIiIiIyQCjBExERERERGSCU4ImIiIiIiAwQSvBEREREREQGCCV4IiIiIiIiA4QSPBERERERkQHi\n/wObf+imt6aihgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id3dsNcmX3EY",
        "colab_type": "text"
      },
      "source": [
        "### There is clear sign of overfitting. Adding droput"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8-LJ4m1IU7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "            x = Dropout(0.1)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "            x = Dropout(0.1)(x)\n",
        "        x = conv(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0_M1mEVYPdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "version = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGAdCg2sYeKx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d30f53c0-c32a-4c37-d01e-2f3ea49f9a42"
      },
      "source": [
        "def lr_schedule(epoch, lr):\n",
        "  return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "if version == 2:\n",
        "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
        "else:\n",
        "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "print(model_type)\n",
        "\n",
        "# Prepare model model saving directory.\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(zoom_range=0.0, \n",
        "                             horizontal_flip=False)\n",
        "\n",
        "\n",
        "# train the model\n",
        "start = time.time()\n",
        "# Train the model\n",
        "model_info = model.fit_generator(datagen.flow(x_train, y_train, batch_size = 128),\n",
        "                                 samples_per_epoch = x_train.shape[0], nb_epoch = 50, \n",
        "                                 callbacks=[LearningRateScheduler(lr_schedule, verbose=1)], #Added Rohan's Learning rate scheduler\n",
        "                                 validation_data = (x_test, y_test), verbose=1)\n",
        "\n",
        "                                            \n",
        "end = time.time()\n",
        "print (\"Model took %0.2f seconds to train\"%(end - start))\n",
        "# plot model history\n",
        "plot_model_history(model_info)\n",
        "# compute test accuracy"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_108 (Conv2D)             (None, 32, 32, 16)   448         input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_96 (BatchNo (None, 32, 32, 16)   64          conv2d_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_96 (Activation)      (None, 32, 32, 16)   0           batch_normalization_96[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 32, 32, 16)   0           activation_96[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_109 (Conv2D)             (None, 32, 32, 16)   272         dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_97 (BatchNo (None, 32, 32, 16)   64          conv2d_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_97 (Activation)      (None, 32, 32, 16)   0           batch_normalization_97[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 32, 32, 16)   0           activation_97[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_110 (Conv2D)             (None, 32, 32, 16)   2320        dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_98 (BatchNo (None, 32, 32, 16)   64          conv2d_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_98 (Activation)      (None, 32, 32, 16)   0           batch_normalization_98[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 32, 32, 16)   0           activation_98[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_112 (Conv2D)             (None, 32, 32, 64)   1088        dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_111 (Conv2D)             (None, 32, 32, 64)   1088        dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_40 (Add)                    (None, 32, 32, 64)   0           conv2d_112[0][0]                 \n",
            "                                                                 conv2d_111[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_99 (BatchNo (None, 32, 32, 64)   256         add_40[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_99 (Activation)      (None, 32, 32, 64)   0           batch_normalization_99[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 32, 32, 64)   0           activation_99[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_113 (Conv2D)             (None, 32, 32, 16)   1040        dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_100 (BatchN (None, 32, 32, 16)   64          conv2d_113[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_100 (Activation)     (None, 32, 32, 16)   0           batch_normalization_100[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 32, 32, 16)   0           activation_100[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_114 (Conv2D)             (None, 32, 32, 16)   2320        dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_101 (BatchN (None, 32, 32, 16)   64          conv2d_114[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_101 (Activation)     (None, 32, 32, 16)   0           batch_normalization_101[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 32, 32, 16)   0           activation_101[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_115 (Conv2D)             (None, 32, 32, 64)   1088        dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_41 (Add)                    (None, 32, 32, 64)   0           add_40[0][0]                     \n",
            "                                                                 conv2d_115[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_102 (BatchN (None, 32, 32, 64)   256         add_41[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_102 (Activation)     (None, 32, 32, 64)   0           batch_normalization_102[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 32, 32, 64)   0           activation_102[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_116 (Conv2D)             (None, 16, 16, 64)   4160        dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_103 (BatchN (None, 16, 16, 64)   256         conv2d_116[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_103 (Activation)     (None, 16, 16, 64)   0           batch_normalization_103[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 16, 16, 64)   0           activation_103[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_117 (Conv2D)             (None, 16, 16, 64)   36928       dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_104 (BatchN (None, 16, 16, 64)   256         conv2d_117[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_104 (Activation)     (None, 16, 16, 64)   0           batch_normalization_104[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 16, 16, 64)   0           activation_104[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_119 (Conv2D)             (None, 16, 16, 128)  8320        add_41[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_118 (Conv2D)             (None, 16, 16, 128)  8320        dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_42 (Add)                    (None, 16, 16, 128)  0           conv2d_119[0][0]                 \n",
            "                                                                 conv2d_118[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_105 (BatchN (None, 16, 16, 128)  512         add_42[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_105 (Activation)     (None, 16, 16, 128)  0           batch_normalization_105[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 16, 16, 128)  0           activation_105[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_120 (Conv2D)             (None, 16, 16, 64)   8256        dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_106 (BatchN (None, 16, 16, 64)   256         conv2d_120[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_106 (Activation)     (None, 16, 16, 64)   0           batch_normalization_106[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 16, 16, 64)   0           activation_106[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_121 (Conv2D)             (None, 16, 16, 64)   36928       dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_107 (BatchN (None, 16, 16, 64)   256         conv2d_121[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_107 (Activation)     (None, 16, 16, 64)   0           batch_normalization_107[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 16, 16, 64)   0           activation_107[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_122 (Conv2D)             (None, 16, 16, 128)  8320        dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_43 (Add)                    (None, 16, 16, 128)  0           add_42[0][0]                     \n",
            "                                                                 conv2d_122[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_108 (BatchN (None, 16, 16, 128)  512         add_43[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_108 (Activation)     (None, 16, 16, 128)  0           batch_normalization_108[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 16, 16, 128)  0           activation_108[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_123 (Conv2D)             (None, 8, 8, 128)    16512       dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_109 (BatchN (None, 8, 8, 128)    512         conv2d_123[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_109 (Activation)     (None, 8, 8, 128)    0           batch_normalization_109[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 8, 8, 128)    0           activation_109[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_124 (Conv2D)             (None, 8, 8, 128)    147584      dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_110 (BatchN (None, 8, 8, 128)    512         conv2d_124[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_110 (Activation)     (None, 8, 8, 128)    0           batch_normalization_110[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 8, 8, 128)    0           activation_110[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_126 (Conv2D)             (None, 8, 8, 256)    33024       add_43[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_125 (Conv2D)             (None, 8, 8, 256)    33024       dropout_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_44 (Add)                    (None, 8, 8, 256)    0           conv2d_126[0][0]                 \n",
            "                                                                 conv2d_125[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_111 (BatchN (None, 8, 8, 256)    1024        add_44[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_111 (Activation)     (None, 8, 8, 256)    0           batch_normalization_111[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 8, 8, 256)    0           activation_111[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_127 (Conv2D)             (None, 8, 8, 128)    32896       dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_112 (BatchN (None, 8, 8, 128)    512         conv2d_127[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_112 (Activation)     (None, 8, 8, 128)    0           batch_normalization_112[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 8, 8, 128)    0           activation_112[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_128 (Conv2D)             (None, 8, 8, 128)    147584      dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_113 (BatchN (None, 8, 8, 128)    512         conv2d_128[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_113 (Activation)     (None, 8, 8, 128)    0           batch_normalization_113[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, 8, 8, 128)    0           activation_113[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_129 (Conv2D)             (None, 8, 8, 256)    33024       dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_45 (Add)                    (None, 8, 8, 256)    0           add_44[0][0]                     \n",
            "                                                                 conv2d_129[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_114 (BatchN (None, 8, 8, 256)    1024        add_45[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_114 (Activation)     (None, 8, 8, 256)    0           batch_normalization_114[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, 1, 1, 256)    0           activation_114[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_6 (Flatten)             (None, 256)          0           average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 10)           2570        flatten_6[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 574,090\n",
            "Trainable params: 570,602\n",
            "Non-trainable params: 3,488\n",
            "__________________________________________________________________________________________________\n",
            "ResNet20v1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., callbacks=[<keras.ca..., validation_data=(array([[[..., verbose=1, steps_per_epoch=390, epochs=50)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "390/390 [==============================] - 61s 156ms/step - loss: 1.7420 - acc: 0.4773 - val_loss: 2.1353 - val_acc: 0.3991\n",
            "Epoch 2/50\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "390/390 [==============================] - 49s 126ms/step - loss: 1.2715 - acc: 0.6296 - val_loss: 1.2544 - val_acc: 0.6232\n",
            "Epoch 3/50\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "390/390 [==============================] - 49s 126ms/step - loss: 1.1195 - acc: 0.6765 - val_loss: 1.3978 - val_acc: 0.5799\n",
            "Epoch 4/50\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "390/390 [==============================] - 49s 126ms/step - loss: 1.0113 - acc: 0.7137 - val_loss: 1.0301 - val_acc: 0.7043\n",
            "Epoch 5/50\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "390/390 [==============================] - 49s 126ms/step - loss: 0.9243 - acc: 0.7439 - val_loss: 1.2228 - val_acc: 0.6578\n",
            "Epoch 6/50\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "390/390 [==============================] - 49s 126ms/step - loss: 0.8560 - acc: 0.7669 - val_loss: 0.9705 - val_acc: 0.7327\n",
            "Epoch 7/50\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "390/390 [==============================] - 49s 126ms/step - loss: 0.7990 - acc: 0.7865 - val_loss: 0.9517 - val_acc: 0.7384\n",
            "Epoch 8/50\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "390/390 [==============================] - 49s 127ms/step - loss: 0.7599 - acc: 0.7968 - val_loss: 0.8415 - val_acc: 0.7666\n",
            "Epoch 9/50\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "390/390 [==============================] - 49s 127ms/step - loss: 0.7243 - acc: 0.8092 - val_loss: 0.8778 - val_acc: 0.7601\n",
            "Epoch 10/50\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "390/390 [==============================] - 49s 127ms/step - loss: 0.6879 - acc: 0.8199 - val_loss: 0.7938 - val_acc: 0.7872\n",
            "Epoch 11/50\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "390/390 [==============================] - 49s 126ms/step - loss: 0.6564 - acc: 0.8320 - val_loss: 0.7270 - val_acc: 0.8056\n",
            "Epoch 12/50\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "390/390 [==============================] - 49s 127ms/step - loss: 0.6337 - acc: 0.8383 - val_loss: 0.7223 - val_acc: 0.8152\n",
            "Epoch 13/50\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "390/390 [==============================] - 49s 127ms/step - loss: 0.6105 - acc: 0.8460 - val_loss: 0.7178 - val_acc: 0.8102\n",
            "Epoch 14/50\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "390/390 [==============================] - 49s 127ms/step - loss: 0.5830 - acc: 0.8550 - val_loss: 0.6962 - val_acc: 0.8205\n",
            "Epoch 15/50\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.5611 - acc: 0.8611 - val_loss: 0.6652 - val_acc: 0.8335\n",
            "Epoch 16/50\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.5453 - acc: 0.8667 - val_loss: 0.7005 - val_acc: 0.8171\n",
            "Epoch 17/50\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.5285 - acc: 0.8713 - val_loss: 0.7142 - val_acc: 0.8133\n",
            "Epoch 18/50\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.5095 - acc: 0.8768 - val_loss: 0.6766 - val_acc: 0.8287\n",
            "Epoch 19/50\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.4975 - acc: 0.8803 - val_loss: 0.7119 - val_acc: 0.8147\n",
            "Epoch 20/50\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.4829 - acc: 0.8855 - val_loss: 0.6347 - val_acc: 0.8430\n",
            "Epoch 21/50\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0004065041.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.4672 - acc: 0.8900 - val_loss: 0.6384 - val_acc: 0.8437\n",
            "Epoch 22/50\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.000389661.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.4620 - acc: 0.8907 - val_loss: 0.6197 - val_acc: 0.8477\n",
            "Epoch 23/50\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0003741581.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.4464 - acc: 0.8962 - val_loss: 0.6190 - val_acc: 0.8502\n",
            "Epoch 24/50\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0003598417.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.4349 - acc: 0.9007 - val_loss: 0.6091 - val_acc: 0.8500\n",
            "Epoch 25/50\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0003465804.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.4218 - acc: 0.9046 - val_loss: 0.6100 - val_acc: 0.8525\n",
            "Epoch 26/50\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0003342618.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.4097 - acc: 0.9092 - val_loss: 0.6196 - val_acc: 0.8515\n",
            "Epoch 27/50\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0003227889.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.4068 - acc: 0.9089 - val_loss: 0.6319 - val_acc: 0.8465\n",
            "Epoch 28/50\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0003120774.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.3951 - acc: 0.9108 - val_loss: 0.6683 - val_acc: 0.8371\n",
            "Epoch 29/50\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.000302054.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.3902 - acc: 0.9141 - val_loss: 0.6288 - val_acc: 0.8501\n",
            "Epoch 30/50\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0002926544.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.3838 - acc: 0.9150 - val_loss: 0.6316 - val_acc: 0.8489\n",
            "Epoch 31/50\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0002838221.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.3734 - acc: 0.9194 - val_loss: 0.6078 - val_acc: 0.8558\n",
            "Epoch 32/50\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0002755074.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.3673 - acc: 0.9207 - val_loss: 0.6814 - val_acc: 0.8331\n",
            "Epoch 33/50\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.000267666.\n",
            "390/390 [==============================] - 50s 129ms/step - loss: 0.3582 - acc: 0.9240 - val_loss: 0.6910 - val_acc: 0.8339\n",
            "Epoch 34/50\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0002602585.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.3495 - acc: 0.9267 - val_loss: 0.6398 - val_acc: 0.8514\n",
            "Epoch 35/50\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.00025325.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.3521 - acc: 0.9247 - val_loss: 0.6287 - val_acc: 0.8563\n",
            "Epoch 36/50\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0002466091.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.3403 - acc: 0.9290 - val_loss: 0.6468 - val_acc: 0.8452\n",
            "Epoch 37/50\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0002403076.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.3320 - acc: 0.9322 - val_loss: 0.6113 - val_acc: 0.8573\n",
            "Epoch 38/50\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0002343201.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.3285 - acc: 0.9328 - val_loss: 0.6095 - val_acc: 0.8560\n",
            "Epoch 39/50\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0002286237.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.3248 - acc: 0.9332 - val_loss: 0.6136 - val_acc: 0.8565\n",
            "Epoch 40/50\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0002231977.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.3155 - acc: 0.9371 - val_loss: 0.5947 - val_acc: 0.8606\n",
            "Epoch 41/50\n",
            "\n",
            "Epoch 00041: LearningRateScheduler setting learning rate to 0.0002180233.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.3160 - acc: 0.9355 - val_loss: 0.6221 - val_acc: 0.8568\n",
            "Epoch 42/50\n",
            "\n",
            "Epoch 00042: LearningRateScheduler setting learning rate to 0.0002130833.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.3076 - acc: 0.9395 - val_loss: 0.5959 - val_acc: 0.8632\n",
            "Epoch 43/50\n",
            "\n",
            "Epoch 00043: LearningRateScheduler setting learning rate to 0.0002083623.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.3070 - acc: 0.9381 - val_loss: 0.6174 - val_acc: 0.8599\n",
            "Epoch 44/50\n",
            "\n",
            "Epoch 00044: LearningRateScheduler setting learning rate to 0.0002038459.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.3027 - acc: 0.9396 - val_loss: 0.6247 - val_acc: 0.8601\n",
            "Epoch 45/50\n",
            "\n",
            "Epoch 00045: LearningRateScheduler setting learning rate to 0.0001995211.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.2920 - acc: 0.9445 - val_loss: 0.6109 - val_acc: 0.8629\n",
            "Epoch 46/50\n",
            "\n",
            "Epoch 00046: LearningRateScheduler setting learning rate to 0.0001953761.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.2928 - acc: 0.9425 - val_loss: 0.6120 - val_acc: 0.8617\n",
            "Epoch 47/50\n",
            "\n",
            "Epoch 00047: LearningRateScheduler setting learning rate to 0.0001913998.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.2918 - acc: 0.9432 - val_loss: 0.6346 - val_acc: 0.8574\n",
            "Epoch 48/50\n",
            "\n",
            "Epoch 00048: LearningRateScheduler setting learning rate to 0.0001875821.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.2878 - acc: 0.9446 - val_loss: 0.6249 - val_acc: 0.8592\n",
            "Epoch 49/50\n",
            "\n",
            "Epoch 00049: LearningRateScheduler setting learning rate to 0.0001839137.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.2856 - acc: 0.9445 - val_loss: 0.6005 - val_acc: 0.8647\n",
            "Epoch 50/50\n",
            "\n",
            "Epoch 00050: LearningRateScheduler setting learning rate to 0.000180386.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.2787 - acc: 0.9478 - val_loss: 0.6176 - val_acc: 0.8591\n",
            "Model took 2494.93 seconds to train\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3gAAAFNCAYAAABSRs15AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB6kDWuZnKf4AAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBdlm9WpYrYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-gbaPGnpQvk",
        "colab_type": "text"
      },
      "source": [
        "## Highest accuracy is 86.47 after adding droput \n",
        "## Experimenting with default LR given in keras documentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gIYHC4Mr7Qi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch > 180:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 45:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 30:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 15:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbOJKsAJsKLT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "version = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYaTHeqJsPbD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "c48c0ebd-d9ef-4b2a-f36b-49501ca43153"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Dec  8 13:27:13 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P0    25W /  75W |   2431MiB /  7611MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dackKz7KsURC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "651f68ed-29e8-4d49-f812-7617c5a4944d"
      },
      "source": [
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "\n",
        "printm()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 10.8 GB  | Proc size: 4.1 GB\n",
            "GPU RAM Free: 7611MB | Used: 0MB | Util   0% | Total 7611MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k6DBKyHsdHD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "679f95af-a913-4766-b244-dc7c2e4a1d7f"
      },
      "source": [
        "if version == 2:\n",
        "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
        "else:\n",
        "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "print(model_type)\n",
        "\n",
        "# Prepare model model saving directory.\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(zoom_range=0.0, \n",
        "                             horizontal_flip=False)\n",
        "\n",
        "\n",
        "# train the model\n",
        "start = time.time()\n",
        "# Train the model\n",
        "model_info = model.fit_generator(datagen.flow(x_train, y_train, batch_size = 128),\n",
        "                                 samples_per_epoch = x_train.shape[0], nb_epoch = 50, \n",
        "                                 callbacks=[LearningRateScheduler(lr_schedule, verbose=1)], #Added Rohan's Learning rate scheduler\n",
        "                                 validation_data = (x_test, y_test), verbose=1)\n",
        "\n",
        "                                            \n",
        "end = time.time()\n",
        "print (\"Model took %0.2f seconds to train\"%(end - start))\n",
        "# plot model history\n",
        "plot_model_history(model_info)\n",
        "# compute test accuracy"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_130 (Conv2D)             (None, 32, 32, 16)   448         input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_115 (BatchN (None, 32, 32, 16)   64          conv2d_130[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_115 (Activation)     (None, 32, 32, 16)   0           batch_normalization_115[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 32, 32, 16)   0           activation_115[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_131 (Conv2D)             (None, 32, 32, 16)   272         dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_116 (BatchN (None, 32, 32, 16)   64          conv2d_131[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_116 (Activation)     (None, 32, 32, 16)   0           batch_normalization_116[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, 32, 32, 16)   0           activation_116[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_132 (Conv2D)             (None, 32, 32, 16)   2320        dropout_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_117 (BatchN (None, 32, 32, 16)   64          conv2d_132[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_117 (Activation)     (None, 32, 32, 16)   0           batch_normalization_117[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_23 (Dropout)            (None, 32, 32, 16)   0           activation_117[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_134 (Conv2D)             (None, 32, 32, 64)   1088        dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_133 (Conv2D)             (None, 32, 32, 64)   1088        dropout_23[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_46 (Add)                    (None, 32, 32, 64)   0           conv2d_134[0][0]                 \n",
            "                                                                 conv2d_133[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_118 (BatchN (None, 32, 32, 64)   256         add_46[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_118 (Activation)     (None, 32, 32, 64)   0           batch_normalization_118[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 32, 32, 64)   0           activation_118[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_135 (Conv2D)             (None, 32, 32, 16)   1040        dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_119 (BatchN (None, 32, 32, 16)   64          conv2d_135[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_119 (Activation)     (None, 32, 32, 16)   0           batch_normalization_119[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 32, 32, 16)   0           activation_119[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_136 (Conv2D)             (None, 32, 32, 16)   2320        dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_120 (BatchN (None, 32, 32, 16)   64          conv2d_136[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_120 (Activation)     (None, 32, 32, 16)   0           batch_normalization_120[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 32, 32, 16)   0           activation_120[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_137 (Conv2D)             (None, 32, 32, 64)   1088        dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_47 (Add)                    (None, 32, 32, 64)   0           add_46[0][0]                     \n",
            "                                                                 conv2d_137[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_121 (BatchN (None, 32, 32, 64)   256         add_47[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_121 (Activation)     (None, 32, 32, 64)   0           batch_normalization_121[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_27 (Dropout)            (None, 32, 32, 64)   0           activation_121[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_138 (Conv2D)             (None, 16, 16, 64)   4160        dropout_27[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_122 (BatchN (None, 16, 16, 64)   256         conv2d_138[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_122 (Activation)     (None, 16, 16, 64)   0           batch_normalization_122[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_28 (Dropout)            (None, 16, 16, 64)   0           activation_122[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_139 (Conv2D)             (None, 16, 16, 64)   36928       dropout_28[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_123 (BatchN (None, 16, 16, 64)   256         conv2d_139[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_123 (Activation)     (None, 16, 16, 64)   0           batch_normalization_123[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_29 (Dropout)            (None, 16, 16, 64)   0           activation_123[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_141 (Conv2D)             (None, 16, 16, 128)  8320        add_47[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_140 (Conv2D)             (None, 16, 16, 128)  8320        dropout_29[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_48 (Add)                    (None, 16, 16, 128)  0           conv2d_141[0][0]                 \n",
            "                                                                 conv2d_140[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_124 (BatchN (None, 16, 16, 128)  512         add_48[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_124 (Activation)     (None, 16, 16, 128)  0           batch_normalization_124[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_30 (Dropout)            (None, 16, 16, 128)  0           activation_124[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_142 (Conv2D)             (None, 16, 16, 64)   8256        dropout_30[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_125 (BatchN (None, 16, 16, 64)   256         conv2d_142[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_125 (Activation)     (None, 16, 16, 64)   0           batch_normalization_125[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_31 (Dropout)            (None, 16, 16, 64)   0           activation_125[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_143 (Conv2D)             (None, 16, 16, 64)   36928       dropout_31[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_126 (BatchN (None, 16, 16, 64)   256         conv2d_143[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_126 (Activation)     (None, 16, 16, 64)   0           batch_normalization_126[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_32 (Dropout)            (None, 16, 16, 64)   0           activation_126[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_144 (Conv2D)             (None, 16, 16, 128)  8320        dropout_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_49 (Add)                    (None, 16, 16, 128)  0           add_48[0][0]                     \n",
            "                                                                 conv2d_144[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_127 (BatchN (None, 16, 16, 128)  512         add_49[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_127 (Activation)     (None, 16, 16, 128)  0           batch_normalization_127[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_33 (Dropout)            (None, 16, 16, 128)  0           activation_127[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_145 (Conv2D)             (None, 8, 8, 128)    16512       dropout_33[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_128 (BatchN (None, 8, 8, 128)    512         conv2d_145[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_128 (Activation)     (None, 8, 8, 128)    0           batch_normalization_128[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_34 (Dropout)            (None, 8, 8, 128)    0           activation_128[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_146 (Conv2D)             (None, 8, 8, 128)    147584      dropout_34[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_129 (BatchN (None, 8, 8, 128)    512         conv2d_146[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_129 (Activation)     (None, 8, 8, 128)    0           batch_normalization_129[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_35 (Dropout)            (None, 8, 8, 128)    0           activation_129[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_148 (Conv2D)             (None, 8, 8, 256)    33024       add_49[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_147 (Conv2D)             (None, 8, 8, 256)    33024       dropout_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_50 (Add)                    (None, 8, 8, 256)    0           conv2d_148[0][0]                 \n",
            "                                                                 conv2d_147[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_130 (BatchN (None, 8, 8, 256)    1024        add_50[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_130 (Activation)     (None, 8, 8, 256)    0           batch_normalization_130[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_36 (Dropout)            (None, 8, 8, 256)    0           activation_130[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_149 (Conv2D)             (None, 8, 8, 128)    32896       dropout_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_131 (BatchN (None, 8, 8, 128)    512         conv2d_149[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_131 (Activation)     (None, 8, 8, 128)    0           batch_normalization_131[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_37 (Dropout)            (None, 8, 8, 128)    0           activation_131[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_150 (Conv2D)             (None, 8, 8, 128)    147584      dropout_37[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_132 (BatchN (None, 8, 8, 128)    512         conv2d_150[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_132 (Activation)     (None, 8, 8, 128)    0           batch_normalization_132[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 8, 8, 128)    0           activation_132[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_151 (Conv2D)             (None, 8, 8, 256)    33024       dropout_38[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_51 (Add)                    (None, 8, 8, 256)    0           add_50[0][0]                     \n",
            "                                                                 conv2d_151[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_133 (BatchN (None, 8, 8, 256)    1024        add_51[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_133 (Activation)     (None, 8, 8, 256)    0           batch_normalization_133[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_7 (AveragePoo (None, 1, 1, 256)    0           activation_133[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_7 (Flatten)             (None, 256)          0           average_pooling2d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 10)           2570        flatten_7[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 574,090\n",
            "Trainable params: 570,602\n",
            "Non-trainable params: 3,488\n",
            "__________________________________________________________________________________________________\n",
            "ResNet20v1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., callbacks=[<keras.ca..., validation_data=(array([[[..., verbose=1, steps_per_epoch=390, epochs=50)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.001.\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 1.8813 - acc: 0.4526 - val_loss: 1.9921 - val_acc: 0.4099\n",
            "Epoch 2/50\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.001.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 1.4492 - acc: 0.5881 - val_loss: 1.4822 - val_acc: 0.5575\n",
            "Epoch 3/50\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.001.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 1.2639 - acc: 0.6447 - val_loss: 1.3139 - val_acc: 0.6229\n",
            "Epoch 4/50\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.001.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 1.1582 - acc: 0.6796 - val_loss: 1.1932 - val_acc: 0.6629\n",
            "Epoch 5/50\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.001.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 1.0760 - acc: 0.7037 - val_loss: 1.4189 - val_acc: 0.5936\n",
            "Epoch 6/50\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.001.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 1.0128 - acc: 0.7221 - val_loss: 1.0877 - val_acc: 0.7041\n",
            "Epoch 7/50\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.001.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.9478 - acc: 0.7449 - val_loss: 1.2032 - val_acc: 0.6554\n",
            "Epoch 8/50\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.001.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.8959 - acc: 0.7627 - val_loss: 0.9178 - val_acc: 0.7600\n",
            "Epoch 9/50\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.001.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.8521 - acc: 0.7791 - val_loss: 1.1802 - val_acc: 0.6671\n",
            "Epoch 10/50\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.001.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.8199 - acc: 0.7889 - val_loss: 0.8921 - val_acc: 0.7656\n",
            "Epoch 11/50\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.001.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.7906 - acc: 0.7970 - val_loss: 0.8701 - val_acc: 0.7722\n",
            "Epoch 12/50\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.001.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.7668 - acc: 0.8037 - val_loss: 0.9518 - val_acc: 0.7382\n",
            "Epoch 13/50\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.001.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.7448 - acc: 0.8125 - val_loss: 0.9071 - val_acc: 0.7581\n",
            "Epoch 14/50\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.001.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.7262 - acc: 0.8189 - val_loss: 1.0567 - val_acc: 0.7153\n",
            "Epoch 15/50\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.001.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.7057 - acc: 0.8259 - val_loss: 0.9221 - val_acc: 0.7513\n",
            "Epoch 16/50\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.001.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.6904 - acc: 0.8295 - val_loss: 0.8600 - val_acc: 0.7851\n",
            "Epoch 17/50\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.0001.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.5877 - acc: 0.8668 - val_loss: 0.6770 - val_acc: 0.8325\n",
            "Epoch 18/50\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0001.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.5574 - acc: 0.8781 - val_loss: 0.6586 - val_acc: 0.8424\n",
            "Epoch 19/50\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0001.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.5416 - acc: 0.8814 - val_loss: 0.6576 - val_acc: 0.8419\n",
            "Epoch 20/50\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.0001.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.5295 - acc: 0.8855 - val_loss: 0.6511 - val_acc: 0.8436\n",
            "Epoch 21/50\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0001.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.5197 - acc: 0.8886 - val_loss: 0.6449 - val_acc: 0.8487\n",
            "Epoch 22/50\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.0001.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.5135 - acc: 0.8888 - val_loss: 0.6455 - val_acc: 0.8466\n",
            "Epoch 23/50\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0001.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.5018 - acc: 0.8919 - val_loss: 0.6441 - val_acc: 0.8490\n",
            "Epoch 24/50\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0001.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.4970 - acc: 0.8950 - val_loss: 0.6393 - val_acc: 0.8499\n",
            "Epoch 25/50\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0001.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.4885 - acc: 0.8971 - val_loss: 0.6349 - val_acc: 0.8521\n",
            "Epoch 26/50\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0001.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.4805 - acc: 0.8981 - val_loss: 0.6342 - val_acc: 0.8505\n",
            "Epoch 27/50\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0001.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.4750 - acc: 0.9003 - val_loss: 0.6498 - val_acc: 0.8467\n",
            "Epoch 28/50\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0001.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.4715 - acc: 0.8998 - val_loss: 0.6406 - val_acc: 0.8481\n",
            "Epoch 29/50\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.0001.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.4649 - acc: 0.9025 - val_loss: 0.6333 - val_acc: 0.8523\n",
            "Epoch 30/50\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0001.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.4574 - acc: 0.9049 - val_loss: 0.6330 - val_acc: 0.8500\n",
            "Epoch 31/50\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0001.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.4549 - acc: 0.9029 - val_loss: 0.6346 - val_acc: 0.8517\n",
            "Epoch 32/50\n",
            "Learning rate:  1e-05\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 1e-05.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.4380 - acc: 0.9106 - val_loss: 0.6274 - val_acc: 0.8526\n",
            "Epoch 33/50\n",
            "Learning rate:  1e-05\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 1e-05.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.4365 - acc: 0.9124 - val_loss: 0.6249 - val_acc: 0.8536\n",
            "Epoch 34/50\n",
            "Learning rate:  1e-05\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 1e-05.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.4354 - acc: 0.9114 - val_loss: 0.6233 - val_acc: 0.8541\n",
            "Epoch 35/50\n",
            "Learning rate:  1e-05\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 1e-05.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.4361 - acc: 0.9107 - val_loss: 0.6252 - val_acc: 0.8537\n",
            "Epoch 36/50\n",
            "Learning rate:  1e-05\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 1e-05.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.4306 - acc: 0.9122 - val_loss: 0.6247 - val_acc: 0.8541\n",
            "Epoch 37/50\n",
            "Learning rate:  1e-05\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 1e-05.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.4312 - acc: 0.9131 - val_loss: 0.6231 - val_acc: 0.8541\n",
            "Epoch 38/50\n",
            "Learning rate:  1e-05\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 1e-05.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.4300 - acc: 0.9136 - val_loss: 0.6235 - val_acc: 0.8542\n",
            "Epoch 39/50\n",
            "Learning rate:  1e-05\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 1e-05.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.4296 - acc: 0.9130 - val_loss: 0.6238 - val_acc: 0.8541\n",
            "Epoch 40/50\n",
            "Learning rate:  1e-05\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 1e-05.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.4303 - acc: 0.9130 - val_loss: 0.6248 - val_acc: 0.8542\n",
            "Epoch 41/50\n",
            "Learning rate:  1e-05\n",
            "\n",
            "Epoch 00041: LearningRateScheduler setting learning rate to 1e-05.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.4248 - acc: 0.9144 - val_loss: 0.6230 - val_acc: 0.8543\n",
            "Epoch 42/50\n",
            "Learning rate:  1e-05\n",
            "\n",
            "Epoch 00042: LearningRateScheduler setting learning rate to 1e-05.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.4274 - acc: 0.9149 - val_loss: 0.6254 - val_acc: 0.8528\n",
            "Epoch 43/50\n",
            "Learning rate:  1e-05\n",
            "\n",
            "Epoch 00043: LearningRateScheduler setting learning rate to 1e-05.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.4285 - acc: 0.9136 - val_loss: 0.6248 - val_acc: 0.8552\n",
            "Epoch 44/50\n",
            "Learning rate:  1e-05\n",
            "\n",
            "Epoch 00044: LearningRateScheduler setting learning rate to 1e-05.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.4262 - acc: 0.9156 - val_loss: 0.6237 - val_acc: 0.8549\n",
            "Epoch 45/50\n",
            "Learning rate:  1e-05\n",
            "\n",
            "Epoch 00045: LearningRateScheduler setting learning rate to 1e-05.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.4258 - acc: 0.9138 - val_loss: 0.6244 - val_acc: 0.8554\n",
            "Epoch 46/50\n",
            "Learning rate:  1e-05\n",
            "\n",
            "Epoch 00046: LearningRateScheduler setting learning rate to 1e-05.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.4238 - acc: 0.9151 - val_loss: 0.6238 - val_acc: 0.8556\n",
            "Epoch 47/50\n",
            "Learning rate:  1e-06\n",
            "\n",
            "Epoch 00047: LearningRateScheduler setting learning rate to 1e-06.\n",
            "390/390 [==============================] - 50s 127ms/step - loss: 0.4217 - acc: 0.9154 - val_loss: 0.6251 - val_acc: 0.8555\n",
            "Epoch 48/50\n",
            "Learning rate:  1e-06\n",
            "\n",
            "Epoch 00048: LearningRateScheduler setting learning rate to 1e-06.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.4223 - acc: 0.9160 - val_loss: 0.6247 - val_acc: 0.8548\n",
            "Epoch 49/50\n",
            "Learning rate:  1e-06\n",
            "\n",
            "Epoch 00049: LearningRateScheduler setting learning rate to 1e-06.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.4255 - acc: 0.9123 - val_loss: 0.6235 - val_acc: 0.8557\n",
            "Epoch 50/50\n",
            "Learning rate:  1e-06\n",
            "\n",
            "Epoch 00050: LearningRateScheduler setting learning rate to 1e-06.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.4184 - acc: 0.9162 - val_loss: 0.6237 - val_acc: 0.8555\n",
            "Model took 2501.10 seconds to train\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3gAAAFNCAYAAABSRs15AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIiIiJjhBI8ERERERGRMeL/AzD3yAaSGeVyAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmdmeJNcsncF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Highest accuracy after using defalu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxsfqTCo5uzP",
        "colab_type": "text"
      },
      "source": [
        "## Highest accuracy after using default LR = 85.57.\n",
        "## So It is better to stick with Rohan's LR\n",
        "## Trying batch size = 32 with dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYYQVlzr5-gj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training parameters\n",
        "batch_size = 32  # orig paper trained all networks with batch_size=128\n",
        "epochs = 200\n",
        "data_augmentation = True\n",
        "num_classes = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4bNuWrW6VOm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "74d0c79f-8ea2-4da0-82f3-4d884933bd3a"
      },
      "source": [
        "def lr_schedule(epoch, lr):\n",
        "  return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "if version == 2:\n",
        "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
        "else:\n",
        "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "print(model_type)\n",
        "\n",
        "# Prepare model model saving directory.\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(zoom_range=0.0, \n",
        "                             horizontal_flip=False)\n",
        "\n",
        "\n",
        "# train the model\n",
        "start = time.time()\n",
        "# Train the model\n",
        "model_info = model.fit_generator(datagen.flow(x_train, y_train, batch_size = 128),\n",
        "                                 samples_per_epoch = x_train.shape[0], nb_epoch = 50, \n",
        "                                 callbacks=[LearningRateScheduler(lr_schedule, verbose=1)], #Added Rohan's Learning rate scheduler\n",
        "                                 validation_data = (x_test, y_test), verbose=1)\n",
        "\n",
        "                                            \n",
        "end = time.time()\n",
        "print (\"Model took %0.2f seconds to train\"%(end - start))\n",
        "# plot model history\n",
        "plot_model_history(model_info)\n",
        "# compute test accuracy"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_152 (Conv2D)             (None, 32, 32, 16)   448         input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_134 (BatchN (None, 32, 32, 16)   64          conv2d_152[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_134 (Activation)     (None, 32, 32, 16)   0           batch_normalization_134[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_39 (Dropout)            (None, 32, 32, 16)   0           activation_134[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_153 (Conv2D)             (None, 32, 32, 16)   272         dropout_39[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_135 (BatchN (None, 32, 32, 16)   64          conv2d_153[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_135 (Activation)     (None, 32, 32, 16)   0           batch_normalization_135[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_40 (Dropout)            (None, 32, 32, 16)   0           activation_135[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_154 (Conv2D)             (None, 32, 32, 16)   2320        dropout_40[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_136 (BatchN (None, 32, 32, 16)   64          conv2d_154[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_136 (Activation)     (None, 32, 32, 16)   0           batch_normalization_136[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_41 (Dropout)            (None, 32, 32, 16)   0           activation_136[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_156 (Conv2D)             (None, 32, 32, 64)   1088        dropout_39[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_155 (Conv2D)             (None, 32, 32, 64)   1088        dropout_41[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_52 (Add)                    (None, 32, 32, 64)   0           conv2d_156[0][0]                 \n",
            "                                                                 conv2d_155[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_137 (BatchN (None, 32, 32, 64)   256         add_52[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_137 (Activation)     (None, 32, 32, 64)   0           batch_normalization_137[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_42 (Dropout)            (None, 32, 32, 64)   0           activation_137[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_157 (Conv2D)             (None, 32, 32, 16)   1040        dropout_42[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_138 (BatchN (None, 32, 32, 16)   64          conv2d_157[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_138 (Activation)     (None, 32, 32, 16)   0           batch_normalization_138[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_43 (Dropout)            (None, 32, 32, 16)   0           activation_138[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_158 (Conv2D)             (None, 32, 32, 16)   2320        dropout_43[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_139 (BatchN (None, 32, 32, 16)   64          conv2d_158[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_139 (Activation)     (None, 32, 32, 16)   0           batch_normalization_139[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_44 (Dropout)            (None, 32, 32, 16)   0           activation_139[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_159 (Conv2D)             (None, 32, 32, 64)   1088        dropout_44[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_53 (Add)                    (None, 32, 32, 64)   0           add_52[0][0]                     \n",
            "                                                                 conv2d_159[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_140 (BatchN (None, 32, 32, 64)   256         add_53[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_140 (Activation)     (None, 32, 32, 64)   0           batch_normalization_140[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_45 (Dropout)            (None, 32, 32, 64)   0           activation_140[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_160 (Conv2D)             (None, 16, 16, 64)   4160        dropout_45[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_141 (BatchN (None, 16, 16, 64)   256         conv2d_160[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_141 (Activation)     (None, 16, 16, 64)   0           batch_normalization_141[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_46 (Dropout)            (None, 16, 16, 64)   0           activation_141[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_161 (Conv2D)             (None, 16, 16, 64)   36928       dropout_46[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_142 (BatchN (None, 16, 16, 64)   256         conv2d_161[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_142 (Activation)     (None, 16, 16, 64)   0           batch_normalization_142[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_47 (Dropout)            (None, 16, 16, 64)   0           activation_142[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_163 (Conv2D)             (None, 16, 16, 128)  8320        add_53[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_162 (Conv2D)             (None, 16, 16, 128)  8320        dropout_47[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_54 (Add)                    (None, 16, 16, 128)  0           conv2d_163[0][0]                 \n",
            "                                                                 conv2d_162[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_143 (BatchN (None, 16, 16, 128)  512         add_54[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_143 (Activation)     (None, 16, 16, 128)  0           batch_normalization_143[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_48 (Dropout)            (None, 16, 16, 128)  0           activation_143[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_164 (Conv2D)             (None, 16, 16, 64)   8256        dropout_48[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_144 (BatchN (None, 16, 16, 64)   256         conv2d_164[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_144 (Activation)     (None, 16, 16, 64)   0           batch_normalization_144[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_49 (Dropout)            (None, 16, 16, 64)   0           activation_144[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_165 (Conv2D)             (None, 16, 16, 64)   36928       dropout_49[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_145 (BatchN (None, 16, 16, 64)   256         conv2d_165[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_145 (Activation)     (None, 16, 16, 64)   0           batch_normalization_145[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_50 (Dropout)            (None, 16, 16, 64)   0           activation_145[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_166 (Conv2D)             (None, 16, 16, 128)  8320        dropout_50[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_55 (Add)                    (None, 16, 16, 128)  0           add_54[0][0]                     \n",
            "                                                                 conv2d_166[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_146 (BatchN (None, 16, 16, 128)  512         add_55[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_146 (Activation)     (None, 16, 16, 128)  0           batch_normalization_146[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_51 (Dropout)            (None, 16, 16, 128)  0           activation_146[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_167 (Conv2D)             (None, 8, 8, 128)    16512       dropout_51[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_147 (BatchN (None, 8, 8, 128)    512         conv2d_167[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_147 (Activation)     (None, 8, 8, 128)    0           batch_normalization_147[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_52 (Dropout)            (None, 8, 8, 128)    0           activation_147[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_168 (Conv2D)             (None, 8, 8, 128)    147584      dropout_52[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_148 (BatchN (None, 8, 8, 128)    512         conv2d_168[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_148 (Activation)     (None, 8, 8, 128)    0           batch_normalization_148[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_53 (Dropout)            (None, 8, 8, 128)    0           activation_148[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_170 (Conv2D)             (None, 8, 8, 256)    33024       add_55[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_169 (Conv2D)             (None, 8, 8, 256)    33024       dropout_53[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_56 (Add)                    (None, 8, 8, 256)    0           conv2d_170[0][0]                 \n",
            "                                                                 conv2d_169[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_149 (BatchN (None, 8, 8, 256)    1024        add_56[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_149 (Activation)     (None, 8, 8, 256)    0           batch_normalization_149[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_54 (Dropout)            (None, 8, 8, 256)    0           activation_149[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_171 (Conv2D)             (None, 8, 8, 128)    32896       dropout_54[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_150 (BatchN (None, 8, 8, 128)    512         conv2d_171[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_150 (Activation)     (None, 8, 8, 128)    0           batch_normalization_150[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_55 (Dropout)            (None, 8, 8, 128)    0           activation_150[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_172 (Conv2D)             (None, 8, 8, 128)    147584      dropout_55[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_151 (BatchN (None, 8, 8, 128)    512         conv2d_172[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_151 (Activation)     (None, 8, 8, 128)    0           batch_normalization_151[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_56 (Dropout)            (None, 8, 8, 128)    0           activation_151[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_173 (Conv2D)             (None, 8, 8, 256)    33024       dropout_56[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_57 (Add)                    (None, 8, 8, 256)    0           add_56[0][0]                     \n",
            "                                                                 conv2d_173[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_152 (BatchN (None, 8, 8, 256)    1024        add_57[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_152 (Activation)     (None, 8, 8, 256)    0           batch_normalization_152[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_8 (AveragePoo (None, 1, 1, 256)    0           activation_152[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_8 (Flatten)             (None, 256)          0           average_pooling2d_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 10)           2570        flatten_8[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 574,090\n",
            "Trainable params: 570,602\n",
            "Non-trainable params: 3,488\n",
            "__________________________________________________________________________________________________\n",
            "ResNet20v1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., callbacks=[<keras.ca..., validation_data=(array([[[..., verbose=1, steps_per_epoch=390, epochs=50)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "390/390 [==============================] - 66s 169ms/step - loss: 1.7463 - acc: 0.4733 - val_loss: 1.8382 - val_acc: 0.4724\n",
            "Epoch 2/50\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 1.2934 - acc: 0.6212 - val_loss: 1.5150 - val_acc: 0.5610\n",
            "Epoch 3/50\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 1.1265 - acc: 0.6760 - val_loss: 1.2766 - val_acc: 0.6291\n",
            "Epoch 4/50\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 1.0226 - acc: 0.7103 - val_loss: 1.0903 - val_acc: 0.6782\n",
            "Epoch 5/50\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.9351 - acc: 0.7438 - val_loss: 1.0179 - val_acc: 0.7042\n",
            "Epoch 6/50\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.8629 - acc: 0.7661 - val_loss: 0.9638 - val_acc: 0.7436\n",
            "Epoch 7/50\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.8107 - acc: 0.7835 - val_loss: 0.8563 - val_acc: 0.7669\n",
            "Epoch 8/50\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.7580 - acc: 0.8010 - val_loss: 0.8189 - val_acc: 0.7797\n",
            "Epoch 9/50\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.7190 - acc: 0.8151 - val_loss: 0.7767 - val_acc: 0.7938\n",
            "Epoch 10/50\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.6864 - acc: 0.8250 - val_loss: 0.8183 - val_acc: 0.7763\n",
            "Epoch 11/50\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.6593 - acc: 0.8324 - val_loss: 0.8130 - val_acc: 0.7867\n",
            "Epoch 12/50\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.6311 - acc: 0.8396 - val_loss: 0.7346 - val_acc: 0.8155\n",
            "Epoch 13/50\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.6063 - acc: 0.8481 - val_loss: 0.7106 - val_acc: 0.8120\n",
            "Epoch 14/50\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.5859 - acc: 0.8531 - val_loss: 0.6950 - val_acc: 0.8124\n",
            "Epoch 15/50\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "390/390 [==============================] - 50s 129ms/step - loss: 0.5657 - acc: 0.8600 - val_loss: 0.7231 - val_acc: 0.8149\n",
            "Epoch 16/50\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.5448 - acc: 0.8668 - val_loss: 0.6504 - val_acc: 0.8326\n",
            "Epoch 17/50\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "390/390 [==============================] - 50s 129ms/step - loss: 0.5282 - acc: 0.8729 - val_loss: 0.6928 - val_acc: 0.8261\n",
            "Epoch 18/50\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.5146 - acc: 0.8768 - val_loss: 0.6867 - val_acc: 0.8266\n",
            "Epoch 19/50\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.4984 - acc: 0.8806 - val_loss: 0.6818 - val_acc: 0.8242\n",
            "Epoch 20/50\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.4815 - acc: 0.8869 - val_loss: 0.6396 - val_acc: 0.8396\n",
            "Epoch 21/50\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0004065041.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.4668 - acc: 0.8908 - val_loss: 0.6806 - val_acc: 0.8306\n",
            "Epoch 22/50\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.000389661.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.4565 - acc: 0.8933 - val_loss: 0.6533 - val_acc: 0.8376\n",
            "Epoch 23/50\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0003741581.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.4484 - acc: 0.8961 - val_loss: 0.6475 - val_acc: 0.8418\n",
            "Epoch 24/50\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0003598417.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.4346 - acc: 0.9001 - val_loss: 0.6412 - val_acc: 0.8431\n",
            "Epoch 25/50\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0003465804.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.4221 - acc: 0.9050 - val_loss: 0.7095 - val_acc: 0.8291\n",
            "Epoch 26/50\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0003342618.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.4137 - acc: 0.9064 - val_loss: 0.6253 - val_acc: 0.8497\n",
            "Epoch 27/50\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0003227889.\n",
            "390/390 [==============================] - 50s 129ms/step - loss: 0.4066 - acc: 0.9101 - val_loss: 0.6504 - val_acc: 0.8425\n",
            "Epoch 28/50\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0003120774.\n",
            "390/390 [==============================] - 50s 129ms/step - loss: 0.3944 - acc: 0.9144 - val_loss: 0.6590 - val_acc: 0.8400\n",
            "Epoch 29/50\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.000302054.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.3877 - acc: 0.9166 - val_loss: 0.6369 - val_acc: 0.8485\n",
            "Epoch 30/50\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0002926544.\n",
            "390/390 [==============================] - 50s 129ms/step - loss: 0.3791 - acc: 0.9186 - val_loss: 0.6185 - val_acc: 0.8543\n",
            "Epoch 31/50\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0002838221.\n",
            "390/390 [==============================] - 50s 129ms/step - loss: 0.3766 - acc: 0.9173 - val_loss: 0.6406 - val_acc: 0.8493\n",
            "Epoch 32/50\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0002755074.\n",
            "390/390 [==============================] - 50s 129ms/step - loss: 0.3624 - acc: 0.9230 - val_loss: 0.6177 - val_acc: 0.8557\n",
            "Epoch 33/50\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.000267666.\n",
            "390/390 [==============================] - 50s 129ms/step - loss: 0.3591 - acc: 0.9249 - val_loss: 0.6141 - val_acc: 0.8519\n",
            "Epoch 34/50\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0002602585.\n",
            "390/390 [==============================] - 50s 129ms/step - loss: 0.3516 - acc: 0.9272 - val_loss: 0.6372 - val_acc: 0.8511\n",
            "Epoch 35/50\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.00025325.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.3459 - acc: 0.9275 - val_loss: 0.6288 - val_acc: 0.8535\n",
            "Epoch 36/50\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0002466091.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.3405 - acc: 0.9312 - val_loss: 0.6115 - val_acc: 0.8545\n",
            "Epoch 37/50\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0002403076.\n",
            "390/390 [==============================] - 50s 129ms/step - loss: 0.3353 - acc: 0.9320 - val_loss: 0.6114 - val_acc: 0.8622\n",
            "Epoch 38/50\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0002343201.\n",
            "390/390 [==============================] - 50s 129ms/step - loss: 0.3312 - acc: 0.9324 - val_loss: 0.6159 - val_acc: 0.8612\n",
            "Epoch 39/50\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0002286237.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.3253 - acc: 0.9342 - val_loss: 0.6182 - val_acc: 0.8567\n",
            "Epoch 40/50\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0002231977.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.3164 - acc: 0.9375 - val_loss: 0.6303 - val_acc: 0.8601\n",
            "Epoch 41/50\n",
            "\n",
            "Epoch 00041: LearningRateScheduler setting learning rate to 0.0002180233.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.3133 - acc: 0.9383 - val_loss: 0.6214 - val_acc: 0.8592\n",
            "Epoch 42/50\n",
            "\n",
            "Epoch 00042: LearningRateScheduler setting learning rate to 0.0002130833.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.3095 - acc: 0.9397 - val_loss: 0.6377 - val_acc: 0.8529\n",
            "Epoch 43/50\n",
            "\n",
            "Epoch 00043: LearningRateScheduler setting learning rate to 0.0002083623.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.3063 - acc: 0.9396 - val_loss: 0.6172 - val_acc: 0.8565\n",
            "Epoch 44/50\n",
            "\n",
            "Epoch 00044: LearningRateScheduler setting learning rate to 0.0002038459.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.3035 - acc: 0.9397 - val_loss: 0.6040 - val_acc: 0.8639\n",
            "Epoch 45/50\n",
            "\n",
            "Epoch 00045: LearningRateScheduler setting learning rate to 0.0001995211.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.2981 - acc: 0.9425 - val_loss: 0.6099 - val_acc: 0.8603\n",
            "Epoch 46/50\n",
            "\n",
            "Epoch 00046: LearningRateScheduler setting learning rate to 0.0001953761.\n",
            "390/390 [==============================] - 50s 129ms/step - loss: 0.2927 - acc: 0.9441 - val_loss: 0.6096 - val_acc: 0.8613\n",
            "Epoch 47/50\n",
            "\n",
            "Epoch 00047: LearningRateScheduler setting learning rate to 0.0001913998.\n",
            "390/390 [==============================] - 50s 129ms/step - loss: 0.2891 - acc: 0.9461 - val_loss: 0.6194 - val_acc: 0.8569\n",
            "Epoch 48/50\n",
            "\n",
            "Epoch 00048: LearningRateScheduler setting learning rate to 0.0001875821.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.2838 - acc: 0.9467 - val_loss: 0.6082 - val_acc: 0.8629\n",
            "Epoch 49/50\n",
            "\n",
            "Epoch 00049: LearningRateScheduler setting learning rate to 0.0001839137.\n",
            "390/390 [==============================] - 50s 128ms/step - loss: 0.2805 - acc: 0.9474 - val_loss: 0.6482 - val_acc: 0.8583\n",
            "Epoch 50/50\n",
            "\n",
            "Epoch 00050: LearningRateScheduler setting learning rate to 0.000180386.\n",
            "390/390 [==============================] - 50s 129ms/step - loss: 0.2806 - acc: 0.9472 - val_loss: 0.6214 - val_acc: 0.8607\n",
            "Model took 2523.41 seconds to train\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3gAAAFNCAYAAABSRs15AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJZTgiYiIiIiI9BL/H55HvFzexfuUAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oami8aIN6gXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_MEj-pHFc9a",
        "colab_type": "text"
      },
      "source": [
        "## It seems batch size of 32 has no big effect. The highest max validation accuracy is 86.39 \n",
        "## Trying 32 filters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1ZHlTKyFrUG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_v2(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 2 Model builder [b]\n",
        "\n",
        "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
        "    bottleneck layer\n",
        "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
        "    Second and onwards shortcut connection is identity.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filter maps is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same filter map sizes.\n",
        "    Features maps sizes:\n",
        "    conv1  : 32x32,  16\n",
        "    stage 0: 32x32,  64\n",
        "    stage 1: 16x16, 128\n",
        "    stage 2:  8x8,  256\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
        "    # Start model definition.\n",
        "    num_filters_in = 32\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "\n",
        "    # Instantiate the stack of residual units\n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                if res_block == 0:  # first layer and first stage\n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "                if res_block == 0:  # first layer but not first stage\n",
        "                    strides = 2    # downsample\n",
        "\n",
        "            # bottleneck residual unit\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if res_block == 0:\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnlHf82tGKGf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training parameters\n",
        "batch_size = 128  # orig paper trained all networks with batch_size=128\n",
        "epochs = 200\n",
        "data_augmentation = True\n",
        "num_classes = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDwwsBC0F3ML",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "69885675-96b3-49f5-e6b0-0dd4fb6a5e29"
      },
      "source": [
        "def lr_schedule(epoch, lr):\n",
        "  return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "if version == 2:\n",
        "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
        "else:\n",
        "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "print(model_type)\n",
        "\n",
        "# Prepare model model saving directory.\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(zoom_range=0.0, \n",
        "                             horizontal_flip=False)\n",
        "\n",
        "\n",
        "# train the model\n",
        "start = time.time()\n",
        "# Train the model\n",
        "model_info = model.fit_generator(datagen.flow(x_train, y_train, batch_size = 128),\n",
        "                                 samples_per_epoch = x_train.shape[0], nb_epoch = 50, \n",
        "                                 callbacks=[LearningRateScheduler(lr_schedule, verbose=1)], #Added Rohan's Learning rate scheduler\n",
        "                                 validation_data = (x_test, y_test), verbose=1)\n",
        "\n",
        "                                            \n",
        "end = time.time()\n",
        "print (\"Model took %0.2f seconds to train\"%(end - start))\n",
        "# plot model history\n",
        "plot_model_history(model_info)\n",
        "# compute test accuracy"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_9 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_174 (Conv2D)             (None, 32, 32, 32)   896         input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_153 (BatchN (None, 32, 32, 32)   128         conv2d_174[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_153 (Activation)     (None, 32, 32, 32)   0           batch_normalization_153[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_57 (Dropout)            (None, 32, 32, 32)   0           activation_153[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_175 (Conv2D)             (None, 32, 32, 32)   1056        dropout_57[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_154 (BatchN (None, 32, 32, 32)   128         conv2d_175[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_154 (Activation)     (None, 32, 32, 32)   0           batch_normalization_154[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_58 (Dropout)            (None, 32, 32, 32)   0           activation_154[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_176 (Conv2D)             (None, 32, 32, 32)   9248        dropout_58[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_155 (BatchN (None, 32, 32, 32)   128         conv2d_176[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_155 (Activation)     (None, 32, 32, 32)   0           batch_normalization_155[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_59 (Dropout)            (None, 32, 32, 32)   0           activation_155[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_178 (Conv2D)             (None, 32, 32, 128)  4224        dropout_57[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_177 (Conv2D)             (None, 32, 32, 128)  4224        dropout_59[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_58 (Add)                    (None, 32, 32, 128)  0           conv2d_178[0][0]                 \n",
            "                                                                 conv2d_177[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_156 (BatchN (None, 32, 32, 128)  512         add_58[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_156 (Activation)     (None, 32, 32, 128)  0           batch_normalization_156[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_60 (Dropout)            (None, 32, 32, 128)  0           activation_156[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_179 (Conv2D)             (None, 32, 32, 32)   4128        dropout_60[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_157 (BatchN (None, 32, 32, 32)   128         conv2d_179[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_157 (Activation)     (None, 32, 32, 32)   0           batch_normalization_157[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_61 (Dropout)            (None, 32, 32, 32)   0           activation_157[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_180 (Conv2D)             (None, 32, 32, 32)   9248        dropout_61[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_158 (BatchN (None, 32, 32, 32)   128         conv2d_180[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_158 (Activation)     (None, 32, 32, 32)   0           batch_normalization_158[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_62 (Dropout)            (None, 32, 32, 32)   0           activation_158[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_181 (Conv2D)             (None, 32, 32, 128)  4224        dropout_62[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_59 (Add)                    (None, 32, 32, 128)  0           add_58[0][0]                     \n",
            "                                                                 conv2d_181[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_159 (BatchN (None, 32, 32, 128)  512         add_59[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_159 (Activation)     (None, 32, 32, 128)  0           batch_normalization_159[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_63 (Dropout)            (None, 32, 32, 128)  0           activation_159[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_182 (Conv2D)             (None, 16, 16, 128)  16512       dropout_63[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_160 (BatchN (None, 16, 16, 128)  512         conv2d_182[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_160 (Activation)     (None, 16, 16, 128)  0           batch_normalization_160[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_64 (Dropout)            (None, 16, 16, 128)  0           activation_160[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_183 (Conv2D)             (None, 16, 16, 128)  147584      dropout_64[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_161 (BatchN (None, 16, 16, 128)  512         conv2d_183[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_161 (Activation)     (None, 16, 16, 128)  0           batch_normalization_161[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_65 (Dropout)            (None, 16, 16, 128)  0           activation_161[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_185 (Conv2D)             (None, 16, 16, 256)  33024       add_59[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_184 (Conv2D)             (None, 16, 16, 256)  33024       dropout_65[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_60 (Add)                    (None, 16, 16, 256)  0           conv2d_185[0][0]                 \n",
            "                                                                 conv2d_184[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_162 (BatchN (None, 16, 16, 256)  1024        add_60[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_162 (Activation)     (None, 16, 16, 256)  0           batch_normalization_162[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_66 (Dropout)            (None, 16, 16, 256)  0           activation_162[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_186 (Conv2D)             (None, 16, 16, 128)  32896       dropout_66[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_163 (BatchN (None, 16, 16, 128)  512         conv2d_186[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_163 (Activation)     (None, 16, 16, 128)  0           batch_normalization_163[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_67 (Dropout)            (None, 16, 16, 128)  0           activation_163[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_187 (Conv2D)             (None, 16, 16, 128)  147584      dropout_67[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_164 (BatchN (None, 16, 16, 128)  512         conv2d_187[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_164 (Activation)     (None, 16, 16, 128)  0           batch_normalization_164[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_68 (Dropout)            (None, 16, 16, 128)  0           activation_164[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_188 (Conv2D)             (None, 16, 16, 256)  33024       dropout_68[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_61 (Add)                    (None, 16, 16, 256)  0           add_60[0][0]                     \n",
            "                                                                 conv2d_188[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_165 (BatchN (None, 16, 16, 256)  1024        add_61[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_165 (Activation)     (None, 16, 16, 256)  0           batch_normalization_165[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_69 (Dropout)            (None, 16, 16, 256)  0           activation_165[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_189 (Conv2D)             (None, 8, 8, 256)    65792       dropout_69[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_166 (BatchN (None, 8, 8, 256)    1024        conv2d_189[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_166 (Activation)     (None, 8, 8, 256)    0           batch_normalization_166[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_70 (Dropout)            (None, 8, 8, 256)    0           activation_166[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_190 (Conv2D)             (None, 8, 8, 256)    590080      dropout_70[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_167 (BatchN (None, 8, 8, 256)    1024        conv2d_190[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_167 (Activation)     (None, 8, 8, 256)    0           batch_normalization_167[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_71 (Dropout)            (None, 8, 8, 256)    0           activation_167[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_192 (Conv2D)             (None, 8, 8, 512)    131584      add_61[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_191 (Conv2D)             (None, 8, 8, 512)    131584      dropout_71[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_62 (Add)                    (None, 8, 8, 512)    0           conv2d_192[0][0]                 \n",
            "                                                                 conv2d_191[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_168 (BatchN (None, 8, 8, 512)    2048        add_62[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_168 (Activation)     (None, 8, 8, 512)    0           batch_normalization_168[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_72 (Dropout)            (None, 8, 8, 512)    0           activation_168[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_193 (Conv2D)             (None, 8, 8, 256)    131328      dropout_72[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_169 (BatchN (None, 8, 8, 256)    1024        conv2d_193[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_169 (Activation)     (None, 8, 8, 256)    0           batch_normalization_169[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_73 (Dropout)            (None, 8, 8, 256)    0           activation_169[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_194 (Conv2D)             (None, 8, 8, 256)    590080      dropout_73[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_170 (BatchN (None, 8, 8, 256)    1024        conv2d_194[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_170 (Activation)     (None, 8, 8, 256)    0           batch_normalization_170[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_74 (Dropout)            (None, 8, 8, 256)    0           activation_170[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_195 (Conv2D)             (None, 8, 8, 512)    131584      dropout_74[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_63 (Add)                    (None, 8, 8, 512)    0           add_62[0][0]                     \n",
            "                                                                 conv2d_195[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_171 (BatchN (None, 8, 8, 512)    2048        add_63[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_171 (Activation)     (None, 8, 8, 512)    0           batch_normalization_171[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_9 (AveragePoo (None, 1, 1, 512)    0           activation_171[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_9 (Flatten)             (None, 512)          0           average_pooling2d_9[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 10)           5130        flatten_9[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 2,272,010\n",
            "Trainable params: 2,265,034\n",
            "Non-trainable params: 6,976\n",
            "__________________________________________________________________________________________________\n",
            "ResNet20v1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., callbacks=[<keras.ca..., validation_data=(array([[[..., verbose=1, steps_per_epoch=390, epochs=50)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "390/390 [==============================] - 112s 287ms/step - loss: 1.8553 - acc: 0.4995 - val_loss: 1.9791 - val_acc: 0.4473\n",
            "Epoch 2/50\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 1.3004 - acc: 0.6374 - val_loss: 1.5224 - val_acc: 0.5731\n",
            "Epoch 3/50\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 1.1256 - acc: 0.6920 - val_loss: 1.3930 - val_acc: 0.6203\n",
            "Epoch 4/50\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 1.0136 - acc: 0.7246 - val_loss: 1.2205 - val_acc: 0.6579\n",
            "Epoch 5/50\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.9266 - acc: 0.7576 - val_loss: 0.9815 - val_acc: 0.7386\n",
            "Epoch 6/50\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 0.8594 - acc: 0.7791 - val_loss: 1.0759 - val_acc: 0.7198\n",
            "Epoch 7/50\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.8015 - acc: 0.7983 - val_loss: 0.8937 - val_acc: 0.7593\n",
            "Epoch 8/50\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.7514 - acc: 0.8142 - val_loss: 0.8678 - val_acc: 0.7745\n",
            "Epoch 9/50\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 0.7082 - acc: 0.8271 - val_loss: 1.1211 - val_acc: 0.7231\n",
            "Epoch 10/50\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.6731 - acc: 0.8393 - val_loss: 0.7965 - val_acc: 0.8049\n",
            "Epoch 11/50\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.6389 - acc: 0.8503 - val_loss: 0.7674 - val_acc: 0.8135\n",
            "Epoch 12/50\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.6101 - acc: 0.8572 - val_loss: 0.7810 - val_acc: 0.8055\n",
            "Epoch 13/50\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.5773 - acc: 0.8697 - val_loss: 0.7975 - val_acc: 0.8049\n",
            "Epoch 14/50\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.5534 - acc: 0.8754 - val_loss: 0.7054 - val_acc: 0.8282\n",
            "Epoch 15/50\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.5294 - acc: 0.8832 - val_loss: 0.7334 - val_acc: 0.8208\n",
            "Epoch 16/50\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.5033 - acc: 0.8932 - val_loss: 0.7123 - val_acc: 0.8282\n",
            "Epoch 17/50\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "390/390 [==============================] - 93s 237ms/step - loss: 0.4830 - acc: 0.8977 - val_loss: 0.7165 - val_acc: 0.8315\n",
            "Epoch 18/50\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.4665 - acc: 0.9045 - val_loss: 0.7609 - val_acc: 0.8189\n",
            "Epoch 19/50\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.4461 - acc: 0.9099 - val_loss: 0.7619 - val_acc: 0.8268\n",
            "Epoch 20/50\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.4306 - acc: 0.9148 - val_loss: 0.8019 - val_acc: 0.8242\n",
            "Epoch 21/50\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0004065041.\n",
            "390/390 [==============================] - 93s 237ms/step - loss: 0.4134 - acc: 0.9213 - val_loss: 0.7056 - val_acc: 0.8306\n",
            "Epoch 22/50\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.000389661.\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.4023 - acc: 0.9229 - val_loss: 0.6907 - val_acc: 0.8447\n",
            "Epoch 23/50\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0003741581.\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.3868 - acc: 0.9288 - val_loss: 0.6888 - val_acc: 0.8535\n",
            "Epoch 24/50\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0003598417.\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.3747 - acc: 0.9330 - val_loss: 0.7298 - val_acc: 0.8375\n",
            "Epoch 25/50\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0003465804.\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.3587 - acc: 0.9378 - val_loss: 0.6850 - val_acc: 0.8474\n",
            "Epoch 26/50\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0003342618.\n",
            "390/390 [==============================] - 93s 237ms/step - loss: 0.3535 - acc: 0.9382 - val_loss: 0.6958 - val_acc: 0.8524\n",
            "Epoch 27/50\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0003227889.\n",
            "390/390 [==============================] - 93s 237ms/step - loss: 0.3452 - acc: 0.9416 - val_loss: 0.7578 - val_acc: 0.8308\n",
            "Epoch 28/50\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0003120774.\n",
            "390/390 [==============================] - 93s 237ms/step - loss: 0.3345 - acc: 0.9451 - val_loss: 0.6779 - val_acc: 0.8564\n",
            "Epoch 29/50\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.000302054.\n",
            "390/390 [==============================] - 93s 237ms/step - loss: 0.3224 - acc: 0.9481 - val_loss: 0.7146 - val_acc: 0.8513\n",
            "Epoch 30/50\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0002926544.\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.3155 - acc: 0.9514 - val_loss: 0.6700 - val_acc: 0.8607\n",
            "Epoch 31/50\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0002838221.\n",
            "390/390 [==============================] - 93s 237ms/step - loss: 0.3068 - acc: 0.9535 - val_loss: 0.7079 - val_acc: 0.8534\n",
            "Epoch 32/50\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0002755074.\n",
            "390/390 [==============================] - 93s 237ms/step - loss: 0.3049 - acc: 0.9533 - val_loss: 0.7352 - val_acc: 0.8486\n",
            "Epoch 33/50\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.000267666.\n",
            "390/390 [==============================] - 92s 237ms/step - loss: 0.2939 - acc: 0.9576 - val_loss: 0.7270 - val_acc: 0.8507\n",
            "Epoch 34/50\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0002602585.\n",
            "390/390 [==============================] - 93s 237ms/step - loss: 0.2869 - acc: 0.9592 - val_loss: 0.7272 - val_acc: 0.8483\n",
            "Epoch 35/50\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.00025325.\n",
            "390/390 [==============================] - 93s 237ms/step - loss: 0.2837 - acc: 0.9597 - val_loss: 0.7155 - val_acc: 0.8504\n",
            "Epoch 36/50\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0002466091.\n",
            "390/390 [==============================] - 92s 237ms/step - loss: 0.2781 - acc: 0.9615 - val_loss: 0.6775 - val_acc: 0.8620\n",
            "Epoch 37/50\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0002403076.\n",
            "390/390 [==============================] - 92s 237ms/step - loss: 0.2674 - acc: 0.9651 - val_loss: 0.7026 - val_acc: 0.8613\n",
            "Epoch 38/50\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0002343201.\n",
            "390/390 [==============================] - 92s 237ms/step - loss: 0.2637 - acc: 0.9654 - val_loss: 0.8061 - val_acc: 0.8365\n",
            "Epoch 39/50\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0002286237.\n",
            "390/390 [==============================] - 92s 237ms/step - loss: 0.2581 - acc: 0.9669 - val_loss: 0.6992 - val_acc: 0.8565\n",
            "Epoch 40/50\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0002231977.\n",
            "390/390 [==============================] - 92s 237ms/step - loss: 0.2562 - acc: 0.9680 - val_loss: 0.6758 - val_acc: 0.8645\n",
            "Epoch 41/50\n",
            "\n",
            "Epoch 00041: LearningRateScheduler setting learning rate to 0.0002180233.\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 0.2508 - acc: 0.9698 - val_loss: 0.6384 - val_acc: 0.8712\n",
            "Epoch 42/50\n",
            "\n",
            "Epoch 00042: LearningRateScheduler setting learning rate to 0.0002130833.\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.2463 - acc: 0.9706 - val_loss: 0.7219 - val_acc: 0.8630\n",
            "Epoch 43/50\n",
            "\n",
            "Epoch 00043: LearningRateScheduler setting learning rate to 0.0002083623.\n",
            "390/390 [==============================] - 93s 237ms/step - loss: 0.2423 - acc: 0.9724 - val_loss: 0.6643 - val_acc: 0.8717\n",
            "Epoch 44/50\n",
            "\n",
            "Epoch 00044: LearningRateScheduler setting learning rate to 0.0002038459.\n",
            "390/390 [==============================] - 93s 237ms/step - loss: 0.2376 - acc: 0.9725 - val_loss: 0.6907 - val_acc: 0.8688\n",
            "Epoch 45/50\n",
            "\n",
            "Epoch 00045: LearningRateScheduler setting learning rate to 0.0001995211.\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.2350 - acc: 0.9736 - val_loss: 0.7212 - val_acc: 0.8631\n",
            "Epoch 46/50\n",
            "\n",
            "Epoch 00046: LearningRateScheduler setting learning rate to 0.0001953761.\n",
            "390/390 [==============================] - 93s 237ms/step - loss: 0.2285 - acc: 0.9754 - val_loss: 0.7147 - val_acc: 0.8673\n",
            "Epoch 47/50\n",
            "\n",
            "Epoch 00047: LearningRateScheduler setting learning rate to 0.0001913998.\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.2287 - acc: 0.9754 - val_loss: 0.7776 - val_acc: 0.8503\n",
            "Epoch 48/50\n",
            "\n",
            "Epoch 00048: LearningRateScheduler setting learning rate to 0.0001875821.\n",
            "390/390 [==============================] - 93s 237ms/step - loss: 0.2265 - acc: 0.9754 - val_loss: 0.8495 - val_acc: 0.8389\n",
            "Epoch 49/50\n",
            "\n",
            "Epoch 00049: LearningRateScheduler setting learning rate to 0.0001839137.\n",
            "390/390 [==============================] - 93s 237ms/step - loss: 0.2223 - acc: 0.9769 - val_loss: 0.6822 - val_acc: 0.8673\n",
            "Epoch 50/50\n",
            "\n",
            "Epoch 00050: LearningRateScheduler setting learning rate to 0.000180386.\n",
            "390/390 [==============================] - 93s 237ms/step - loss: 0.2177 - acc: 0.9782 - val_loss: 0.7251 - val_acc: 0.8580\n",
            "Model took 4656.30 seconds to train\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3gAAAFNCAYAAABSRs15AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzdd3zV9b3H8dcn6wSygCQQCCNskSkg\n4AStIlpHrXXPVuXaq622XqttvbXD3u7tqqtuLe5Rt6I4QAEHICB7JBCSQAjZ83v/+J5ICJmQk5Px\nfj4e53Fyfut8TqKc8znf7/fzMeccIiIiIiIi0vlFhDsAERERERERaRtK8ERERERERLoIJXgiIiIi\nIiJdhBI8ERERERGRLkIJnoiIiIiISBehBE9ERERERKSLUIIncpDMLMPMnJlFteDYy8zs/faIS0RE\npLPSe6vIgVOCJ92KmW0yswozS6m3/dPgG0lGeCLbJ5Z4Mysys1fCHYuIiEhzOvJ7a2sSRZGuQgme\ndEcbgfNrH5jZeKBn+MLZz1lAOXCimaW15xPrDVBERA5QR39vFek2lOBJd/QwcEmdx5cCD9U9wMyS\nzOwhM8s1s81mdrOZRQT3RZrZH80sz8w2AF9v4Nz7zGy7mWWZ2a1mFtmK+C4F7gKWARfVu/YgM3sm\nGNdOM7utzr4rzWyVmRWa2Uozmxzc7sxsRJ3jHjCzW4M/zzKzTDO70cyygX+ZWW8zeyn4HPnBnwfW\nOb+Pmf3LzLYF9z8X3L7CzE6rc1x08Hd0WCteu4iIdE4d/b11P2YWMLO/Bt/PtgV/DgT3pQTf/3ab\n2S4ze69OrDcGYyg0sy/N7GsHE4dIW1OCJ93RIiDRzMYE3xzOAx6pd8w/gCRgGDAT/6b17eC+K4FT\ngcOAqcC36p37AFAFjAgeMxu4oiWBmdkQYBbwaPB2SZ19kcBLwGYgA0gHngjuOxv4efD4ROB0YGdL\nnhNIA/oAQ4C5+H8X/hV8PBgoBW6rc/zD+G9lxwJ9gb8Etz/EvgnpKcB259ynLYxDREQ6rw773tqE\nnwIzgEnARGAacHNw3/VAJpAK9AN+AjgzGw1cAxzunEsATgI2HWQcIm1KCZ50V7XfNJ4IrAKyanfU\neWP6sXOu0Dm3CfgTcHHwkHOAvzrntjrndgG/qXNuP3xic51zrtg5l4NPgM5rYVwXA8uccyvxydvY\nOiNg04ABwA3Ba5c552oXlV8B/N45t9h565xzm1v4nDXALc65cudcqXNup3PuaedciXOuEPg1/o0Y\nM+sPnAxc5ZzLd85VOufeDV7nEeAUM0us81oebmEMIiLS+XXU99bGXAj80jmX45zLBX5RJ55KoD8w\nJPhe955zzgHVQAA41MyinXObnHPrDzIOkTal9TbSXT0MLACGUm8KCZACRONHymptxo+YgU+yttbb\nV2tI8NztZla7LaLe8U25BLgHwDmXZWbv4qe5fAoMAjY756oaOG8QcKBvMLnOubLaB2bWE//GOQfo\nHdycEHxzHgTscs7l17+Ic26bmX0AnGVmz+ITwWsPMCYREel8Oup7a2MGNBDPgODPf8DPjHk9+Jx3\nO+d+65xbZ2bXBfeNNbPXgB8657YdZCwibUYjeNItBUe3NuK/EXym3u48/Dd3Q+psG8zebyK34xOd\nuvtqbcUXSElxzvUK3hKdc2Obi8nMjgRGAj82s+zgmrjpwAXB4idbgcGNFELZCgxv5NIl7LvQvX7h\nFlfv8fXAaGC6cy4ROLY2xODz9DGzXo0814P4aZpnAwudc1mNHCciIl1MR3xvbca2BuLZFnwthc65\n651zw/DLHn5Yu9bOOfeYc+7o4LkO+N1BxiHSppTgSXd2OXC8c6647kbnXDUwD/i1mSUE18X9kL1r\nCeYB3zezgWbWG7ipzrnbgdeBP5lZoplFmNlwM5vZgnguBd4ADsWvB5gEjAN64EfDPsa/Af7WzOLM\nLNbMjgqeey/wP2Y2xbwRwbgBPsMniZFmNofgdMsmJODX3e02sz7ALfVe3yvAHcFiLNFmdmydc58D\nJuNH7up/eysiIl1fR3tvrRUIvm/W3iKAx4GbzSzVfIuHn9XGY2anBt9LDSjAT82sMbPRZnZ8sBhL\nGf79sqaVvyORkFKCJ92Wc269c25JI7u/BxQDG4D3gceA+4P77gFeAz4HPmH/bykvAWKAlUA+8BR+\nHn+jzCwWv/7gH8657Dq3jfgpL5cG3xxPwy8w34Jf/H1u8LU8iV8r9xhQiE+0+gQvf23wvN349QbP\nNRUL8Fd8UpmHXzT/ar39F+O/hV0N5ADX1e5wzpUCT+On59T/vYiISBfXkd5b6ynCJ2O1t+OBW4El\n+KrVy4PPe2vw+JHAm8HzFgJ3OOfm49ff/Rb/HpmNLzb241bEIRJy5teLioi0DTP7GTDKOXdRsweL\niIiISJtSkRURaTPBKZ2Xs7cKmYiIiIi0I03RFJE2YWZX4hfCv+KcWxDueERERES6I03RFBERERER\n6SJCNoJnZvebWY6ZrWhkv5nZ381snZktM7PJoYpFRERERESkOwjlFM0H8I2SG3MyvkLRSGAucGcI\nYxEREREREenyQlZkxTm3wMwymjjkDOAh5+eILjKzXmbWP9jrpFEpKSkuI6Opy4qISFexdOnSPOdc\narjj6Cz0Hiki0j009f4Yziqa6fiCDLUyg9uaTPAyMjJYsqSx9ioiItKVmNnmcMfQmeg9UkSke2jq\n/bFTVNE0s7lmtsTMluTm5oY7HBERERERkQ4pnAleFjCozuOBwW37cc7d7Zyb6pybmpqqmToiIiIi\nIiINCWeC9wJwSbCa5gygoLn1dyIiIiIiItK4kK3BM7PHgVlAipllArcA0QDOubuAl4FTgHVACfDt\nA32uyspKMjMzKSsrO9iwO7TY2FgGDhxIdHR0uEMREREREQkbff5vXCiraJ7fzH4HXN0Wz5WZmUlC\nQgIZGRmYWVtcssNxzrFz504yMzMZOnRouMMREZFmmNkg4CGgH+CAu51zf6t3jAF/w3/hWQJc5pz7\nJLjvUuDm4KG3OucebK/YRUQ6On3+b1ynKLLSnLKyMpKTk7vsHxfAzEhOTu7y31KIiHQhVcD1zrlD\ngRnA1WZ2aL1jGuwJa2Z98DNfpgPTgFvMrHd7BS4i0tHp83/jukSCB3TpP26t7vAaRUS6Cufc9trR\nOOdcIbAK3w6orq96wjrnFgG9zKw/cBLwhnNul3MuH3gDmNOO4YuIdHjd4bPxgbzGLpPghdPu3bu5\n4447Wn3eKaecwu7du0MQkYiIdCRmlgEcBnxUb1djPWEb297QtdVKSESknXXkz/9K8NpAY3/gqqqq\nJs97+eWX6dWrV6jCEhGRDsDM4oGngeucc3va+vpqJSQi0v468uf/kBVZ6U5uuukm1q9fz6RJk4iO\njiY2NpbevXuzevVq1qxZwze+8Q22bt1KWVkZ1157LXPnzgUgIyODJUuWUFRUxMknn8zRRx/Nhx9+\nSHp6Os8//zw9evQI8ysTEWleRVUN+SUV7Cr2t53FFRSXV3H+tMHhDi3szCwan9w96px7poFDGusJ\nm4WvRF13+zuhibKOlc9DIAGGHx/ypxIR6cw68ud/JXht4Le//S0rVqzgs88+45133uHrX/86K1as\n+Krazf3330+fPn0oLS3l8MMP56yzziI5OXmfa6xdu5bHH3+ce+65h3POOYenn36aiy66KBwvR0S6\nmOLyKorL632jaPUfNjzHv6SiiuyCMrL3lJGzp5zsPf7nHQVl5BaVs6u4gsKy/b+tNINzpg4iMqLr\nr49oTLBC5n3AKufcnxs57AXgGjN7Al9QpcA5t93MXgP+r05hldnAj0Me9PzfQPJwJXgiIs3oyJ//\nu1yC94sXv2DltradAXPogERuOW1si4+fNm3aPqVM//73v/Pss88CsHXrVtauXbvfH3jo0KFMmjQJ\ngClTprBp06aDD1xEurSq6hp2l1aSX1xBfkklO/aUkbW7lG3BW9buMrbtLqWgtLLNnrNHdCRpSbH0\nTQgwYWAvkuNi6BMXQ++4mK9+rr1149yu1lHAxcByM/ssuO0nwGBouiesc26Xmf0KWBw875fOuV0h\njzhxAOzJCvnTiIi0JX3+31eXS/A6gri4uK9+fuedd3jzzTdZuHAhPXv2ZNasWQ2WOg0EAl/9HBkZ\nSWlpabvEKiLh4ZyjoLSSrN2lZOXXJmSlbNtdRmllNc45HOAcwXuHc1BcUUV+cCrkngZGzgASYqNI\n79WD9F49mDqkNwN69SAhNoraQlzO1YuliThjoyJIS4olLTGWvomxJMZGdYuqZW3BOfc++42V7ndM\noz1hnXP3A/eHILTGJaXDjhXt+pQiIl1BR/r83+USvNZk2m0lISGBwsLCBvcVFBTQu3dvevbsyerV\nq1m0aFE7Ryci7aGyuoacwnK27y5le0EZ2QVl7CmrpLCsisKyKorKKykqr6Io+HjHnjKKK6r3uUYg\nKoIBvXoQF4jEMMyC2YH5CZRmEB+IYmDvnvTpGU2vnntHz3r3jCY1IcCAXj1IjI0Ox69AuoLEdCjK\ngaoKiIoJdzQiIi2iz//76nIJXjgkJydz1FFHMW7cOHr06EG/fv2+2jdnzhzuuusuxowZw+jRo5kx\nY0YYIxWR1iipqCK30K8zyy+pIL+48qtiIvkllewqLid7TznZBaXkFpZTU28oLCKYkCXERhMfiCI+\nNopePWMY2LsnM0enfjXKNqBXD9J79yA5LkajYxJeiQMAB4XbofeQcEcjItJhdeTP/+bqz9Xp4KZO\nneqWLFmyz7ZVq1YxZsyYMEXUvrrTaxVpKzl7yvhi2x5KK6uprK6hstoF72uoqKqhvKqGvKJycgvL\nySksJy94X1S/MElQVIQFR8+i6ZcYS/+kWNKSejAgKZa0pFj6J/UgLUnTGduCmS11zk0NdxydRUPv\nka2y7k145Cz49qsw5Ii2C0xEpI11p8/EDb3Wpt4fNYInIl2Kc44NecUs2bSLjzfms2TzLjbvLGn2\nvLiYSPomxpIaH2DMgERmJgTomxBLSnwMKfGBr6ZB9o6LISGgxE26qMRgL3UVWhER6bSU4IlIp1RQ\nWukLk+SXsq3A32/IK2bp5nx2FVcAkBwXw9SM3lw8YwgTB/UiMTaaqEgjJjKC6MgIoiKN6MgIAlER\nxEZHhvkViXQAXyV428Ibh4iIHDAleCISNs45dpdUftVbLTtYnGRXcQVlldWUV9VQXhW8r/Q/F5VX\nsX13GYX1pk/GREYwsE8Pjhvdl2lDezM1ow/DUuI00ibSGrGJEJOgETwRkU5MCZ6ItJucwjIWrMnj\n3TW5LMvcTXZBGeVVNfscYwa9ekTTIzqSQHQkgaiI4C2SuEAUKfEBjhyesk9xkgG9YkmJCxChxmsi\nB0+98EREOjUleCISMpXVNSzdnM+7a3J598tcVm73TUhT4gNMH9aHk8amfVWkpF9i7FcNtKMjI8Ic\nuUg3ljhAUzRFRDoxJXgi0qzyqmq27iply65iNu8sYfPOErbsKmHzzmIKSisxMyIMIs2IiDAizIiM\nMHKDlSijIowpQ3rzozmjmTkqlTFpiRptE+moEtNh/epwRyEiIgdICV4YxMfHU1RUFO4wRBpUU+NY\nm1PEJ1vyWbo5n0+25LMxr5i6HVXiYiIZnBzHyL4J9ImPwTl/Xo1zVDuHc1Bd40jsEcUxI1M5cngy\nCWq+LdI5JKVDYTZUV0Kk/r8VEWkL7fn5XwmeSDeXW1jOim0FfLZlN59syeezLbu/KmDSJy6GyYN7\ncdqEAWSk9GRwnziGJPdUQ26RruyrZufZ0GtQuKMREZFWUoLXBm666SYGDRrE1VdfDcDPf/5zoqKi\nmD9/Pvn5+VRWVnLrrbdyxhlnhDlS6c6cc2wvKGNFVgErtu3hi6wCVmwrYMeecgAiDEb1S+D0SQOY\nPLg3k4f0JiO5pxI5ke6mbqsEJXgiIg3qyJ//leC1gXPPPZfrrrvuqz/wvHnzeO211/j+979PYmIi\neXl5zJgxg9NPP10flqXdlFVWsyKrgKWb9061zCvy/eEiDIanxnPk8BTGDkhkXHoSYwckahqliARH\n8FAlTRGRJnTkz/9dL8F75SbIXt6210wbDyf/ttHdhx12GDk5OWzbto3c3Fx69+5NWloaP/jBD1iw\nYAERERFkZWWxY8cO0tLS2jY2kaCcPWVfJXNLt+SzIquAymq/cC4juSfHjkpl4sBejEtPYkz/BHrG\ndL3//UXk4OwuqaA6MoVkUCVNEek89Pl/H/qE10bOPvtsnnrqKbKzszn33HN59NFHyc3NZenSpURH\nR5ORkUFZWVm4w5Quoqq6hi93FPJJMKFbsjmfzPxSAGKiIpg4MInvHD2UKcGplinxgTBHLCKdwRm3\nf8CE9CT+ER2nBE9EpBkd9fN/10vwmsi0Q+ncc8/lyiuvJC8vj3fffZd58+bRt29foqOjmT9/Pps3\nbw5LXNK51NQ4PsvczTurc8grrqC8sobyqmrKq2r8rbKasspq1uUUUVxRDUDfhABTM3pz2ZEZTBnS\nm7EDkoiJUh85EWm91PgAuUXlwV54meEOR0SkZfT5fx9dL8ELk7Fjx1JYWEh6ejr9+/fnwgsv5LTT\nTmP8+PFMnTqVQw45JNwhSgdVVlnNh+vzeGPlDt5clUNuYTmREUbvnjEEoiIIREcQExlBIDqSQFQE\niT2iOWvKQKYM6c3kwb0Z2LuH1naKSJtIiQ+wLrcIktXsXESkOR31878SvDa0fPneub8pKSksXLiw\nwePUA09y9pTx7ppc3ly1gwVr8iitrCYuJpJZo/ty4qH9OG50X5J6quCJiLSvlIQYFm0sh6HpsPHd\ncIcjItLhdcTP/0rwRNpBeVU1Szfl8+6aXN5dk8vq7EIA+iUG+ObkdE48tB9HDE8mEBUZ5khFpDtL\niQ+wu6SS6oT+RBZmQ3UVROqjgohIZ6J/tUVCJL+4gpeWbWP+l7ksXL+T0spqoiONKUN686M5ozl2\nZCqH9k8kIkLTK0WkY6gtyFQUSCPJVUNxzt62CSIi0ikowRNpQ845Fm7YyRMfb+XVFdlUVNeQkdyT\ns6cO5NiRqcwYnkx8QP/biUjHlJrgE7z8yBSSAAqylOCJiHQyXeaTpnOuyxeacM6FOwRpRG5hOU8t\nzeTfi7ewaWcJibFRnD9tEOdNG8yY/onhDk9EpEVqR/ByLJkMCDY7PzyMEYmINE6f/xvWJRK82NhY\ndu7cSXJycpf9Izvn2LlzJ7GxseEOpdsqragmp7CM7IIydhSWk7PH/7whr5gFa3KpqnFMG9qHa08Y\nycnj+hMbrfV0ItK5pAYTvG2uj9+gSpoi0kHp83/jukSCN3DgQDIzM8nNzQ13KCEVGxvLwIEDwx1G\nt7JlZwkPLdzEc59lkVdUsd/+2OgI+if14DtHD+XcwwcxPDW+/YMUEWkjKQkxAGwrj4WoHsERPBGR\njkef/xvXJRK86Ohohg4dGu4wpItwzvHh+p3864NNvLV6B5FmzB7bj7EDkuiXGEu/xABpibH0TYwl\nMTaqy35rJCLdT8+YKHrGRJJXVBlsdq4RPBHpmPT5v3FdIsETaQslFVU8+2kWD3ywibU5RSTHxXDN\ncSO4cPoQ0pI0NVZEuofUhAB5ReXBBE8jeCIinY0SPOnWyquq+WBdHq8sz+a1L7LZU1bF2AGJ/PHs\niZw6QevoROTAmdn9wKlAjnNuXAP7bwAuDD6MAsYAqc65XWa2CSgEqoEq59zU9onaF1rJLSyHlHTY\n/EF7Pa2IiLQRJXjS7ZRWVPPumlxeXbGdt1blUFheRUJsFCeO6ccF0wczZUhvTbsUkbbwAHAb8FBD\nO51zfwD+AGBmpwE/cM7tqnPIcc65vFAHWV9KfAwbcoth2AAo3A411RChL7tERDoLJXjSLZRVVvPO\nlzm8+Pl23l6dQ2llNb17RnPK+P7MGZ/GUcNTiImKCHeYItKFOOcWmFlGCw8/H3g8dNG0XEp8gI83\n7oKkdKipguJcSEgLd1giItJCSvCky6qucSzasJPnPs3i1RXZFJZXkRIfw1lT0jl5XH+mD+1DVKSS\nOhEJLzPrCcwBrqmz2QGvm5kD/umcu7uJ8+cCcwEGDx580PGkxAfIL6mkKr6//5CwJ0sJnohIJxLS\nBM/M5gB/AyKBe51zv623fwhwP5AK7AIucs5lhjIm6dqccyzPKuC5T7fx4rJt5BaWEx+IYs64NM6Y\nNIAjhiUrqRORjuY04IN60zOPds5lmVlf4A0zW+2cW9DQycHk726AqVOntr4jbj2pCb4XXkF0X5IB\nCrIgfcrBXlZERNpJyBI8M4sEbgdOBDKBxWb2gnNuZZ3D/gg85Jx70MyOB34DXByqmKTrqqyu4eXl\n27n3vY0szyogJjKC4w5J5YxJ6Rx/SF8VSxGRjuw86k3PdM5lBe9zzOxZYBrQYILX1lKCzc5zSPYJ\nnloliIh0KqEcwZsGrHPObQAwsyeAM4C6Cd6hwA+DP88HngthPNIFFZRW8u/FW3jgg01sKyhjWGoc\nv/rGOE6fMICkntHhDk9EpElmlgTMBC6qsy0OiHDOFQZ/ng38sr1iSg02O8+uimNMZECtEkREOplQ\nJnjpwNY6jzOB6fWO+Rz4Jn4a55lAgpklO+d2hjAu6QK27irh/g82Mm/xVoorqjliWDK3njmOWaP6\nEhGhCpgiEn5m9jgwC0gxs0zgFiAawDl3V/CwM4HXnXPFdU7tBzwbrOYbBTzmnHu1veKuHcHLK6pQ\ns3MRkU4o3EVW/ge4zcwuw089ycL3/NlHWy8gl85pY14xb63awZurdvDxxl1EmHH6xAF85+ihjEtP\nCnd4IiL7cM6d34JjHsC3U6i7bQMwMTRRNW/fBC9dCZ6ISCcTygQvCxhU5/HA4LavOOe24UfwMLN4\n4Czn3O76F2rrBeTSOVRV1/DJlt28GUzqNuT6L7hH90vgmuNGcMH0IaQlxYY5ShGRriUuEEXPmEjy\nisr9CN7WReEOSUREWiGUCd5iYKSZDcUnducBF9Q9wMxSgF3OuRrgx/iKmtLNVVbXcNc767nvg43s\nLqkkOtKYMSyZS2YM4Wtj+jGoT89whygi0qWlxAd8gpc8APZsh5oaiFAFYhGRziBkCZ5zrsrMrgFe\nw7dJuN8594WZ/RJY4px7Ab824TfBPj8LgKtDFY90DiuyCrjhqWWs2r6HEw/tx5mHpXPMyBQSYlUw\nRUSkvaTEx5BbWA5D06GmEkryIL5vuMMSEZEWCOkaPOfcy8DL9bb9rM7PTwFPhTIG6RzKKqv5x9tr\nuevdDSTHxXD3xVOYPVaNdUVEwiElPsCmncWQlO437MlSgici0kmEu8iKCJ9syedHTy1jXU4RZ08Z\nyM1fP1QtDkREwig1IcCSzfl+DR74QisDDgtvUCIi0iJK8CRsSiqq+NPra7j/g430T4zlwe9MY+ao\n1HCHJSLS7aXEB8gvqaAqrr//oFCgXngiIp2FEjxpd2WV1Tz20RbueGc9eUXlXDRjMDfOOUTr7ERE\nOoiUhADOwS4S6RsRrWbnIiKdiBI8aTcVVTXMW7KV295eR/aeMqYP7cNdF01makafcIcmIiJ1pMbH\nAJBTVEnfxP7qhSci0okowZOQq6qu4ZlPsvj722vJzC9l8uBe/OmciRw5PBkzC3d4IiJSz95m5+WQ\nOFAJnohIJ6IET0JmT1klL36+jXsWbGDTzhImDEzi1m+MY+aoVCV2IiIdWGpCbYJX4QutZC0Nc0Qi\nItJSSvCkTdXUOBZt3MmTSzJ5ZcV2yiprOLR/IvdcMpUTxvRVYici0gnsO4I3AFa9CM6B/g0XEenw\nlOBJm8jaXcrTSzN5culWtu4qJSEQxVmTB3LO1EFMGJikxE66l7x1sPxJGHQ4DP9ayz8Ub/sU5v/G\nN5ZOmwD9J0DaROgzDCIiQhuzSB1xgSh6REeSV1gOKelQXQ4lOyEuJdyhiYhIM5TgyUEpq6zm5udW\n8PQnmTgHR41I5voTR3PS2DR6xESGOzyR9uMcbFwAC2+Hta/t3T70WDjhF5A+ufFzC7PhrV/BZ49C\nz2RI7O+vU1Pp98fEQ79xkDYehh8HI2dDpKrOSmilJMT4Ebxhtb3wspTgiYh0Akrw5IBlF5Qx9+El\nLMss4MpjhnLJERkM6tMz3GGJtK+qclj+FCy6A3asgJ4pMPMmmHwxrHoJFvwe7jkOxn4Tjr8Zkofv\nPbeyDBbdDu/92V/nyO/BsTdAbCJUVUDuasheBtuX+fvPH4fF9/jnmHAuTLoA0sY1Hlv+Zlj/Fqyf\nDymj/PO31Wj6yuf9tL1ZP973NUmXkRIfILeoHJLS/YY926D/xPAGJSIizVKCJwfk0y35zH14KSXl\nVdx98RRmj00Ld0gi7ausAD76J3x8DxTnQN9D4fTbYPzZEB3rj5lxlU/CPvy7H5Fb9QJM+TbM/BFs\nWQiv/y/s3gyjvw6zf7VvohQV46do9p8AhwW3VVfCurf8SN/Hd/vkMG0CTLrQP29UADa9D+vf9ond\nznX+vB59/HNHxcLMGw7+tZcXwks/hJI8WPkCHPNDOOq6va9buoTU+ACbd5ZAYobfoF54IiKdghI8\nabWnl2by42eX0y8xwCOXH8XotIRwhyTiR8A2LgCL8FMaY+KCt3iI6QnRPdtm9KqyFBbfC+/9CUrz\nYcSJcMTVMGxWw9ePTfQjZ4dfAe/+DpbcD0v/BTVVPim85Hl/bktERsPoOf5Wssuv8/vsUXj1Rnj9\nZn9MTSVE9YCMo/1zDj8ekkfCc9+F+bf66Z+HXXRwv4OFt/vk7rzH/OjlO7+BZfPg63/0zyddQkpC\ngKWb8yEuFSKioEAJnohIZ6AET1qsusbxu1dXc/eCDRwxLJk7LpxM77iYcIcloVRZ5qceluyEaXOh\n95BwR9Sw/M3w5GWw7ZPGj4ntBRc/2/RauKZUV8Fnj8A7v4PCbb54ytf+FwYc1vy5AAlpcOpfYMZ/\n+5G/vmNg8qUQeYD/DPfsA9P/y9+yV8Cyf/vtw4+HwUfsP5p2xm1+pPGF70NcXxg1+8CetygXPvwH\njDkdDvm6v02+GP5zPTx8pp+KetL/+URSOrWU+AC7SiqockZUgpqdi4h0FkrwpEUKSiv5/uOf8u6a\nXC49Ygg3n3oo0ZGq6telreIQ+FkAACAASURBVJ/vP7TvWu+/vV90J0w4B47+IaSOCnd0e615DZ6Z\nC64GzrzbJ6EVxXVuRf5+0Z3w0g/gyrchohUFgGpqYOWz8Pav/e9i4DQ46x4/QnYgUkb6ka62lDau\n6bV44Ef/znkI/nUKPHkpXPYSpE9p/XO990eoLIHj/3fvtuHHw3cXwgd/9esJ174Bx/8UDr/ywBNY\nCbvU+Bicg10lFfRNHKApmiIinYTeeaVJu4oreGTRZh5auIndJZX835njuWD64HCHJQdi4e2wewsc\n+g0YNL3xsvtFOfDaT2H5PF+e/+JnIfUQP2qz5F/w+RNw6BlwzPV+fVi4VFfB/F/D+3/21SXPecjH\n25heg+Cp7/gpktOubNlzFOXCo9+C7Z9B37Fw/hMwak7n7QUWSIALn4L7ToRHz4HLX29dgZT8zbD4\nPj/Fs36SHx0Ls27yawFf/h9Y8AdfCKZnn7Z9DdJuanvh5RaW0zcxHbZ/HuaIRESkJZTgSYM27yzm\nvvc3Mm/JVsoqazj+kL58/2sjmTSoV7hD6zxqqmHDOz6ZCsSHN5aspT5pw8FHd0FCf5/ojT0TBh7u\nk72aGvjkAXjz536d2cwb/Whd7VS/Ob/xjxfd4degrXwORp7kKz8OObJ1o2IHq3AHPH05bHrPT3M8\n+XcQ3aPpc8Z+Ez55CN76pZ9emNCv6eNrquGZK3wlyzPvhvHfat/XGCoJ/eCiZ3yS98g34fI3IL5v\ny86d/3/+dzDzpsaPSR7ur1+QqeSuk0tNqG12XuGbnX/5ipqdi4h0AkrwZB+fbMnnngUbePWLbKIj\nIvjGYQO44phhjOqnQiqtsnkhvPIjX9p+8JFw0dO+0MfByN/kKxZ++QqMP8sX0GiJmhp4+Qb/IX7u\nu7D5A/jiWT+S9dGdkJjuR+QyF/tbxjHw9T83PA0zPhVOuAWOutZXj1x0Bzx4qi/CMPpknzgNPdZX\nc2wupprK5o9ryKb3/Uhc2R74xp2+SmVLmMEpf4I7j4DXfwpn3dv08Qv+6BP00/8BE89tfZwdWcoI\nuPBJeOBUeOwcuPSl5r+EqF3nd+T39pbNb4yZHzGVTq12BC+vsNz/O1FV6gsLKXEXEenQlOAJADl7\nyrjhqWW8uyaXxNgovjtzOJcdmUHfRJU9b5WCTHjjZ7Diaf+B6KjrfIn8f18E5z/e+oRm53pf3v6L\n5/w0QfCNsF+5CdKnwoBJzV/js0f9CN6Zd/vCF+O/5W9le2DNq7DiGT8iF0iAM//pp9U19w19j16+\n3P4R/+2vseolWPGsHyGLSfAFPA451Zfw370Zdm3wCequDXt/riqDpME+kUypc0sdDT16+/U+uzYG\njw/e79oEOV/snTrab2zrfp8pI/zfZMHv4bCLYdjMho9bP99Xhpx4vj+uKxo4Fc5+AJ44Hx471/8c\nn9r48W//ylcEPfoH7RWhhFnKVyN45ZBSp9m5EjwRkQ5NCZ4wf3UO1z/5OSUVVfzklEO4YPoQ4gP6\nT6NVKkvhg7/D+38BnJ/eeNR1ftQueQS8cI0fdTr7weaLTlRV+B5ny56A7OV+W/oUOPGXfqQtkAh3\nHukLi/zXu01PTSzN91MuB83wBVLqik302yacA+VFvpBKa/uYxcTBuLP8raocNrwLq1+E1S/7JLeu\nqB7QOwP6DIcRJ/j2BTvXQd4a2PSBHx2oZRG+aEqtiOjguUNh5Ik+yYhNbF2stY75oV9f+J/r4bsf\n7J9079kOT1/hE82v/6lrT0cbPccn9c9fA3cdDd+6r+HiMZsX+kT+a7fow303EhcTSWx0hE/whtVp\ndp42PryBiYhIk/Qpvhsrr6rm969+yX3vb+SQtARuu2AGI/pqKmarffGc70FWsNWva5v9K+hVpxDN\n5It9FcdXb4Tnr/bTChsrcJK1FJ7/nh+lGng4zP41HHr6vtcDf42HvwFv3AKn/L7x2Ob/Bkp3wSl/\naDpRaYs1glEBP3I3ajac+lfY+pEfqes1xI+4JaQ1HkNNDezJhNw1PuErzvWvuc8wn9Qlprfd+rfo\nHnDKH33xlA//DsfWafxdXeUT8coSX7QlJq5tnrMjm3COb9kw71J48DQ47idw9PV7/xt1Dt68BeLT\nYPpV4Y1V2pWZkRIfILewfO+0XFXSFBHp8JTgdVMbcov43uOf8sW2PVx2ZAY3nXwIsdFdoIBEe3IO\n3vqFH7XrNx7OvKvx0vkzroKKQnj7Vp801B8Zqiz1BSwW3gbx/Xy1xtEnN/7cw4/z/dQW3QEjZ8PI\nE/Y/JnsFLL4Hpn6n/atdRkT6witDjmzh8RE+oes1uOHX0tZGnujXCy74o6/62DvDb59/K2z50E9n\nTR0d+jg6irTxfjT4xWv9f6ObP4Rv3gNxKX7kbutHvoffwa4jlU4nNSHgi6zE9wOLVC88EZFOQI3M\nuhnnHE8tzeTUf7xP1u5S7rlkKj8/faySu9aqqfY91d7/C0y5DOa+03xftGP+x0/bXHKfHxFxzm/f\n/CHceZQfTTrsYvjvRU0nd7W+dgukjoHn/xuKd+67zzlf5CW2Fxz30wN4gd3AnN/6D6wv/8j/vta8\ntvfv2dWKqrREIAHOus+Pvm76wE/Z3PgevPkLP622q65FlCalxAf8FM2ISD8KX6ARPBGRjk4jeN1I\naUU1P3l2Oc9+msX0oX3423mHkZakIiqtVlUBz/4XfPGMbxvwtZ+1bJ2WGZzwc994+4O/QWTAr5Fb\nfI+fxnjJC40X/WhIdKxvuH3P8fDi9+HcR/bGseJpXy3z1L9qzVRjktLhuB/76bUf3QXv/NaPZM35\nXbgjCx8zmPptv+bzyUt9hVTwBVgio8MaWmdkZvcDpwI5zrn9OtGb2SzgeWBjcNMzzrlfBvfNAf4G\nRAL3Oud+2y5B15MSH+DTLfn+Qe8MXxlYREQ6NCV43cSWnSXMfXgJX+4o5AcnjOKa40cQGdGFi0eE\nSkUJzLsY1r3pi54cdW3rzjeDk//g1+Qt+D1gMP278LX/PbD1XmnjfYL5+s2+WuZhF/mCKa/fDP0n\nweRLWn/N7mT6VfDZ4/DqTb7659kPtr7QTFfUf4JvqfHKjVC+B8acEe6IOqsHgNuAh5o45j3n3Kl1\nN5hZJHA7cCKQCSw2sxeccytDFWhjUuNj2FVcQXWNI3Lsmb6J/bbPWlbBV0REwkIJXjewYE0u33v8\nU5xz/Ouyw5k1uoVNjWVfpbt9OfnMj+G0v8OUSw/sOhERcPptvrDF4CNg0LSDi2vG1X564Ss3+jVv\nSx+Ewu1wzsNdozF3KEVG+7VlT1zg75OHhzuijiM2Ec68M9xRdGrOuQVmlnEAp04D1jnnNgCY2RPA\nGUC7J3gpCQFqHOwqriB1/Nn+y6NPHlKCJyLSgWkNXhfmnOOOd9Zx6b8+pn9SLC9+7+iukdxVV/mR\ntPZUuMM3hc5aCt/614End7Uio/zo38Emd+ATxjPv8uvJ/n0xLLwdJl0Igw4/+Gt3B4Onw/+s9dVK\nRdrfEWb2uZm9Yma1jR3Tga11jskMbmt3qcFm57mF5b7/5aFnwPKn2v/fYBERaTGN4HVRReVV/Oip\nz3l5eTanTRzA784aT8+YTvrnLsyGzMXB21LY9okvcnL6P0JfDKO6Ela/5AtNFO2AC+fB8OND+5wH\nImkgnPpnePpy3yfvhJ+HO6LOpbG2FSKh9QkwxDlXZGanAM8BI1t7ETObC8wFGDx4cDNHt84+zc7B\nF9tZ9m9Y9QJMPK9Nn0tERNpGJ/3EL03ZmFfM3IeWsD63iJ+eMoYrjhmKhatZ8+aFPjGbfDH06N2y\nc2pq4MuXYfmTfsSsIPhFdkS0Xxs0+RLfAPzZuZC7Co7/Wdt/QC/cAZ88CEvu99Mde2fAJc+3zYhb\nqIz/lu9RlTIK4rvASK1IF+ec21Pn55fN7A4zSwGygEF1Dh0Y3NbYde4G7gaYOnWqa8sYU+LrJXgZ\nR/v+lJ88rARPRKSDUoLXxXy4Lo+rHllKZITx8OXTOWpESviCKd4J/74ISvJgwR9g2lzfuy0uueHj\na6ph5fO+N1nOF5DQ369Rm/Hfvul32vi9BTCqKuCVG3xZ+9w18M1/+jLvjSnK9ceufA6SBvn1b30P\nDd6P8f2+nPPJ6Md3++blNZUw4gQ47W/+vjOsZ2tt0RcRCRszSwN2OOecmU3DL5vYCewGRprZUHxi\ndx5wQThiTImPAeokeGZ+FO+tX8DO9Vq3KiLSASnB60LmLdnKT55ZztCUOO6/7HAG9QlzU+JXb4Ky\nAr9mbeXz8N6fYNGdcPjlcOT39o4yVVf5sv7v/RHy1vgRqDPvhnFn+bVqDYmK8S0AUsfAaz+G+06C\nC57wjbLrKt0NH/7DP29VKYya47d98Sws/dfe4+JSITYJdq7zUxwPv8LfUkaE5ncjIl2emT0OzAJS\nzCwTuAWIBnDO3QV8C/iumVUBpcB5zjkHVJnZNcBr+DYJ9zvnvgjDSyA+EEUgKsI3O6816QJ4+1b4\n9GFNBxcR6YCU4HUBNTWOP7+xhtvmr+PoESnccdFkEmPD3LNqzWuwfB7MvAnGfdPfclb7JG/hbX6U\nbMplkHqI7wmXvxH6jvXJ4KFntGy0zAxmXAUpI+HJb8Pdx8F5j8LgGb4NwUd3+WuXFcDYb8JxP/HH\ngh+tK9oBOSshZ5W/37PNjxZOOBcC8SH99YhI1+ecO7+Z/bfh2yg0tO9l4OVQxNUaZkZqQsAXWamV\nkAajToLPHoPjbm78izgREQkL/avcyZVVVnPDU8t48fNtnDt1ELeeOY7oyDAXjCjbAy/9wE+BPOb6\nvdv7HuIbc8+6Cd77Myy+F2qqoP9EOPdRGH3Kga2lG/E1uPIt38LggVN9o+YvnoXiXD9id9xP/dq9\nusz8h5SEtI5ZNEVEpINIiQ/snaJZ67CL/Vrpta/DIaeEJzAREWmQErxObGdROXMfXsrSzfncOOcQ\nrpo5LHzFVOp685a9fdiiYvbfnzwcvnE7zLoR9mz3hUsONu6UkXDFm/DkZX50MOMYOO+xjl0URUSk\nE0iJD5CZX68twsjZEJ/me+IpwRMR6VCU4HVS63OL+M4Di9leUMbtF0zm6xP6hzskb+N7vvLkEdfA\nwClNH9tr8P5r5g5Gzz5w0TOwa71fx9cRkl0RkU4uNSGGz7bu3ndjZBRMOt9Pg9+zHRI7yHuQiIio\n0Xln9GV2Id+680OKyqp4/MoZbZPcFe6AV38Cb/wMivMO7BoVJfDi96H3UD8tMhwioyB1tJI7EZE2\nkhIfYFdxOdU19TowHHYxuBr4/LHwBCYiIg3SCF4nsyG3iAvv/YiYqAj+PfcIMlLiDu6ClWWw6Ha/\nJq6qzL9ZL74Pjrjaj8LFJrb8Wu/8H+zaAJe+BDFhruApIiJtIjUhQI2D/JKKr/riAX66/ZCjfU+8\no37Q9v1IRUTkgOhf405k664SLrz3I5xzPHrFjINL7pzzrQluOxze+iUMnQlXfwz//ZEvWvLu7+Bv\nE32LgcrS5q+XtRQW3g5Tvg1DjznwuEREpEOpTer2qaRZa/Ilvgry5g/aOSoREWlMSBM8M5tjZl+a\n2Tozu6mB/YPNbL6ZfWpmy8xMK7UbkV1QxgX3LqKkoppHrpjOiL4HUcY/ayncPwee+o4fobvkBTj/\nMf9tbOooOOchuHI+DJgEr98Mf58MSx+A6sqGr1dVAc9f4xfcn/iLA49LREQ6nNoEb79KmgCHng6B\nJF9sRUREOoSQTdE0s0jgduBEIBNYbGYvOOdW1jnsZmCec+5OMzsU3/MnI1QxdVa5heVccO8i8osr\nefSK6Yzp34ppk/W99Uvfiy4uFU77Oxx2UcM959Inw8XP+qIpb/0CXrzW36JiIboHRPcM3vfwjcpz\nV8EF83yzcBER6TJS4n015AYTvOgeMOFs+PQRKP099OjdztGJiEh9oVyDNw1Y55zbAGBmTwBnAHUT\nPAfUZitJwLYQxtMp7S6p4OL7PmL77jIeunwaEwf1OvCL1VTDojth5Elw1r0tW1839Bi4/A1Y+wZs\n+wQqS/yUzcoSX1Sl9ucJZ/vGtyIi0qWkJARH8AorGj5g8iW+r+nyp2Dale0YmYiINCSUCV46sLXO\n40xger1jfg68bmbfA+KAE0IYT6ezp6ySS+7/mA15xdx/6eEcntHn4C64a6NPxsac1rriKWYwara/\niYhIt5IQiCIQFdHwCB5A/4mQNgE+f1wJnohIBxDuIivnAw845wYCpwAPm9l+MZnZXDNbYmZLcnNz\n2z3IcCivqubyBxazctse7rpoMkePTDn4i2Yv8/dp4w/+WiIi0i2YGSnxgYaLrNQ69HS/vrsop/0C\nExGRBoUywcsCBtV5PDC4ra7LgXkAzrmFQCywXybjnLvbOTfVOTc1NTU1ROF2LL94cSWLN+Xzl3Mn\ncfwh/drmojtWgEVC6iFtcz0REekWUhIC5DY2ggcwMjjDY+0b7ROQiIg0KpQJ3mJgpJkNNbMY4Dzg\nhXrHbAG+BmBmY/AJXvcYomvCvCVbeeyjLXx31nBOmzig7S6cvcI3AY+ObbtriohIl5caH0NeUSNr\n8MBP0UzoD2tfb7+gRESkQSFL8JxzVcA1wGvAKny1zC/M7JdmdnrwsOuBK83sc+Bx4DLnnAtVTJ3B\niqwCbn5uBUePSOF/Zo9u24tnL4d+49r2miIi0uWlxAcaX4MHfq32yBNh/duNt9QREZF2EcoiKzjn\nXsa3Pqi77Wd1fl4JHBXKGDqT/OIKrnpkKSlxMfztvElERljbXbx4JxRu0/o7ERFptdSEALuKK6iu\ncY2/N42c7fvhbVnkKzCLiEhYhLvIigRV1ziu/fdn5Owp546LppAcbCzbZnYs9/dpGsETEZHWSYkP\nUF3jyC9pYprmsFkQEa1pmiIiYaYEr4P421trWbAml5+fPpZJB9PrrjHZK/x9P43giYhI66QEv3Rs\ncppmIAEyjlKCJyISZkrwOoC3Vu3g72+t5ewpAzl/2qDmTzgQ2cshPg3iu0cVUhERaTsp8TFAE83O\na42cDbmrIX9zO0QlIiINUYIXZpvyirnu358xLj2RX31jHGZtuO6urh0rtP5OREQOSEpCC0bwAEae\n5O81iiciEjZK8MKorLKaqx5ZSmSEceeFU4iNjgzNE1WV+29Utf5OREQOQGpLE7yUEdBnmBI8EZEw\nUoIXRr99ZTWrswv5y7mTGNSnZ+ieKPdLqKlSiwQRETkgCYEoYqIimm52XmvkbNi4ACpLQx+YiIjs\nRwlemMz/MocHPtzEd44aynGj+7bu5Jpq+OhuyFvXsuOzaytoTmjd84iIiABmRmp8gNzCFiZ4VWWw\n8b3QByYiIvtRghcGeUXl3PDkMg5JS+BHc1rZzLymGl74HrxyA8y/tWXn7FgBUT0geXjrgxUREQH6\nJgbIzG/BqFzG0RDdE9a+FvqgRERkP0rw2plzjpueXsaeskr+et6k1q27q03uPnsUEgbAurehurL5\n87KXQ79DISJEa/xERKTLm5bRh0+35FNcXtX0gVEB3xNv7evgXHuEJiIidSjBa2ePfrSFN1flcNOc\nQzgkLbHlJ9bUwAvf98ndzJvglD9AeQFsWdT0ec4FEzytvxMRkQM3c3QqldWOD9blNX/wyNmwe4tf\nAy4iIu1KCV47WpdTxK3/WckxI1O47MiMlp9YUxMcuXsEZt4Ix/3YfzsaGdP8FJiCTCjbrRYJIiJy\nUKYO6UN8IIr5X+Y2f/DI2f5e0zRFRNqdErx2UlFVw3X//pQe0ZH86eyJRES0sN9d/eRu1o/99kC8\nX+ewppk3zx0r/L0SPBGRdmVm95tZjpmtaGT/hWa2zMyWm9mHZjaxzr5Nwe2fmdmS9ou6cTFRERw9\nIoV3vszBNTf1MindzxxZ+0b7BCciIl9RgtdO/vzGGlZk7eF3Z02gb2Jsy06qqYEXg8ndsT/yyV3d\nRugjT4K8NbBrQ+PXqK2g2W/sgQcvIiIH4gFgThP7NwIznXPjgV8Bd9fbf5xzbpJzbmqI4mu14w5J\nZXtBGWt2FDV/8MjZsGUhlBWEPjAREfmKErx2sHD9Tv65YD3nTxvM7LFpLT/xpevg02Byd9xP9k3u\nAEYFp8CsaaKhbPZy6D0UAgmtD1xERA6Yc24BsKuJ/R865/KDDxcBA9slsIMwc5Rv6zP/y5zmDx51\nku/Buv7tEEclIiJ1KcELsaLyKq6f9xlDk+P431PHtPzE7Z/DJw/CEdc0nNwB9BkGKaOaXuOwY4Wm\nZ4qIdHyXA6/UeeyA181sqZnNDVNM+0lLimVM/0Tmr25Bgpc+FWJ7aZqmiEg7azbBM7PvmVnv9gim\nK3ri4y1sKyjj99+aQM+YqJaf+OkjEBmAY65vOLmrNeok2PQ+lDcwXaa80E/fVIInItJhmdlx+ATv\nxjqbj3bOTQZOBq42s2ObOH+umS0xsyW5uS0ogHKQjhudypLN+ewpa6ZNT2QUjDjBt0uoqQl5XCIi\n4rVkBK8fsNjM5pnZHLOmsg2pq6Kqhvve38iMYX2YmtGn5SdWlsGyeTDmVOjZzHkjT4LqCtjwzv77\ndqz090rwREQ6JDObANwLnOGc21m73TmXFbzPAZ4FpjV2Defc3c65qc65qampqaEOmVmj+1Jd4/hg\nbQvaJYw6CYpzYfunIY9LRES8ZhM859zNwEjgPuAyYK2Z/Z+ZDQ9xbJ3ei59vY3tBGf81s5W/qtUv\n+dYGh13c/LGDZ0AgCda8uv++7GX+Xj3wREQ6HDMbDDwDXOycW1Nne5yZJdT+DMwGGqzEGQ6TB/ci\nITaqZevwhn8NME3TFBFpRy1ag+d8PeTs4K0K6A08ZWa/D2FsnZpzjrsXbGB0vwRmjWrlN6qfPgJJ\ng2DozOaPjYyGEcf7N8/6U2B2rPDrH5I6/Lp9EZEux8weBxYCo80s08wuN7OrzOyq4CE/A5KBO+q1\nQ+gHvG9mnwMfA/9xzjXwLV54REVGcOyoVN75Mrf5dglxyTDwcCV4IiLtqNlFYWZ2LXAJkIefRnKD\nc67SzCKAtcCPQhti5/TOmly+3FHIn86eSKtmte7e4qdbzrwRIlpYA2fUHPjiWcj+HAYctnd79nI/\nPVOzakVE2p1z7vxm9l8BXNHA9g3AxP3P6DhmjUrlP8u2s3L7HsYOSGr64Iyj4cN/+OUH0S1sEyQi\nIgesJRlEH+CbzrmTnHNPOucqAZxzNcCpIY2uE/vnu+vpnxTLaRMHtO7Ezx7394dd2PJzRpwA2L5N\nz2uq/Ro8rb8TEZE2NnO0n5nyzpctKOqSPgVqKv2sEhERCbmWJHivUKePj5klmtl0AOfcqlAF1pl9\nvnU3izbs4vKjhxIT1YpOFDU1vqn5sJnQa3DLz4tL8VNg6iZ4O9dDVanW34mISJvrmxDL+PSkFrZL\nmOzvs5aGNigREQFaluDdCdStwV8U3CaNuHvBBhJiozhvWiuSNIBNC/wUzZYUV6lv1GzY9gkU7vCP\ndyz39xrBExGREDhudCqfbMlnd0lF0wcmDoCE/krwRETaSUsSPHN1VlEHp2a2oqFb97Ipr5hXVmzn\nohlDiA9EQVkB3HsCrHur+ZM/fQRik+CQA5j5OmqOv18XXMievRwioiB1dOuvJSIi0oyZo/tS4+C9\nlrRLSJ+iBE9EpJ20JMHbYGbfN7Po4O1aYEOoA+us7n1/A1EREXz7yAy/YetiyFwMT37bT5tsTGk+\nrHwBxp9zYIvQ+42DxPS97RKyV0DKaIgKtP5aIiIizZg0qBe9eka3rF1C+mTYuc6/14mISEi1JMG7\nCjgSyAIygenA3FAG1VnlFZXz5JJMvjk5nb6JwSStthddRAQ8fj6UFzZ88vKnoLocDrvowJ7cDEbO\nhvXzoapibwVNERGREIiMMGaOSuXdL3OpqWmmXUL6FH+/TQ3PRURCrSWNznOcc+c55/o65/o55y5w\nzrXg67ru56GFmymvquGKY4bt3Zi9HHoNgbMf9N9ePvNf+/erAz89M208DJh04AGMOgkqimDVC1CU\nDWkqsCIiIqEza3QqO4srWJ5V0PSBtS18NE1TRCTkmk3wzCzWzK42szvM7P7aW3sE15mUVFTx0MJN\nnHhoP0b0jd+7o3YkbdhMOOnX8OV/YEG9/vDZy2H7ZwdWXKWuoTMhKhbe+5N/rBE8EZE2YWbDzSwQ\n/HlWcOlCr3DHFW7HjkzFrAXtEmKTIGUUZH3SPoGJiHRjLZmi+TCQBpwEvAsMBBqZZ9h9zVu8ld0l\nlVw1s87oXUWxH7VLm+AfT78KJl4A7/wGVv9n73GfPgKRMTD+7IMLIqYnZBwDOSv9435K8ERE2sjT\nQLWZjQDuBgYBj4U3pPBLjg8wcWCvFq7DmwKZS8A1M51TREQOSksSvBHOuf8Fip1zDwJfx6/Dk6Cq\n6hrueW8jU4b0ZsqQPnt37FgJuL0jaWZw6l9gwGR4Zi7krIaqclj2b185s2efBq/fKqNO8vcJAyAu\n+eCvJyIiADXOuSrgTOAfzrkbgP5hjqlDmDU6lc8zd7OzqLzpA9OnQHEO7Mlqn8BERLqpliR4lcH7\n3WY2DkgC+oYupM7nP8u3k7W7lP86dti+O7I/9/d1p0pGx8K5j0B0T3jifPj8cV9V7ECLq9RXm+Bp\n/Z2ISFuqNLPzgUuBl4LbosMYT4dx3Oi+OAcL1jYzTVMNz0VE2kVLEry7zaw3cDPwArAS+F1Io+pE\nnHPc9e4GRvSN54Qx/fbdmb0cYntB0sB9tyelw7kPw+6t8OJ1kDgQhs1qm4B6DYapl8OkC9rmeiIi\nAvBt4Ajg1865jWY2FL+Eodsbn55EclwMb69uJsHrNw4iorUOT0QkxJpM8MwsAtjjnMt3zi1wzg0L\nVtP8ZzvF1+EtWJvHqu17mHvsMCIibN+dtQVWzPY/cfAMOOUPgPOjdxGRbRfUqX+GsWe23fVERLo5\n59xK59z3nXOPB7/0THDO6ctOICLCOGlcGq9/kd30NM3/Z+++w6Qqzz6Of5/tfZdtsJSl7lIUpQkq\nRcASsCdRg8aob+xRjuy6fgAAIABJREFUY4kxJjExMcVoTDUae41do0GjwQaiKAgKiCAdZIFdYFm2\n993n/eOZlQG2DLKzMzvz+1zXuWbmnDNn7j3McOaep9xRse6aGMgWvE3z4a58qC4JXAwiIn7WboJn\nrW0GbuqiWLql++ZtoFdKHGeO6rPvhqZG2LFy7wQrrRn3f3Dx2zDlRv8GKSIih8QYM88Yk2KMSQc+\nBR40xvw50HEFi+9PHEBdYzP/Wril/R37jIXty6C5qWsC29+6N6Fyx94atSIiIciXLppvG2NuNMb0\nM8aktyx+j6wbWFZQykcbd3PxpIHERO13Kks2QGMt5LST4AH0OwoiNYxDRCTIpVpry4FvAU9YaycA\nJwQ4pqAxJDuZaUOzeHLhZmob2kne+oyF+gooXtd1wXnbvszdBur1RUS6gC8J3neAq4D5wCeeZYk/\ng+ou7pu3gZS4KM6dkHvgxqIV7la16EREQkGUMSYHOIe9k6yIl0snD6K4sp5XlrYzS2afse42EN00\nrYVCT8td8dquf30RkS7SYYJnrR3YyjKoo+eFug27KpmzqojvHdOfpNioA3co+szVtsvM7/rgRESk\ns90GzAE2WGsXG2MGAWoG8nLM4AxG5KTw0AebaG5uo9ZdxhCITQlMgleyEerK3H214IlICGslM9mX\nMeaC1tZba5/o/HC6jwfnbyQ6MoKLjh3Y+g5FKyB7uLpfioiEAGvtC8ALXo83At8OXETBxxjDpVMG\ncv1zy3lv7S6mDWulolJEBPQeHZgEr9DTPTNzqBI8EQlpvnTRPMprmQz8Cjjdl4MbY2YYY9YYY9Yb\nY25uZftfjDHLPMtaY0zpQcQeMDvLa/n3p9s4e2xfspJjD9yhpRuIumeKiIQEY0xfY8zLxpidnuUl\nY0zfjp8ZXk49oje9UuJ46IONbe/UZyzs+Bwaats/2OYPYO7tnRfc9mWuZ82IM6B8K9RXdd6xRUSC\niC9dNK/xWi4FxgBJHT3PGBMJ3APMBEYA5xpjRux37OuttaOstaOAu4F/f50/oqs9vGATjc3NXLZ/\nYfMWFUVQXdz+DJoiItKdPIqrBdvbs7zqWSdeoiMjuGjiABas383K7WWt79RnLDQ37h2r3prGevjP\nVfDeH6CsnTF9B6NwOWSPgJ6Huce713fOcUVEgowvLXj7qwLa6Je4j/HAemvtRmttPfAscEY7+58L\nPPM14ulS5bUNPL1wCzNH5tA/I7H1nTTBiohIqMmy1j5qrW30LI8BWYEOKhidOz6XhJhIHn5/U+s7\n9Bnjbtvrpvnp47Bns7u/4d1DD8pal+D1HgWZeW6dummKSIjqMMEzxrxqjJntWV4D1gAv+3DsPkCB\n1+OtnnWtvUZ/XNLYCf+L+9dTC7dQUdfIlccNbnunlvo6Lb8SiohId7fbGHO+MSbSs5wP7A50UMEo\nNT6ac8b1Y/by7RSVtdINM6U3JOe0neDVVcJ7d0L/iW6/De8celB7NkNtKeSMgvTBgNFMmiISsnxp\nwbsL+JNnuR2YYq09YDzdIZoFvGitbbV4jjHmMmPMEmPMkl27dnXyS/uutqGJRxZsYtKQTA7vk9r2\njkUroMcAiGtnHxER6U6+jyuRUAQUAmcBFwUyoGB28aSBNFvLYx9ubn2HPmPbTvAW/hOqdsIJv4bB\n02HjvEMvjN4ywUrvURAdBz36qwVPREKWLwneFmCRtfY9a+0C3K+YA3x43jagn9fjvp51rZlFO90z\nrbUPWGvHWWvHZWUFrkfMy0u3sauijivaa70Dl+Cpe6aISMiw1n5prT3dWptlrc221p6JZtFsU7/0\nBGYc3ounF31JVV3jgTv0GQMlG6Bmz77rq3bDh3+HYadCv6NcglezZ2+B8q9r+zKIiHZj8AAy8pTg\niUjI8iXBewFo9nrchNdU0e1YDOQZYwYaY2JwSdzs/XcyxgwDegAf+XDMgGlqtjwwfyOH90lh4pCM\ntnesq3C1dnod2XXBiYhIINwQ6ACC2SWTB1Fe28gLSwoO3NhS8Hz70n3Xf/BnqK+E6b9wjwdNA8yh\nd9MsXOZKF0V5Zr7OzHeTrDQ3t/88EZFuyJcEL8ozSQoAnvsxHT3JWtsIXI0rDPsF8Ly1dqUx5jZj\njHeZhVnAs9baNqqiBocF64vZVFzF5VMGY4xpe8cdqwCrFjwRkdDXzsVAxuT2YGz/HjyyYDNN+xc+\n7z3a3Xp30ywtgI8fhCPPg+xhbl1ihutWeSgTrVjrWvB6j9q7LjMPGmtcuQQRkRDjS4K3yzshM8ac\nART7cnBr7evW2nxr7WBr7e88635prZ3ttc+v/DCmr9PNXbOT2KgIThzRs/0dWyZYUYInIhLqOvxh\n0hjziKdu3udtbDfGmL976sV+ZowZ47XtQmPMOs9yYWcG3lUunTyQLSXVvLmyaN8NcamuFW3bp3vX\nzfuDu52631eCwdOh4GOobaPsQkdKt+ydYKVFZr671UQrIhKCfEnwrgB+ZozZYozZAvwEuNy/YQWf\n+Wt3MWFQBnHRke3vWPQZxKe7WcJERKRbM8ZUGGPKW1kqcPXwOvIYMKOd7TOBPM9yGfBPz+umA7cC\nE3Blh241xvQ4hD8lIE4c0Yvc9ATue28DB3TU6TMWti5xLWw7V8Pyp2H8pZDWb9/9Bh8Ptgk2zf96\nQXhPsNJCpRJEJIT5Uuh8g7X2aFyx8hHW2mOttWFVHXTrnmo27KpiSl5mxzu3TLDSXjdOERHpFqy1\nydbalFaWZGttlA/Pnw+UtLPLGcAT1lkIpBljcoBvAG9Za0ustXuAt2g/UQxKkRGGq6cPYfnWMv6z\nbPu+G/uMdbNllm+Dd38DMUkwqZVhjf3Gu21ft5vm9mUQEQXZXqWLErNcK6ISPBEJQb7Uwfu9MSbN\nWltpra00xvQwxvy2K4ILFvPXuh6px+V3MINnU6Mbg6fumSIi4pu2asb6XEs22J01pi8j+6Ry+xtf\n7DujZkvB80X3werX4NgfujF3+4uMhoFTYP07rrXvYLVMsBIdt3edMa6bprpoikgI8qWL5kxrbWnL\nA88viSf7L6TgM3/tLnJS4xiSndT+jrvXQVMd9DqiawITEZGwFyy1YtsSEWH41ekj2FFex33vbdi7\noefhrnTBh3dDYjYcfWXbBxk8HUq/dLNUH4yWCVa8x9+1UKkEEQlRviR4kcaY2JYHxph4ILad/UNK\nY1MzCzYUMyUvq/3ZM8F1zwS14ImIiK/aqhnrcy3ZYKkV256x/dM5Y1Rv7p+/kYKSarcyKnbv9fK4\nmyC2nR9Rhxzvbg+2m2ZZAdSUQE4rpYsy86CyCGrLD+6YIiJBzpcE7yngHWPMxcaYS3DjAB73b1jB\nY1lBKRW1jRw31IeLZtFnEBm7d/C2iIhI+2YDF3hm0zwaKLPWFuJKDJ3kGRbRAzjJs67bunnmMCKN\n4fY3vti7Mn+Ga8kb08EkoemDoMdA103zYLQUSG8py+CtZSbN3WrFE5HQ4sskK3cAvwWGA0NxF5j+\nfo4raMxfu4sIAxMH+zjBSs8RbryAiIiEPWPMM8BHwFBjzFbPj6VXGGOu8OzyOrARWA88CPwAwFpb\nAvwGWOxZbvOs67ZyUuO5cupgXl9RxEcbdruVU38CV3wAUR2W13XdNDe/D431He/bonA5mEjoediB\n2zSTpoiEKF9a8AB24Or9nA1MxxUuDwvvrStmVL80UhM6SNqs3TuDpoiICGCtPddam2OtjbbW9rXW\nPmytvc9ae59nu7XWXuWpFzvSWrvE67mPWGuHeJZHA/dXdJ7LpgyiT1o8v3515d7i577OOj3keKiv\nhIJFvr/gVxOsxB+4rcdAl/wpwRORENNmgmeMyTfG3GqMWQ3cDWwBjLV2mrX2H10WYQCVVNXz2dZS\npnQ0eyZA+Xao3q0JVkRERNoQFx3Jz08ZzuqiCp5dvOXgnjxgsit34Os4vPYmWAHXapg+UDNpikjI\naa8FbzWute5Ua+0ka+3dQFPXhBUcPlhfjLX4luBpghUREZEOzTy8FxMGpnPXnDWUVTf4/sS4FOg7\nHjb4OA6vfBtUF+9b4Hx/mflqwRORkNNegvctoBCYa4x50BhzPBBW1bvnr91Fanw0R/ZN63jnlgSv\ntX7+IiIiAoAxhl+eNoKymgb+9s5BJldDprtxdZU+lINomWClrRY8gIwhULIBmsPq92sRCXFtJnjW\n2lestbOAYcBc4Dog2xjzT2PMSV0VYKBYa3l/3S4m5WUSGeFDXlv0mZvlKzbZ/8GJiIh0Y4f1TmXW\n+Fye+Ggz63dW+P7EwZ5yCRvndbxv4TIwEe3/8JqZD031rsaeiEiI8GUWzSpr7dPW2tNwdXiWAj/x\ne2QBtmZHBTvK6zguz8eaQppgRURExGc/OjGf+JhIfjV7FdZa356UMwri033rprl9GWQNg5iEtvdp\nKZWgbpoiEkJ8nUUTAGvtHk9B1eP9FVCweG+N6/4xOd+H8gi15bBnkxI8ERERH2UkxXLTjGF8sL6Y\nB9/f6NuTIiJg8DQ30Up7SaG1rgWvve6ZoFIJIhKSDirBCyfz1+0iv2cSOamtTK28v80fuNucVgqp\nioiISKvOn5DLjMN6ccf/1vDJlz6W+Rt8PFTugB0r296nohCqdrU/wQpAQjokZGgmTREJKUrwWlFd\n38jiTXs4zpfZMwE+eQySesKg4/wal4iISCgxxnDHWUfQOy2Oa55eyp4qH4qYD57ubtvrpunLBCst\nNJOmiIQYJXitWLSxhPqmZt/KI5QWwPq3YPT3ILKDYugiIiKyj9T4aO49byzFlfX86IXlNDd3MB4v\nJQeyR8D6dhK8lglWfBk6kTEEdivBE5HQoQSvFe+t3UVcdARHDUjveOdPn3B9/cde6P/AREREQtDI\nvqn8/JThvLt6Jw/4Mh4v70TY/D68ei2UbT1w+/ZlkDm0/QlWWmTmu+6c1T52ERURCXJK8Foxf+0u\nJgzMIC46sv0dmxph6ZPuQpOW2zXBiYiIhKALjunPKSNz+OOcNSzZ3EGyNflHMO5iWPoU/H00vH4T\nVBTt3V64DHKO9O2FW2bS3L3+6wUuIhJklODtp6Ckmo3FVb51z1z7PzeQe+z/+T8wERGREGaM4fZv\nj6Rvj3iufnopJe2Nx4tLhVPugh9+CkfOgsUPwd9GwZu3QNHnbhKWjiZYaaGZNEUkxCjB28/8da48\ngk8TrCx5BJJ7Q17I130XERHxu5S4aO45bwwlVfXc8PyyjsfjpeXC6XfD1YthxBnw0T1w/2S3zZcJ\nVgDS+kNEtGbSFJGQoQRvP/PX7qJPWjyDsxLb33HPZleHZ8wFEBnVJbGJiIiEusP7pPKL00Ywb80u\n7pu/wbcnZQyGb90PP1joEr2sYb530YyMcs9XC56IhAhlJl4ampr5cP1uTj0yB2NM+zt/8jgY4xI8\nERER6TTnT8hl0cbd3DVnDUN7JnP88J6+PTFrKJz92MG/YGYe7Fpz8M8TEQlCasHzsryglIq6Rqbk\nddA9s7Eelv4L8mdAap+uCU5ERCRMGGO486wjOKx3Ktc8s5TPt5X59wUz8qBkIzQ1+Pd1RCQ0lG2D\ne49tv1xLACnB87JuZyUAR/RLa3/HNf+Fqp2aXEVERMRPEmKiePjCcfRIiOH7jy1me2mN/14sMx+a\nG93wCxGR9jQ3wytXws6VrsEnCCnB81JYVkuEgezk2PZ3XPIopPaDIcd3TWAiIiJhKDsljkcuOoqa\n+ia+/9hiKmr91MLWUipB4/BEpCOL7oNN77lcYP07QdnyrwTPS1FZDVnJsURHtnNadm9w/6hjL4SI\nDurkiYiIyCEZ2iuZe747hnU7K7n66aU0NjV3/otkDnG3mklTRNqzYxW8/SsYejLMuB3qyuDLDwMd\n1QGU4HkpLKulV2p8+zt98hhERMHo73VJTCIiIuFuSn4Wvz3zcN5bu4tfzl6JtR2UTzhYcamQ1FMt\neCLStsY6+PdlEJcCp/0dBk2DyBhXFzvIKMHzUlRWS05KXNs7NNbBsqdg6ExI7tV1gYmIiIS5c8fn\ncsVxg3l60RYefH9j579AZj7sVoInIm2Y+zvYsQJO/wckZUFsEgycAmvegM7+0ekQKcHzUlRWS6/U\ndhK8L16F6t2aXEVERCQAbvrGUE45Ioffv76aN1YUdu7BW0olBNkXtaBQXQJVxYGOQgLhw3/AGz8J\ndBSBt/kDWPB3GHsRDJ2xd33+DNizKei6dyvB86iobaCirpGc9hK8JY9CjwGuSVZERES6VESE4U9n\nH8mY3DSufW4Z89fu6ryDZ+RBbSmUbumc4zXUwBevuS/HpQWdc8xA2PAu3D0GHj4pKCeTED+q3Anv\n/tZNKlK4PNDRBE5tGbx8BaQPhJN+t++2fE+yt+aNro+rHUrwPIrKagHabsErXgdffuAy9widNhER\nkUCIi47k4QuPYkhWEpc8sYR5a3Z2zoEHTHJj7O+fDAv/+fWSmfoqWPkyvHAR3DkYnvuu+3L8v5s7\nJ8auZC188Ff417chKh5KNgTtlPDiJx/9A5rqIDrRvRfC1es3Qfl2+NaDrlumt7R+0HNk0I3DU6bi\nUehJ8HLammRl5cuAgSPP7bqgRERE5AA9EmN46pIJ5GUncdkTnzB3dSckeTlHwBULoM9Yl5D9cyKs\nf7vj51UVw2fPw3Pnu6TuhYtg0/twxDnwvVdg6k9h9WuwZeGhx9hV6ird3/H2rTDiDLh6MfQdD+/d\nCQ21gY5OukJ1CSx+GA77Foy/FFa94maSDzcrX4bPnoUpP4a+41rfZ+gMKFjkzlmQUILnUfRVgtdG\nC94Xs6HfBE2uIiIiEgRakrz8Xklc/uQnvLt6x6EfNHsYnP9vOPdZaG5wrVdPz9r3i21DLWycB2/d\nCvdNhj8Ohn9fCgWLYfT5cOFrcONaOO2vMHgaHPtDSM6BN3/RPcb37d4AD53gvveceBuc9ahrtTj+\nl1CxHZY8HOgIpSss/CfUV8KUG+HoH0BENCz4W6Cj6lqlW+DV69yPPlNubHu//Jlgm2Hdm10XWweU\n4Hm0tOD1bG0WzZJNULQChp/WxVGJiEh3ZoyZYYxZY4xZb4w5oJ+eMeYvxphlnmWtMabUa1uT17bZ\nXRt595CWEMNTFx/N0F7JXP7kJ7y9qhOSPGPcbNk/WOgSnM0fwD0TYPY18OQ34Y4B8MQZrvtaTBJM\n+zlc/Dbc8AWcchcMnLxvndyYBJj2M9j6sZusLZitnQMPTIPKIpfoTrzWnQ9wf9egqfD+n6CuIpBR\nir/VlsGi+9333uzhkNwTRn8Xlj8D5Z08uVGwaqhxrfK22XXNjIxue9/eo12ZlSAahxcV6ACCRVF5\nDZlJscREtZLztvyHPPzUrg1KRES6LWNMJHAPcCKwFVhsjJltrV3Vso+19nqv/a8BRnsdosZaO6qr\n4u2uUhOi+dclE7jg4UVc+dQn3PvdsZw4ouehHzgq1iU4R8yCd29z488y891Y/MHToP/EA8fjtOXI\n8+Cjez0Fkme2/2XRn1b/F7Z94sYXNje626Z6d7+u3E0K0+tw+M5T0KP/gc+f/kt4aLpr3Tnupq6P\n/1A1N+2bfEvrPn7AFfCe8uO96479oasFvfAeOOm3AQutS1gLr14LhZ+51vyMwe3vHxEBeSfByleg\nsR6iYromzvZCCnQAwaKwrLad7pmvQs6RbgZNERER34wH1ltrN1pr64FngTPa2f9c4JkuiSzEpMZH\n8+QlEzisdyo/eOoT5qws6ryDJ/eEM+6BW3bBVYtg5h8g/xu+J3cAkVFw4q/dRCWfPNZ5sfnKWnjn\nN/DsefDBX9wX+E+fhM9fdK0OG+e5L7NjL4Tvv9l6cgfQdywMOxU+vDuoxhv55NMn4c6BriuttK2u\n0v0YkfcN9923RfpANx5vyaNQsydw8XWFRffBZ8+5lnfvkgjtGXoy1FfAlwv8G5uPlOB5tFkDr7zQ\ndatQ90wRETk4fQDv+fG3etYdwBjTHxgIvOu1Os4Ys8QYs9AYc6b/wgwNKXHRPHHxeA7vk8oPnvqU\nZz/upHIHLSIPsdNT3kkwYDLM+0PXdnFsrIdXroT374IxF7hE9ZYd8LOt8JPN8ON1cMMquO4zOO1v\nrktpe6b93MXfncZjbVkIr13vuh6++kOVe2jPkkegpmTf1rsWk6534/I+fqjr4+oqm+bDnJ+7HzIm\ntzPubn+DpkJUXNDMpunXBK+jsQeefc4xxqwyxqw0xjztz3ja02YL3urX3O3w07s2IBERCSezgBet\ntU1e6/pba8cB5wF/Nca02k/IGHOZJxFcsmtXJ9aF64ZS4qJ58uIJTBqSyc3/XsEd/1tNc3OQTGxi\njGvFqy52BZO7Qm05PH22Gzs17edw2t8PPVHtOcLNELrofqjoxJZSfynbBs99z01nf+Z9sHOVa4GU\nAzXUuHMzaCr0O+rA7b0Ody17i/4J9dVdHZ3/lW5xs8dmDIFv3ndwZdFiEmDgca5FPAgmU/Jbguc1\n9mAmMAI41xgzYr998oCfAhOttYcB1/krnvZU1zdSVtPQegveF7Ndn/usoV0fmIiIdGfbgH5ej/t6\n1rVmFvt1z7TWbvPcbgTmse/4PO/9HrDWjrPWjsvKyjrUmLu9pNgoHr5wHN+dkMs/523gmmeWUtvQ\n1PETu0KfsXD4t90ELe1NVlFaAG/90nXnrK/6eq9Vvh0enekmiTnjXjdmrmXClEM19WY3y+j8uzrn\nePuz1iWnh6qhxtUibKiGWc/AqHNdj6z37oCSjYd+/K+juRlWvAj3TXIT2rx8pasxt+YNF1NzAN+r\nnz4BVTthSjvjKyddD9W7YemTXRdXV2iZVKWpAWY9BbHJB3+MoTOg9EvYtbrz4ztI/pxk5auxBwDG\nmJaxB6u89rkUuMdauwfAWttJ1UoPTmFbJRKqdsPmBe7NLCIicnAWA3nGmIG4xG4WrjVuH8aYYUAP\n4COvdT2AamttnTEmE5gI3NklUYeAqMgIfnvm4QzMTOR3r3/B9rIaHrxgHJlJsYEODab/AlbNhnm3\nw+n7teTVlrsxcgvvhcY6wMKbv3TlF466uOPJHlrsWAVPneW6JJ73PAw5vnP/hvRBMPp7LgE99pq2\nx+x9HdUlrhWlYBF8++GvP8Fdy0QZ25fCrKddCQyAmXfChvHw2g3wvZc7L+n1RUt5jcJlkD0CEjJg\n41xY7tWBLSoOMvI8CYZ1f4dt3nsfC2n9IfcYyD0aeh7WORPHNNa5bre5x8KAiW3v1/8Y99of3g3j\nvu+fCYOs7dp/l/0nVcnM+3rHyZ8BXO+S9ezhbb/WB39x43h7Hva1Q+6IPxO81sYeTNhvn3wAY8wC\nIBL4lbX2gM6rxpjLgMsAcnNzOz3Qlhp4vVL2K3K+5nWwTRp/JyIiB81a22iMuRqYg7vGPWKtXWmM\nuQ1YYq1tKX0wC3jW2n369QwH7jfGNON62/zBe/ZN6ZgxhksmD6JvjwSue24p37x3AY9edBRDsr/G\nL/OdKX2gKxy96D5XXyx7GDQ1wqePwdzbXRfOI74D029x3QsXPwgf3+9mLxxyAhx1KeSduO+X+uYm\nlxhV7YJdX8Cr10N0PPzfG66Auz8cdxMse9q1hp15b+ccc+cX8Mws1/qYPgie/x6cfJdLbg/WR/d4\nJsr4OQw7Ze/6lN5wwq3w+o2w4gXX3bQjh5pwFK1wid2GdyC1H3zzfhh5zt4ugDWlULzWtfzsWuOW\nRk9BeWMA425NhItly0ew8t9ue0wy9Bvvkr3co13N5qiv8UPGsqehfBuc8Y+O9510g+v6u+JF1yra\nmT55zI1T/e4L0GvkoR+vsd59dhY/DHFp7seItP773q5/e+97xddJVVqT0ttNTLP2fzD5hlZiqYPZ\nP3SF02v2wEm/+fqv1QFj/dRP1BhzFjDDWnuJ5/H3gAnW2qu99nkNaADOwXVdmQ+MtNaWtnJIAMaN\nG2eXLFnSqbG++MlWbnxhOfNunMqAzMS9G546x/1nc91nXftLgoiIAGCM+cQzDk184I9rZChYXlDK\nxY8voa6xifvPH8uxQzIDG1DVbvj7KOh/LIz9P3jrF+4Lfv9J7ktfnzH77l9R5L74LnnU1ahL6+8S\nhapdLiGsLgG8vs9lDoXzX4S0zv9RfB9zfu5aG3+w8NCHsqz+L/z7MohJhO/8y7VuvPB/sG6Om+xi\n+i2+fxdb/45rwRx2Kpz9+IFjqZqb4OGTYM9muHoxJKS3fhxr3Yyj826HqT+FCZcf3N+050uY+zv4\n7HmIT3N/x1GXQHQbs7b7ylo3XqxgkUv2tix031ex7t/+nMfbbkFqTVMD3D0GErPgknc6Ps/Wui6m\nzY1w5UcHN1atPRVFcPc4Nxtlcm+45G1IbXVeKt9sfM8l8sVr3bjCyFjXhXLPl9BYs+++w06Fc548\n9L9l7u3uR48fr4dEr/9nqna7LqBbPoRpt7jC6YeYW7R3ffRnC54vYw+2AoustQ3AJmPMWiAP162l\nyxSVuX/kfcbg1Za7ZvOjLlVyJyIi0o0d2S+NV646lu8/tpgLHvmYa4/P48qpg4mKDNBk4okZbvjH\nO792v/ZnDHFjxIbObP07R3IvN+5t8o9c6aalT0JDrUuqEie6L+YJme4LZWIm9BnX8WyYnWHSDS7x\nfPlymHAFDDnR/W0Hw1qY/0eXCPUe48Y/pfR222Y9Df+9wc0AWr7ddWntqEvg7g3w4vchazic+c/W\nv7BHRLoZQx84ziXXZ9xz4D5Vu+E/V8HaN1yy8cZNUFYAJ9zWcRJgrRvP9oZnLNuk62DidS7J6wzG\nuJanHv33tkDW7IENc91rPjgdTvkTjDqgR3jrlj/jEsaZf/TtO68x7v370sXu/Hi3kB6KN38BTXUu\n0XrlB/D0Oa4VOi7l4I5TXghv/hw+f8mVODvvedclsoW17seRPV+6hK+uAkae3TmJ6tAZ8N4fYN2b\ne89/8Tp46mz3Hv72wzDyrEN/nQ74M8HzZezBK7i6P496xhjkA10+6rWwrJb0xBjior26O6x70xX/\nHKHZM0VERLqgC2S2AAAgAElEQVS7vj0SePHKY/nFK5/zp7fW8u6anfzlnFH79tzpSkdf6Vpd+h4F\n4/7Pt7FMkdFw+LfcEgwSM1wXyrdvdUkexnUXzP+GG4+UPaL9hKG+ypVwWPUf1y31tL+5rqUtIqPc\nupQ+MO/3ULnDtU61NgFGYz3s+NwlBsZ4Jspop1Zhr8Pd+MEP/uKK2Q+cvHfbpvdda2J1Mcz4g/ux\n/38/cePOyre7xLGtbpB1lS4p/ew5GDTNJY+H0grlq/ge7n3R/1h46RJ3Xr9c4JK2tpL9HStda9Oq\n/0Dv0fsmQR0ZcSa8+xt48xbXdTTvG4eWIG3+AFY87yZ4GXG6+7d76mx4/gLXXdOXz0dTg5vddd7t\n7v5xN7vkOnq/IVjGQFK2W1qbLfRQ5IyC5Bw3Dm/Uee699Nz5EBEFF74KufuPVvMPv3XRBDDGnAz8\nlb1jD37nPfbAGGOAPwEzgCbgd9baZ9s7pj+6n1z82GIKy2p5/VqvD/fzF7gm7xtWd17Ts4iIHBR1\n0Tw46qLpm9nLt3PLyytoaLLccupwzhufi1Fvna+vuRmKlsPaOa5FcvtStz6lr0v4YhIhOsF90Y5O\ncF0Uo+JdK9fOlXDCr12y1d6/wadPwKvXucTsvBegoQq2fQpbl8C2JW6CjKY6iIh23VMHTe047oYa\nuPdo9+X7igXu9r07XIti+iA4+9G9xb6thQV/hbd/5eoZfudfB7bI7VgFL1wIu9fD1J+5FtdAfIds\nanStSPPvcl01z34csvK94vRK7GKS4egr4JirXJJ4MDbMdWPKyra4FtOJ17rWqYOdeKWpAe6b7P5N\nf7Bob0K69F+uFXXU+W5sYFvvD2vd+MY3f+neT0NOhJPvdP+GgfDqtW584km/dV1E0wfDec+58bed\nqL3ro18TPH/wx8Xr5L+9T05qHA9f5MniG2rgzkFw5Cw49S+d+loiIuI7JXgHRwme74rKavnxi8t5\nf10x04ZmccdZR5CdfIhjo8SpKHI9odb8z9Wda6x1pQoaal0S1iIuFb79COSd4Ntx185xM2w21rlJ\n8MAlir1HuRIUfcdBv6MhJcf3WDe8C09+042N27HSjWk78jw4+Y+ttwAuf84lHZl5rmUpta9bv/Rf\n8N8bXXfCbz8EA6f4HoO/rH8H/n2pO++n/c0le+/d4UqAxSS7VuSjr2x7DKIvmhpg5cuu1MPOlS6p\nP+YqGHNB+y2o3j6827UEznoGhp2877a5t7tkderPYOpPDnzuloXwzm2utTItF77xezeeLpA/2Kz5\nHzzzHXd/0FSXYHdW91wvSvA6MOY3bzHz8F787pue2XpW/xeePc9Nnzt4eqe+loiI+E4J3sFRgndw\nmpstT3y0mdvfWE1CTCS3f2skMw4/iORADl5zk/shvbHWteYd7FjB7UtdkpWV78YaZo849OLtL13q\nugfGJMOpf+54Zs2N8+DZ811X0e886WZoXP60S+q+9RAk9zy0eDpT+XZ48WI3uQdAbIobL3moid3+\nrIV1b7lWzi8XuNbAY66Cide3/+9Tvh3+cRT0n+haufZPzKx13W6XP+26xraMaytaAe/8xk3Ck5jt\nZnUdc8HXm0G0szXUuBbJgVNg5h3+KSVB4CZZ6RZqG5ooqarftwbeqtluKtUBk9t+ooiIiHRrERGG\niyYOZFJeFtc/t4wr/vUp3xrdh1tPP4zUeP98KQt7EZGuZcfX1p399R7tls408w7X6jfmQt9qDQ6a\nCt9/w40Re+h4wLjxXsfd1Dk16TpTSm839mvBX1xyPf6yzk3sWhgD+Se5pWAxfPBnePe3sO5tOOvh\nvS2d+3vzFtcKOPOO1lvdjHGtjxXbYfY1e7tjfv6SawE+/lY3u2lMgMbStiY6Hq4J7A9tYZ/gfVUD\nL9UzALOx3s0INPQUv2XcIiIiEjyGZCfx7x8cy93vrueeuev5cMNu7jzrCKbkZwU6NOkKCelw4m0H\n95xeI+Hit1z3wFHnweBp/omtM0RGwZQfd93r9TsKzn3GjUN79VpXUuHMf7pZYr1tfM8lalN/2v74\ntKgYOOcJeGQm/OcHruV38o1u3KYfuj6GgrCfPaTQk+B91YK3+X2oLVNxcxERkTASHRnBDSfm8/IP\njiUpLooLHvmYW15ZQVVdY6BDk2CV1g++/WBwJ3eBNPIsuHy+q9n4zCxXN7Gx3m1rrIfXf+zKGEy8\ntuNjxaXC+S/Bib+Ba5fD8b9QcteOsE/wisr3q4H3xWyITtSHVUREJAwd0TeN166ZxCWTBvLUoi3M\n/Nv7LN5cEuiwRLqnjMGuYPn4y+Gjf8Aj33AF5hfeC8VrYOadB5YxaEtKDkz8oStvIO0K+wRvnxa8\n5iY3wUr+Sb6/2URERCSkxEVHcsupI3j20qOxWM65/yN+//oXVNerNU/koEXFurIF5zzpCtHfN8XN\n5jn05IOrvSc+C/sEr6isltT4aBJiomDrYlfZXt0zRUREwt6EQRn879opnDs+lwfmb+T4P73HK0u3\n0dzcvWYgFwkKI06HK+ZD5hBXHH3GHwIdUcgK+wSvsKx27/i7navcbb+jAxeQiIiIBI3E2Ch+/82R\nvHDFMWQmxXLdc8v41j8/5NMtewIdmkj302OAm5zmuhXQo3+gowlZYZ/gFZXV7h1/V1oAEVGQ3Cuw\nQYmIiEhQOWpAOv+5aiJ3nX0k20tr+Na9H3Lts0vZXloT6NBEupeISP+UapCvhH2Ct08LXlkBpPQJ\nvhomIiIiEnAREYazxvZl7o1TuWb6EP73eRHT/zSPP7+1ltqGpkCHJyIChHmCV9fYRHFlHb1SPBOq\nlBa4qVxFRERE2pAYG8WPThrKOz86jhNH9OLv76xjxl/ns3Dj7kCHJiIS3gnezvI6gH1b8NKU4ImI\niEjH+vZI4O5zR/P0JRNotjDrgYX8/OUVVNQ2BDo0EQljYZ3gtZRI6JUaB00NUFGoFjwRERE5KMcO\nyWTOdVO4ZNJAnvl4Cyf9ZT7vrt4R6LBEJEyFeYLnBkbnpMZB+TawzWrBExERkYMWH+Nq57105bEk\nx0Xx/ceWcN2zSympqg90aCISZsI6wSvybsEr2+pWqgVPREREvqbRuT149ZpJXHt8Hq99VsiJf36P\nRz7YRE29JmERka4R1gleYVktybFRJMdFuwlWANJyAxuUiIiIdGuxUZFcf2I+r/1wEoOzk7jttVVM\nuuNd7pm7nnKNzxMRPwvrBG+fGnhlngQvpU/gAhIREZGQMaxXCs9ffgzPX34Mh/dJ5Y9z1jDxD+9y\n15w16ropIn4TFegAAqmw3LvI+RZIzIbouMAGJSIiIiFl/MB0xg8cz+fbyrhn7nrumbeehz/YxLnj\nc7lsyqC930VERDpBWCd4RWU1DO2Z5R6oRIKIiIj40eF9Uvnn+WNZv7OCe+dt4PGPNvPkws2cNbYv\nl08ZzIDMxECHKCIhIGy7aDY0NbOzoo5eqSpyLiIiIl1nSHYyfz5nFPNunMp3jurHS59uY/qf5nHt\ns0tZXVQe6PBEpJsL2wRvZ0Ud1npKJDQ3u1k01YInIiIiXaRfegK/PXMkH9w0jUsnD+LtVTuY8df3\nueTxJSzdsifQ4YlINxW2CV6RpwZer9Q4qC6GpjpI1QyaIiIi0rWyU+L46cnDWXDzdK47IY/Fm0v4\n5r0fcsEjH7O8oDTQ4YlINxO2CV6hpwZeTmqcV4kEteCJiEjnMcbMMMasMcasN8bc3Mr2i4wxu4wx\nyzzLJV7bLjTGrPMsF3Zt5BIIaQkxXHdCPgtuns5PZw5jxdZSzrhnAZc9sURdN0XEZ2Gb4LUUOc9J\niYeyLW6lxuCJiEgnMcZEAvcAM4ERwLnGmBGt7PqctXaUZ3nI89x04FZgAjAeuNUY06OLQpcAS4qN\n4vLjBjP/pmnccGI+H23Yzcy/vc8Pn1nKxl2VgQ5PRIJc2CZ4hWW1JMREkhIfpRY8ERHxh/HAemvt\nRmttPfAscIaPz/0G8Ja1tsRauwd4C5jhpzglSCXHRfPD4/N4/yfTuPK4wby1agcn/mU+N724XIme\niLQpbBO8liLnxhhXIiE2BeJSAx2WiIiEjj5AgdfjrZ51+/u2MeYzY8yLxpiWXxp9fa6EgbSEGG6a\nMYz5N03jwmMG8Mqy7Uz/03uc+8BCXl2+nfrG5kCHKCJBJGwTvMKyGjf+DlQiQUREAuVVYIC19ghc\nK93jB3sAY8xlxpglxpglu3bt6vQAJXhkJcfyy9NG8MFPpvHjbwylYE811zyzlGNuf4fb3/iCzcVV\ngQ5RRIJAGCd4tfRK8dTAU5FzERHpfNsA74tLX8+6r1hrd1tr6zwPHwLG+vpcr2M8YK0dZ60dl5WV\n1SmBS3DLTo7jqmlDmP/jaTz+/fGMG9CDh97fxNS75nH+Q4t4eelWymsbAh2miARIVKADCIRGT5Hz\nr1rwygog95jABiUiIqFmMZBnjBmIS85mAed572CMybHWFnoeng584bk/B/i918QqJwE/9X/I0p1E\nRBiOy8/iuPwsdpTX8vziAp5dXMD1zy0nJjKCyXmZzByZw4nDe5KaEB3ocEWki4RlgldcWU9Ts3U1\n8GrLobZMLXgiItKprLWNxpircclaJPCItXalMeY2YIm1djbwQ2PM6UAjUAJc5HluiTHmN7gkEeA2\na21Jl/8R0m30TInjmuPzuGraEJYWlPLGikLe+LyId1bvJDrSMHFIJicfnsM3DuulZE8kxIVlglfo\nKXKekxrnWu9AY/BERKTTWWtfB17fb90vve7/lDZa5qy1jwCP+DVACTkREYax/Xswtn8Pfn7KcJZv\nLeONFYX8d0UhN730Gb/4z+ecdmRvzpuQy+h+aW6yOREJKWGZ4LXUwOu1T5Hz3ABGJCIiItK5jDGM\n6pfGqH5p3DxzGJ9tLeO5JQX8Z+k2XvxkK8NzUvjuhFzOHN2HpNiw/EooEpLC8tNc2FLkPDUetra0\n4PUNYEQiIiIi/mOM4ch+aRzZL42fnTyc/yzbxlMLt3DLK5/z+9e/4IxRfThrbF9G90sjIkKteiLd\nWVgmeEXltcRGRdAjIRpKt0BkDCRmBzosEREREb9Lio3iuxP6c974XJZvLeOphV/y8tKtPPPxFjKT\nYjlheDYnjujJxCGZxEVHBjpcETlIYZngFZbVkuNd5Dy1L0SEbcUIERERCUPeXTh/cdoI5q7eyZur\ndvDaZ4U8u7iA+OhIpuRncsLwnpwwvCc9EmMCHbKI+CAsE7yisho3/g5U5FxERETCXkpcNGeM6sMZ\no/pQ19jEoo0lvLVqB2+t2sGclTuIijBMysvktCN6c9JhPUmO00ycIsEqLBO87aW1jB+Y7h6UbYW8\nEwIbkIiIiEiQiI2KZEp+FlPys7jtjMP4fFs5/11RyKvLt/OjF5YT83IE04ZmcdqRvTl+WE/iY9SN\nUySY+DXBM8bMAP6Gq//zkLX2D/ttvwj4I64ALMA/rLUP+TOm5mbLjvJa14LXWAeVRZCqGTRFRERE\n9meMYWTfVEb2TeUnM4aytKCUV5dv57XPCpmzcgcJMZFMzstkdG4PRvdLY2TfVBJiwrL9QCRo+O0T\naIyJBO4BTgS2AouNMbOttav22/U5a+3V/opjf8VVdTQ2W08NvK1upYqci4iIiLTLGMOY3B6Mye3B\nLaeMYNGm3by6vJAF64uZs3IHAJERhvyeyYzOdWP7xg9IZ0BmYoAjFwkv/vyJZTyw3lq7EcAY8yxw\nBrB/gtelvqqBlxIHZavdSpVIEBEREfFZZITh2MGZHDs4E4DdlXUsKyj9anl12XaeXrQFgCHZSZw0\noicnHdaLI/qkqgyDiJ/5M8HrAxR4Pd4KTGhlv28bY6YAa4HrrbUFrezTafapgbezpQaeWvBERERE\nvq6MpFiOH96T44f3BNyQmI3FlXywrpg3V+3g/vkbuXfeBrKTYzlhRE9OGtGTYwZnEBul8XsinS3Q\nnaRfBZ6x1tYZYy4HHgem77+TMeYy4DKA3NxDGy9XXtNATGSEG4O3rgAwkNLnkI4pIiIiIntFRBiG\nZCczJDuZiyYOpLS6nrlrdvLmyh28snQbTy/aQlx0BGNye3DUgHQmDExndG4PTdgi0gn8meBtA7yb\nxvqydzIVAKy1u70ePgTc2dqBrLUPAA8AjBs3zh5KUGeP68e3x/TFGFyJhOQciFJdFxERERF/SUuI\n4Zuj+/LN0X2pbWjiww3FzF9bzMebSvj7u+uwFqIi3IQu4wekc/SgDI4ZnKFC6yJfgz8TvMVAnjFm\nIC6xmwWc572DMSbHWlvoeXg68IUf4/nKV32/ywo0wYqIiIhIF4qLjmT6sJ5MH+a6c5bXNvDJl3v4\neFMJizeV8MiCTdw/fyOJMZFMHZrNSYf1ZPqwbNXeE/GR3xI8a22jMeZqYA6uTMIj1tqVxpjbgCXW\n2tnAD40xpwONQAlwkb/iaVVZAfQZ16UvKSIiIiJ7pcRFM21oNtOGZgNQ29DEwo27mbPSFVr/74pC\noiPdpC7fOKwX04Zl0TM5TpO1iLTBWHtIPR673Lhx4+ySJUsO/UDNzfDbbDj2ajjhV4d+PBER6XTG\nmE+stfolzkeddo0UCRJNzZalW/YwZ2URc1buYEtJNQDRkYbs5Dh6pcbRKyWOnilx5KTGkZMWx5Ds\nJAZmJmoCFwlp7V0fAz3JSuBUFkFzg2bQFBEREQlSkRGGcQPSGTcgnZ+dPJzVRRV8vKmEwrJadpTX\nUlRWyxeF5cxds5Pq+qavnhdhYEBGIoOzk8jLTiKvZxJ52ckM7ZVMdGREAP8iEf8L3wSvVCUSRERE\nRLoLYwzDc1IYnpNywDZrLRV1jWzbU8P6nZWs21nJuh0VrNtZydzVO2lsdj3WYqIiOLx3Ckf2c4XY\nj+ybRv+MBIxRd08JHeGb4JV5EjxNsiIiIiLSrRljSImLJiUn+oAEsL6xmS93V/FFUQWfFZSyfGsp\nz3y8hUcXbAYgLSGaI/umMX5gOuMHpnNE31R175RuLXwTvNIt7lYteCIiIiIhKyYqgryeyeT1TOb0\nI3sD0NjUzNodlSzfWsryglKWbinlj3PWfLX/6H5pTBiYzviBGYzpn0ZCTPh+ZZbuJ3zfrWVbIb4H\nxCYFOhIRERER6UJRkRGM6J3CiN4pnDs+F4A9VfUs3lzCx5tK+HhzCf+Yu57md9cTYaB3WjwDMhLJ\nzUigf3oC/TMSyE1PpH9GAomx4ft1WoJT+L4jywrUeiciIiIiAPRIjOGkw3px0mG9AKiobeDTLaV8\n8uUeNhdX8WVJNW+sKGRPdcM+z+uTFk9+zyTyeyUztGcy+T2TGZKdpCLtEjDhm+CVFkDG4EBHISIi\nIiJBKDkumuPyszguP2uf9WU1DWzZXc2XJVVsLq5i3c5K1hRV8MH6Yhqa3GQuEQb6ZyTSIyGa+JhI\n4qMjiYt2t/ExbhmclcSY3DQGZSappp90qvBM8Kx1LXiDpgY6EhERERHpRlLjoxnZN5WRfVP3Wd/Q\n5CZzWVNUyZqictbtrKSitpGahib2VDVQ29BETUMTtQ1NVNU1Ud/UDEByXBSj+qUxul8ao3N7MKpf\nGj0SYwLxp0mICM8Er2YP1FdCat9ARyIiIiIiISA6MoIh2ckMyU7mlCNy2t23udmysbiKpVv2sNQz\nycs/5q7HU82B7OTYfcb5uSWR/ukJpCVEq6yDtCs8EzyVSBARERGRAImIMAzJTmJIdhJnj3PfR6vq\nGlmxrYxlBaVs2FnJlyXVfLB+Fy99WrfPc6MjXUmI1PhokuPdbUpcFKnx0WQkxdI7NY5eqXHkpMbT\nKzWOlLgoJYRhJjwTPBU5FxEREZEgkhgbxdGDMjh6UMY+62vqmyjYU83m4iq2lFRTXFlPeW0DZTUN\nlNe424KSaspqGthTXY+1+x43ISaSXqlxDMpMYnSuK+5+RL9UUuKiu/Cvk64Ungle2VZ3m5Yb2DhE\nRCSkGWNmAH8DIoGHrLV/2G/7DcAlQCOwC/i+tfZLz7YmYIVn1y3W2tO7LHARCRrxMZHke2bn7EhD\nUzM7K+ooKquhsKyWorJaCstqKSyrYXVRBW9/seOrfQdnJTKqXw9G9UtlcHYS6YkxpCfEkJYQQ0xU\nhD//JPGzME3wCiAqHhIyOt5XRETkazDGRAL3ACcCW4HFxpjZ1tpVXrstBcZZa6uNMVcCdwLf8Wyr\nsdaO6tKgRaRbi46MoE9aPH3S4lvdXlbTwGee4u7LCkp5b+1OXvp06wH7JcVGkZYQTXpiDBmJMfRK\njSfnq66fcZ778SSpBmBQCs9/ldItbvyd+iOLiIj/jAfWW2s3AhhjngXOAL5K8Ky1c732Xwic36UR\nikhYSY2PZnJeFpPzXOkHay1b99RQsKeaPVWui+eeqnpKqusprW6gpKqeHeV1fLa1jN1V9QccLzYq\ngoSWMhCe25ZSEMlxUWQnx9EzJY5eqbH0TPHcT4lTcXg/C8+zqyLnIiLif32AAq/HW4EJ7ex/MfCG\n1+M4Y8wSXPfNP1hrX+n8EEUknBlj6JeeQL/0hA73rW1oYmd5HYVlNRSVu66fuyvrqG1o/qr8Q0sp\niKq6RraX1vD+2mIq6hoPOFZSbBSZSTFkJsWSmRRLVrK7zUx269ITY77qMpoaH606gQcpPBO80gLo\ndUSgoxAREQHAGHM+MA44zmt1f2vtNmPMIOBdY8wKa+2GVp57GXAZQG6uxpaLiH/ERUeSm5FAbkbH\nyaC3qrpGispr2eFZisrq2FlRS3FlPbsqalm/q5KFm3ZTWt3Q6vMjDPRIiPkq6ctKjt27JO29n5kU\nS1JsFAkxkWE/a2j4JXj11VBdrBIJIiLib9sA74tNX8+6fRhjTgB+Dhxnrf1qPnRr7TbP7UZjzDxg\nNHBAgmetfQB4AGDcuHF2/+0iIoGUGBvF4KwkBmcltbtffWMzu6vq2F1ZT0nV3mVPdT27q1zX0d2V\n9azaXs6uirpWWwbBJYSJMVEkxkaRGBtJUmwUyXHR9E6Lo1+PBE+LZTz9eiSQlRwbkslg+CV4LTNo\npupXThER8avFQJ4xZiAusZsFnOe9gzFmNHA/MMNau9NrfQ+g2lpbZ4zJBCbiJmAREQlJMVER5KTG\nk5Pa+gQx+6upb6K4so6dFXXsqqijuLKOyrpGquoa97mtrGuirKaBuWt2sati35qCsVER9E6LJyrC\nYHFjEt2tuw+u5TI+JpLEmCjiYyJJ+GqJokeCqz2YnhhDZlIM6YmxZCTFkBwb2NqDYZjgbXG3asET\nERE/stY2GmOuBubgyiQ8Yq1daYy5DVhirZ0N/BFIAl7wfBloKYcwHLjfGNMMRODG4K1q9YVERMJQ\nfEykz+MHW9TUN7GttJqCEjexTEFJNdvLamluthgDBuNujaElPattaKK6vonq+kaKK+s89904w5qG\nplZfJyYyggzPGEPXfTRm7zjDpFiG56QwJLv9Fs1DEYYJXksLnhI8ERHxL2vt68Dr+637pdf9E9p4\n3ofASP9GJyISXuJjIhmSncyQ7I5rCvqitqGJEk/XUe/upcVVdRRX1FNcWceO8lo+3+ZmIW1qdq2C\nVxw3mJtnDuuUGFoTfgneiDMh+zBI6R3oSEREREREpJuKi46kd1o8vduoO+itudmyp7qe4sp6kuL8\nm4KFX4IXnwb9jgp0FCIiIiIiEiYiIgwZSbFkJMX6/7X8/goiIiIiIiLSJZTgiYiIiIiIhAgleCIi\nIiIiIiFCCZ6IiIiIiEiIUIInIiIiIiISIpTgiYiIiIiIhAgleCIiIiIiIiFCCZ6IiIiIiEiIUIIn\nIiIiIiISIpTgiYiIiIiIhAhjrQ10DAfFGLML+NKHXTOB4ja2pQJlnbzNX8f1x7auPjfdZVt75yUQ\n8QTTtlB/zxzKc0P93Pjr8+Sr/tbarE44TlgI4mtkd9n2dc+Lv+IJpm3h/J7paHs4n5tQOC+BeM3O\nuEa2fX201obkAixpZ9sDnb3NX8f107YuPTfdaFub5yUIYw2acxNkcQbi8xvS58ZfnyctgV30vu3c\n8xKEf0fQnJtQ2KZzE9rvmWA7N52xhGsXzVf9sM1fx/VXrMESSzBt60gwxRpM5yaY4gzE59cfxwyF\nbdJ9BdP7KJjet6HyHUD/1x38Nl+2d/ZrhsK29gRbnMF0bg5Zt+ui6StjzBJr7bhAxxGMdG5ap/PS\nNp2btunctE7nJbjp36d1Oi9t07lpm85N63Re2ubvcxPKLXgPBDqAIKZz0zqdl7bp3LRN56Z1Oi/B\nTf8+rdN5aZvOTdt0blqn89I2v56bkG3BExERERERCTeh3IInIiIiIiISVkIywTPGzDDGrDHGrDfG\n3BzoeALJGPOIMWanMeZzr3Xpxpi3jDHrPLc9AhljIBhj+hlj5hpjVhljVhpjrvWs17kxJs4Y87Ex\nZrnn3Pzas36gMWaR53P1nDEmJtCxBoIxJtIYs9QY85rnsc4LYIzZbIxZYYxZZoxZ4lkX9p+nYKPr\n4166PrZO18e26frYPl0fWxeI62PIJXjGmEjgHmAmMAI41xgzIrBRBdRjwIz91t0MvGOtzQPe8TwO\nN43Aj6y1I4Cjgas87xOdG6gDpltrjwRGATOMMUcDdwB/sdYOAfYAFwcwxkC6FvjC67HOy17TrLWj\nvAaO6/MURHR9PMBj6PrYGl0f26brY/t0fWxbl14fQy7BA8YD6621G6219cCzwBkBjilgrLXzgZL9\nVp8BPO65/zhwZpcGFQSstYXW2k899ytw/yH1QecG61R6HkZ7FgtMB170rA/Lc2OM6QucAjzkeWzQ\neWlP2H+egoyuj150fWydro9t0/Wxbbo+HjS/fp5CMcHrAxR4Pd7qWSd79bTWFnruFwE9AxlMoBlj\nBgCjgUXo3ABfdbNYBuwE3gI2AKXW2kbPLuH6uforcBPQ7Hmcgc5LCwu8aYz5xBhzmWedPk/BRdfH\njuk960XXxwPp+tgmXR/b1uXXx6jOPJh0P9Zaa4wJ26lUjTFJwEvAddbacveDkxPO58Za2wSMMsak\nAS8Dw2sQsSUAAAQUSURBVAIcUsAZY04FdlprPzHGTA10PEFokrV2mzEmG3jLGLPae2M4f56kewr3\n96yuj63T9fFAuj52qMuvj6HYgrcN6Of1uK9nney1wxiTA+C53RngeALCGBONu3g9Za39t2e1zo0X\na20pMBc4BkgzxrT8KBSOn6uJwOnGmM24rm3Tgb+h8wKAtXab53Yn7kvPePR5Cja6PnZM71l0ffSF\nro/70PWxHYG4PoZigrcYyPPM3BMDzAJmBzimYDMbuNBz/0LgPwGMJSA8fcMfBr6w1v7Za5POjTFZ\nnl8mMcbEAyfixmDMBc7y7BZ258Za+1NrbV9r7QDc/yvvWmu/S5ifFwBjTKIxJrnlPnAS8Dn6PAUb\nXR87FvbvWV0f26brY+t0fWxboK6PIVno3BhzMq4vcCTwiLX2dwEOKWCMMc8AU4FMYAdwK/AK8DyQ\nC3wJnGOt3X+geUgzxkwC3gdWsLe/+M9w4wzC/dwcgRvwG4n7Eeh5a+1txphBuF/m0oGlwPnW2rrA\nRRo4ni4oN1prT9V5Ac85eNnzMAp42lr7O2NMBmH+eQo2uj7upetj63R9bJuujx3T9XFfgbo+hmSC\nJyIiIiIiEo5CsYumiIiIiIhIWFKCJyIiIiIiEiKU4ImIiIiIiIQIJXgiIiIiIiIhQgmeiIiIiIhI\niFCCJ9KFjDFNxphlXsvNnXjsAcaYzzvreCIiIl1J10iRzhHV8S4i0olqrLWjAh2EiIhIENI1UqQT\nqAVPJAgYYzYbY+40xqwwxnxsjBniWT/AGPOuMeYzY8w7xphcz/qexpiXjTHLPcuxnkNFGmMeNMas\nNMa8aYyJD9gfJSIi0gl0jRQ5OErwRLpW/H7dT77jta3MWjsS+AfwV8+6u4HHrbVHAE8Bf/es/zvw\nnrX2SGAMsNKzPg+4x/5/O3esWlUQhAH4HyRFICCiTSCFjZVtnsBXsFCxClYpglXwBXwKm7xGQKwE\nbcUHELsISWGRJkiYFDmBW0RI8N7cy/p9zZnd4jBbDbN79nQ/TfI7yfMFrwcA5kWNhDmo7l52DvDf\nqKrT7t64Zv5nkmfd/aOq1pL86u6HVXWSZLO7/0zzR939qKqOk2x199nMOx4n+djdT6bxuyRr3f1+\n8SsDgH+jRsJ8OMGD1dF/iW/jbCY+j3u2AIxBjYQb0uDB6ngx8/w6xV+SvJzi10k+T/GnJLtJUlX3\nqur+XSUJAEugRsIN2bmAu7VeVd9mxofdffUb6AdV9T2XO4yvprm9JAdVtZ/kOMnONP82yYeqepPL\nXcjdJEcLzx4AFkeNhDlwBw9WwHS/YLu7T5adCwCsEjUSbscnmgAAAINwggcAADAIJ3gAAACD0OAB\nAAAMQoMHAAAwCA0eAADAIDR4AAAAg9DgAQAADOICdffIJG7GYoQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RTBPdJvGQul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trXIUPBIV-DC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlRrpOvDV_wg",
        "colab_type": "text"
      },
      "source": [
        "## Trying Leslie Smith LR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bvn4rU8DWDyH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = []\n",
        "def lr_schedule(epoch, lr): #Implements cyclical learning rate\n",
        "  step_size = 2300 #One triangle is completed at the 46th epoch and the learning rate remains the base rate for the rest 4 epochs\n",
        "  iterations = epoch * 100\n",
        "  base_lr = .1 #base learning rate\n",
        "  max_lr = 1.5 #max learning rate\n",
        "  cycle = np.floor(1+iterations/(2*step_size))\n",
        "  x = np.abs(iterations/step_size - 2*cycle + 1)\n",
        "  new_lr = base_lr + (max_lr-base_lr)*np.maximum(0, (1-x))\n",
        "  print((lr, new_lr))\n",
        "  if(round(lr, 2) <= base_lr and epoch > 1):\n",
        "    return base_lr \n",
        "  else:\n",
        "    return new_lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqjSzgg7W9BJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "588628ec-1105-43a6-8f53-0a73cca5d5ce"
      },
      "source": [
        "if version == 2:\n",
        "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
        "else:\n",
        "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "print(model_type)\n",
        "\n",
        "# Prepare model model saving directory.\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(zoom_range=0.0, \n",
        "                             horizontal_flip=False)\n",
        "\n",
        "\n",
        "# train the model\n",
        "start = time.time()\n",
        "# Train the model\n",
        "model_info = model.fit_generator(datagen.flow(x_train, y_train, batch_size = 128),\n",
        "                                 samples_per_epoch = x_train.shape[0], nb_epoch = 50, \n",
        "                                 callbacks=[LearningRateScheduler(lr_schedule, verbose=1)], #Added Rohan's Learning rate scheduler\n",
        "                                 validation_data = (x_test, y_test), verbose=1)\n",
        "\n",
        "                                            \n",
        "end = time.time()\n",
        "print (\"Model took %0.2f seconds to train\"%(end - start))\n",
        "# plot model history\n",
        "plot_model_history(model_info)\n",
        "# compute test accuracy"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_11\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_11 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_218 (Conv2D)             (None, 32, 32, 32)   896         input_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_191 (BatchN (None, 32, 32, 32)   128         conv2d_218[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_191 (Activation)     (None, 32, 32, 32)   0           batch_normalization_191[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_93 (Dropout)            (None, 32, 32, 32)   0           activation_191[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_219 (Conv2D)             (None, 32, 32, 32)   1056        dropout_93[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_192 (BatchN (None, 32, 32, 32)   128         conv2d_219[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_192 (Activation)     (None, 32, 32, 32)   0           batch_normalization_192[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_94 (Dropout)            (None, 32, 32, 32)   0           activation_192[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_220 (Conv2D)             (None, 32, 32, 32)   9248        dropout_94[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_193 (BatchN (None, 32, 32, 32)   128         conv2d_220[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_193 (Activation)     (None, 32, 32, 32)   0           batch_normalization_193[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_95 (Dropout)            (None, 32, 32, 32)   0           activation_193[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_222 (Conv2D)             (None, 32, 32, 128)  4224        dropout_93[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_221 (Conv2D)             (None, 32, 32, 128)  4224        dropout_95[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_70 (Add)                    (None, 32, 32, 128)  0           conv2d_222[0][0]                 \n",
            "                                                                 conv2d_221[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_194 (BatchN (None, 32, 32, 128)  512         add_70[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_194 (Activation)     (None, 32, 32, 128)  0           batch_normalization_194[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_96 (Dropout)            (None, 32, 32, 128)  0           activation_194[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_223 (Conv2D)             (None, 32, 32, 32)   4128        dropout_96[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_195 (BatchN (None, 32, 32, 32)   128         conv2d_223[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_195 (Activation)     (None, 32, 32, 32)   0           batch_normalization_195[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_97 (Dropout)            (None, 32, 32, 32)   0           activation_195[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_224 (Conv2D)             (None, 32, 32, 32)   9248        dropout_97[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_196 (BatchN (None, 32, 32, 32)   128         conv2d_224[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_196 (Activation)     (None, 32, 32, 32)   0           batch_normalization_196[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_98 (Dropout)            (None, 32, 32, 32)   0           activation_196[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_225 (Conv2D)             (None, 32, 32, 128)  4224        dropout_98[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_71 (Add)                    (None, 32, 32, 128)  0           add_70[0][0]                     \n",
            "                                                                 conv2d_225[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_197 (BatchN (None, 32, 32, 128)  512         add_71[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_197 (Activation)     (None, 32, 32, 128)  0           batch_normalization_197[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_99 (Dropout)            (None, 32, 32, 128)  0           activation_197[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_226 (Conv2D)             (None, 16, 16, 128)  16512       dropout_99[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_198 (BatchN (None, 16, 16, 128)  512         conv2d_226[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_198 (Activation)     (None, 16, 16, 128)  0           batch_normalization_198[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_100 (Dropout)           (None, 16, 16, 128)  0           activation_198[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_227 (Conv2D)             (None, 16, 16, 128)  147584      dropout_100[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_199 (BatchN (None, 16, 16, 128)  512         conv2d_227[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_199 (Activation)     (None, 16, 16, 128)  0           batch_normalization_199[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_101 (Dropout)           (None, 16, 16, 128)  0           activation_199[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_229 (Conv2D)             (None, 16, 16, 256)  33024       add_71[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_228 (Conv2D)             (None, 16, 16, 256)  33024       dropout_101[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "add_72 (Add)                    (None, 16, 16, 256)  0           conv2d_229[0][0]                 \n",
            "                                                                 conv2d_228[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_200 (BatchN (None, 16, 16, 256)  1024        add_72[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_200 (Activation)     (None, 16, 16, 256)  0           batch_normalization_200[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_102 (Dropout)           (None, 16, 16, 256)  0           activation_200[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_230 (Conv2D)             (None, 16, 16, 128)  32896       dropout_102[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_201 (BatchN (None, 16, 16, 128)  512         conv2d_230[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_201 (Activation)     (None, 16, 16, 128)  0           batch_normalization_201[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_103 (Dropout)           (None, 16, 16, 128)  0           activation_201[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_231 (Conv2D)             (None, 16, 16, 128)  147584      dropout_103[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_202 (BatchN (None, 16, 16, 128)  512         conv2d_231[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_202 (Activation)     (None, 16, 16, 128)  0           batch_normalization_202[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_104 (Dropout)           (None, 16, 16, 128)  0           activation_202[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_232 (Conv2D)             (None, 16, 16, 256)  33024       dropout_104[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "add_73 (Add)                    (None, 16, 16, 256)  0           add_72[0][0]                     \n",
            "                                                                 conv2d_232[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_203 (BatchN (None, 16, 16, 256)  1024        add_73[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_203 (Activation)     (None, 16, 16, 256)  0           batch_normalization_203[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_105 (Dropout)           (None, 16, 16, 256)  0           activation_203[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_233 (Conv2D)             (None, 8, 8, 256)    65792       dropout_105[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_204 (BatchN (None, 8, 8, 256)    1024        conv2d_233[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_204 (Activation)     (None, 8, 8, 256)    0           batch_normalization_204[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_106 (Dropout)           (None, 8, 8, 256)    0           activation_204[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_234 (Conv2D)             (None, 8, 8, 256)    590080      dropout_106[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_205 (BatchN (None, 8, 8, 256)    1024        conv2d_234[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_205 (Activation)     (None, 8, 8, 256)    0           batch_normalization_205[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_107 (Dropout)           (None, 8, 8, 256)    0           activation_205[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_236 (Conv2D)             (None, 8, 8, 512)    131584      add_73[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_235 (Conv2D)             (None, 8, 8, 512)    131584      dropout_107[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "add_74 (Add)                    (None, 8, 8, 512)    0           conv2d_236[0][0]                 \n",
            "                                                                 conv2d_235[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_206 (BatchN (None, 8, 8, 512)    2048        add_74[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_206 (Activation)     (None, 8, 8, 512)    0           batch_normalization_206[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_108 (Dropout)           (None, 8, 8, 512)    0           activation_206[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_237 (Conv2D)             (None, 8, 8, 256)    131328      dropout_108[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_207 (BatchN (None, 8, 8, 256)    1024        conv2d_237[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_207 (Activation)     (None, 8, 8, 256)    0           batch_normalization_207[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_109 (Dropout)           (None, 8, 8, 256)    0           activation_207[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_238 (Conv2D)             (None, 8, 8, 256)    590080      dropout_109[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_208 (BatchN (None, 8, 8, 256)    1024        conv2d_238[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_208 (Activation)     (None, 8, 8, 256)    0           batch_normalization_208[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_110 (Dropout)           (None, 8, 8, 256)    0           activation_208[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_239 (Conv2D)             (None, 8, 8, 512)    131584      dropout_110[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "add_75 (Add)                    (None, 8, 8, 512)    0           add_74[0][0]                     \n",
            "                                                                 conv2d_239[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_209 (BatchN (None, 8, 8, 512)    2048        add_75[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_209 (Activation)     (None, 8, 8, 512)    0           batch_normalization_209[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_11 (AveragePo (None, 1, 1, 512)    0           activation_209[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_11 (Flatten)            (None, 512)          0           average_pooling2d_11[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 10)           5130        flatten_11[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 2,272,010\n",
            "Trainable params: 2,265,034\n",
            "Non-trainable params: 6,976\n",
            "__________________________________________________________________________________________________\n",
            "ResNet20v1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., callbacks=[<keras.ca..., validation_data=(array([[[..., verbose=1, steps_per_epoch=390, epochs=50)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "(0.0010000000474974513, 0.1)\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.1.\n",
            "390/390 [==============================] - 114s 292ms/step - loss: 2.9647 - acc: 0.2208 - val_loss: 2.2918 - val_acc: 0.2163\n",
            "Epoch 2/50\n",
            "(0.10000000149011612, 0.16086956521739126)\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.16086956521739126.\n",
            "390/390 [==============================] - 93s 240ms/step - loss: 2.1388 - acc: 0.2719 - val_loss: 3.6396 - val_acc: 0.1158\n",
            "Epoch 3/50\n",
            "(0.1608695685863495, 0.22173913043478252)\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.22173913043478252.\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 2.1940 - acc: 0.2779 - val_loss: 3.7400 - val_acc: 0.1000\n",
            "Epoch 4/50\n",
            "(0.22173912823200226, 0.2826086956521738)\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.2826086956521738.\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 2.3096 - acc: 0.2651 - val_loss: 2.8805 - val_acc: 0.1001\n",
            "Epoch 5/50\n",
            "(0.28260868787765503, 0.343478260869565)\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.343478260869565.\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 2.2759 - acc: 0.2560 - val_loss: 2.4782 - val_acc: 0.1890\n",
            "Epoch 6/50\n",
            "(0.343478262424469, 0.40434782608695663)\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.40434782608695663.\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 6.8113 - acc: 0.1707 - val_loss: 28.2164 - val_acc: 0.1000\n",
            "Epoch 7/50\n",
            "(0.40434783697128296, 0.4652173913043479)\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.4652173913043479.\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 13.3580 - acc: 0.1290 - val_loss: 11.5581 - val_acc: 0.1000\n",
            "Epoch 8/50\n",
            "(0.46521738171577454, 0.5260869565217391)\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.5260869565217391.\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 12.7593 - acc: 0.1035 - val_loss: 11.7646 - val_acc: 0.1080\n",
            "Epoch 9/50\n",
            "(0.5260869860649109, 0.5869565217391304)\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.5869565217391304.\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 13.4382 - acc: 0.1266 - val_loss: 25.9578 - val_acc: 0.1000\n",
            "Epoch 10/50\n",
            "(0.5869565010070801, 0.6478260869565217)\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.6478260869565217.\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 42.2295 - acc: 0.0995 - val_loss: 31.3709 - val_acc: 0.1000\n",
            "Epoch 11/50\n",
            "(0.647826075553894, 0.7086956521739128)\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.7086956521739128.\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 65.1968 - acc: 0.1117 - val_loss: 62.6709 - val_acc: 0.1000\n",
            "Epoch 12/50\n",
            "(0.708695650100708, 0.7695652173913041)\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.7695652173913041.\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 83.5347 - acc: 0.1042 - val_loss: 70.9355 - val_acc: 0.1000\n",
            "Epoch 13/50\n",
            "(0.769565224647522, 0.8304347826086957)\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.8304347826086957.\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 87.9265 - acc: 0.1029 - val_loss: 71.6483 - val_acc: 0.1000\n",
            "Epoch 14/50\n",
            "(0.8304347991943359, 0.8913043478260867)\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.8913043478260867.\n",
            "390/390 [==============================] - 93s 240ms/step - loss: 77.4536 - acc: 0.1121 - val_loss: 175.7827 - val_acc: 0.0521\n",
            "Epoch 15/50\n",
            "(0.8913043737411499, 0.9521739130434782)\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.9521739130434782.\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 104.5457 - acc: 0.0995 - val_loss: 171.9236 - val_acc: 0.1026\n",
            "Epoch 16/50\n",
            "(0.9521738886833191, 1.0130434782608695)\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 1.0130434782608695.\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 205.9256 - acc: 0.0990 - val_loss: 255.5990 - val_acc: 0.1000\n",
            "Epoch 17/50\n",
            "(1.0130435228347778, 1.0739130434782609)\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 1.0739130434782609.\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 302.7675 - acc: 0.0996 - val_loss: 340.0557 - val_acc: 0.1000\n",
            "Epoch 18/50\n",
            "(1.0739130973815918, 1.134782608695652)\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 1.134782608695652.\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 246.8586 - acc: 0.0999 - val_loss: 202.9406 - val_acc: 0.1000\n",
            "Epoch 19/50\n",
            "(1.1347825527191162, 1.1956521739130435)\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 1.1956521739130435.\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 416.7947 - acc: 0.1018 - val_loss: 351.3351 - val_acc: 0.1000\n",
            "Epoch 20/50\n",
            "(1.1956521272659302, 1.2565217391304349)\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 1.2565217391304349.\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 438.5795 - acc: 0.1012 - val_loss: 351.0276 - val_acc: 0.1000\n",
            "Epoch 21/50\n",
            "(1.2565217018127441, 1.3173913043478258)\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 1.3173913043478258.\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 392.6788 - acc: 0.1001 - val_loss: 336.3547 - val_acc: 0.1000\n",
            "Epoch 22/50\n",
            "(1.317391276359558, 1.3782608695652174)\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 1.3782608695652174.\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 363.6051 - acc: 0.1011 - val_loss: 575.3742 - val_acc: 0.1000\n",
            "Epoch 23/50\n",
            "(1.378260850906372, 1.4391304347826088)\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 1.4391304347826088.\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 448.0737 - acc: 0.0985 - val_loss: 457.0072 - val_acc: 0.0977\n",
            "Epoch 24/50\n",
            "(1.439130425453186, 1.5)\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 1.5.\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 321.6575 - acc: 0.1014 - val_loss: 394.6373 - val_acc: 0.1000\n",
            "Epoch 25/50\n",
            "(1.5, 1.4391304347826088)\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 1.4391304347826088.\n",
            "390/390 [==============================] - 93s 239ms/step - loss: 368.1858 - acc: 0.0986 - val_loss: 795.4331 - val_acc: 0.1000\n",
            "Epoch 26/50\n",
            "(1.439130425453186, 1.3782608695652174)\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 1.3782608695652174.\n",
            "165/390 [===========>..................] - ETA: 51s - loss: 484.8210 - acc: 0.0986"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-8ad38023c809>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m                                  \u001b[0msamples_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                                  \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLearningRateScheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_schedule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#Added Rohan's Learning rate scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                                  validation_data = (x_test, y_test), verbose=1)\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7IDv_ZFYGLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9pRSjFvquAG",
        "colab_type": "text"
      },
      "source": [
        "### Leslie Smith LR didn't work. May be my base Lr and max lr are wrong"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkvPlpwQrIrB",
        "colab_type": "text"
      },
      "source": [
        "## Resnet20V1 with 32 filters and extra layer of dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtTn5lyvqzvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "            x = Dropout(0.1)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "            x = Dropout(0.1)(x)\n",
        "        x = conv(x)\n",
        "        x = Dropout(0.1)(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLaHZIGerUnY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training parameters\n",
        "batch_size = 128  # orig paper trained all networks with batch_size=128\n",
        "epochs = 200\n",
        "data_augmentation = True\n",
        "num_classes = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA-bSxIPrd_7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "34c6da9a-c016-49e9-9b61-c2396d5deeb2"
      },
      "source": [
        "def lr_schedule(epoch, lr):\n",
        "  return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "if version == 2:\n",
        "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
        "else:\n",
        "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "print(model_type)\n",
        "\n",
        "# Prepare model model saving directory.\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(zoom_range=0.0, \n",
        "                             horizontal_flip=False)\n",
        "\n",
        "\n",
        "# train the model\n",
        "start = time.time()\n",
        "# Train the model\n",
        "model_info = model.fit_generator(datagen.flow(x_train, y_train, batch_size = 128),\n",
        "                                 samples_per_epoch = x_train.shape[0], nb_epoch = 50, \n",
        "                                 callbacks=[LearningRateScheduler(lr_schedule, verbose=1)], #Added Rohan's Learning rate scheduler\n",
        "                                 validation_data = (x_test, y_test), verbose=1)\n",
        "\n",
        "                                            \n",
        "end = time.time()\n",
        "print (\"Model took %0.2f seconds to train\"%(end - start))\n",
        "# plot model history\n",
        "plot_model_history(model_info)\n",
        "# compute test accuracy"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_12\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_12 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_240 (Conv2D)             (None, 32, 32, 32)   896         input_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_210 (BatchN (None, 32, 32, 32)   128         conv2d_240[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_210 (Activation)     (None, 32, 32, 32)   0           batch_normalization_210[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_111 (Dropout)           (None, 32, 32, 32)   0           activation_210[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_241 (Conv2D)             (None, 32, 32, 32)   1056        dropout_111[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_112 (Dropout)           (None, 32, 32, 32)   0           conv2d_241[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_211 (BatchN (None, 32, 32, 32)   128         dropout_112[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_211 (Activation)     (None, 32, 32, 32)   0           batch_normalization_211[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_113 (Dropout)           (None, 32, 32, 32)   0           activation_211[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_242 (Conv2D)             (None, 32, 32, 32)   9248        dropout_113[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_114 (Dropout)           (None, 32, 32, 32)   0           conv2d_242[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_212 (BatchN (None, 32, 32, 32)   128         dropout_114[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_212 (Activation)     (None, 32, 32, 32)   0           batch_normalization_212[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_115 (Dropout)           (None, 32, 32, 32)   0           activation_212[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_243 (Conv2D)             (None, 32, 32, 128)  4224        dropout_115[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_244 (Conv2D)             (None, 32, 32, 128)  4224        dropout_111[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_116 (Dropout)           (None, 32, 32, 128)  0           conv2d_243[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_76 (Add)                    (None, 32, 32, 128)  0           conv2d_244[0][0]                 \n",
            "                                                                 dropout_116[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_213 (BatchN (None, 32, 32, 128)  512         add_76[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_213 (Activation)     (None, 32, 32, 128)  0           batch_normalization_213[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_117 (Dropout)           (None, 32, 32, 128)  0           activation_213[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_245 (Conv2D)             (None, 32, 32, 32)   4128        dropout_117[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_118 (Dropout)           (None, 32, 32, 32)   0           conv2d_245[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_214 (BatchN (None, 32, 32, 32)   128         dropout_118[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_214 (Activation)     (None, 32, 32, 32)   0           batch_normalization_214[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_119 (Dropout)           (None, 32, 32, 32)   0           activation_214[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_246 (Conv2D)             (None, 32, 32, 32)   9248        dropout_119[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_120 (Dropout)           (None, 32, 32, 32)   0           conv2d_246[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_215 (BatchN (None, 32, 32, 32)   128         dropout_120[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_215 (Activation)     (None, 32, 32, 32)   0           batch_normalization_215[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_121 (Dropout)           (None, 32, 32, 32)   0           activation_215[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_247 (Conv2D)             (None, 32, 32, 128)  4224        dropout_121[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_122 (Dropout)           (None, 32, 32, 128)  0           conv2d_247[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_77 (Add)                    (None, 32, 32, 128)  0           add_76[0][0]                     \n",
            "                                                                 dropout_122[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_216 (BatchN (None, 32, 32, 128)  512         add_77[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_216 (Activation)     (None, 32, 32, 128)  0           batch_normalization_216[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_123 (Dropout)           (None, 32, 32, 128)  0           activation_216[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_248 (Conv2D)             (None, 16, 16, 128)  16512       dropout_123[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_124 (Dropout)           (None, 16, 16, 128)  0           conv2d_248[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_217 (BatchN (None, 16, 16, 128)  512         dropout_124[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_217 (Activation)     (None, 16, 16, 128)  0           batch_normalization_217[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_125 (Dropout)           (None, 16, 16, 128)  0           activation_217[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_249 (Conv2D)             (None, 16, 16, 128)  147584      dropout_125[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_126 (Dropout)           (None, 16, 16, 128)  0           conv2d_249[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_218 (BatchN (None, 16, 16, 128)  512         dropout_126[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_218 (Activation)     (None, 16, 16, 128)  0           batch_normalization_218[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_127 (Dropout)           (None, 16, 16, 128)  0           activation_218[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_250 (Conv2D)             (None, 16, 16, 256)  33024       dropout_127[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_251 (Conv2D)             (None, 16, 16, 256)  33024       add_77[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_128 (Dropout)           (None, 16, 16, 256)  0           conv2d_250[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_78 (Add)                    (None, 16, 16, 256)  0           conv2d_251[0][0]                 \n",
            "                                                                 dropout_128[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_219 (BatchN (None, 16, 16, 256)  1024        add_78[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_219 (Activation)     (None, 16, 16, 256)  0           batch_normalization_219[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_129 (Dropout)           (None, 16, 16, 256)  0           activation_219[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_252 (Conv2D)             (None, 16, 16, 128)  32896       dropout_129[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_130 (Dropout)           (None, 16, 16, 128)  0           conv2d_252[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_220 (BatchN (None, 16, 16, 128)  512         dropout_130[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_220 (Activation)     (None, 16, 16, 128)  0           batch_normalization_220[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_131 (Dropout)           (None, 16, 16, 128)  0           activation_220[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_253 (Conv2D)             (None, 16, 16, 128)  147584      dropout_131[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_132 (Dropout)           (None, 16, 16, 128)  0           conv2d_253[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_221 (BatchN (None, 16, 16, 128)  512         dropout_132[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_221 (Activation)     (None, 16, 16, 128)  0           batch_normalization_221[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_133 (Dropout)           (None, 16, 16, 128)  0           activation_221[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_254 (Conv2D)             (None, 16, 16, 256)  33024       dropout_133[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_134 (Dropout)           (None, 16, 16, 256)  0           conv2d_254[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_79 (Add)                    (None, 16, 16, 256)  0           add_78[0][0]                     \n",
            "                                                                 dropout_134[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_222 (BatchN (None, 16, 16, 256)  1024        add_79[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_222 (Activation)     (None, 16, 16, 256)  0           batch_normalization_222[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_135 (Dropout)           (None, 16, 16, 256)  0           activation_222[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_255 (Conv2D)             (None, 8, 8, 256)    65792       dropout_135[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_136 (Dropout)           (None, 8, 8, 256)    0           conv2d_255[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_223 (BatchN (None, 8, 8, 256)    1024        dropout_136[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_223 (Activation)     (None, 8, 8, 256)    0           batch_normalization_223[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_137 (Dropout)           (None, 8, 8, 256)    0           activation_223[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_256 (Conv2D)             (None, 8, 8, 256)    590080      dropout_137[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_138 (Dropout)           (None, 8, 8, 256)    0           conv2d_256[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_224 (BatchN (None, 8, 8, 256)    1024        dropout_138[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_224 (Activation)     (None, 8, 8, 256)    0           batch_normalization_224[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_139 (Dropout)           (None, 8, 8, 256)    0           activation_224[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_257 (Conv2D)             (None, 8, 8, 512)    131584      dropout_139[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_258 (Conv2D)             (None, 8, 8, 512)    131584      add_79[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_140 (Dropout)           (None, 8, 8, 512)    0           conv2d_257[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_80 (Add)                    (None, 8, 8, 512)    0           conv2d_258[0][0]                 \n",
            "                                                                 dropout_140[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_225 (BatchN (None, 8, 8, 512)    2048        add_80[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_225 (Activation)     (None, 8, 8, 512)    0           batch_normalization_225[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_141 (Dropout)           (None, 8, 8, 512)    0           activation_225[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_259 (Conv2D)             (None, 8, 8, 256)    131328      dropout_141[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_142 (Dropout)           (None, 8, 8, 256)    0           conv2d_259[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_226 (BatchN (None, 8, 8, 256)    1024        dropout_142[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_226 (Activation)     (None, 8, 8, 256)    0           batch_normalization_226[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_143 (Dropout)           (None, 8, 8, 256)    0           activation_226[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_260 (Conv2D)             (None, 8, 8, 256)    590080      dropout_143[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_144 (Dropout)           (None, 8, 8, 256)    0           conv2d_260[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_227 (BatchN (None, 8, 8, 256)    1024        dropout_144[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_227 (Activation)     (None, 8, 8, 256)    0           batch_normalization_227[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_145 (Dropout)           (None, 8, 8, 256)    0           activation_227[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_261 (Conv2D)             (None, 8, 8, 512)    131584      dropout_145[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_146 (Dropout)           (None, 8, 8, 512)    0           conv2d_261[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_81 (Add)                    (None, 8, 8, 512)    0           add_80[0][0]                     \n",
            "                                                                 dropout_146[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_228 (BatchN (None, 8, 8, 512)    2048        add_81[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_228 (Activation)     (None, 8, 8, 512)    0           batch_normalization_228[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_12 (AveragePo (None, 1, 1, 512)    0           activation_228[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_12 (Flatten)            (None, 512)          0           average_pooling2d_12[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 10)           5130        flatten_12[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 2,272,010\n",
            "Trainable params: 2,265,034\n",
            "Non-trainable params: 6,976\n",
            "__________________________________________________________________________________________________\n",
            "ResNet20v1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., callbacks=[<keras.ca..., validation_data=(array([[[..., verbose=1, steps_per_epoch=390, epochs=50)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "390/390 [==============================] - 143s 366ms/step - loss: 1.9072 - acc: 0.4753 - val_loss: 1.7910 - val_acc: 0.4889\n",
            "Epoch 2/50\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "390/390 [==============================] - 119s 306ms/step - loss: 1.3661 - acc: 0.6151 - val_loss: 1.5520 - val_acc: 0.5752\n",
            "Epoch 3/50\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "390/390 [==============================] - 119s 306ms/step - loss: 1.1904 - acc: 0.6680 - val_loss: 1.3042 - val_acc: 0.6277\n",
            "Epoch 4/50\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "390/390 [==============================] - 119s 305ms/step - loss: 1.0961 - acc: 0.6981 - val_loss: 1.3799 - val_acc: 0.6149\n",
            "Epoch 5/50\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "390/390 [==============================] - 119s 306ms/step - loss: 1.0220 - acc: 0.7212 - val_loss: 1.0501 - val_acc: 0.7138\n",
            "Epoch 6/50\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "390/390 [==============================] - 119s 305ms/step - loss: 0.9535 - acc: 0.7444 - val_loss: 1.0419 - val_acc: 0.7153\n",
            "Epoch 7/50\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "390/390 [==============================] - 119s 305ms/step - loss: 0.8943 - acc: 0.7650 - val_loss: 1.3332 - val_acc: 0.6569\n",
            "Epoch 8/50\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "390/390 [==============================] - 119s 306ms/step - loss: 0.8364 - acc: 0.7847 - val_loss: 1.0065 - val_acc: 0.7258\n",
            "Epoch 9/50\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "390/390 [==============================] - 119s 306ms/step - loss: 0.7937 - acc: 0.8005 - val_loss: 0.9456 - val_acc: 0.7448\n",
            "Epoch 10/50\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "390/390 [==============================] - 119s 305ms/step - loss: 0.7551 - acc: 0.8109 - val_loss: 0.8740 - val_acc: 0.7792\n",
            "Epoch 11/50\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "390/390 [==============================] - 119s 306ms/step - loss: 0.7173 - acc: 0.8251 - val_loss: 0.8786 - val_acc: 0.7800\n",
            "Epoch 12/50\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "390/390 [==============================] - 119s 305ms/step - loss: 0.6928 - acc: 0.8324 - val_loss: 0.8844 - val_acc: 0.7805\n",
            "Epoch 13/50\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "390/390 [==============================] - 119s 305ms/step - loss: 0.6622 - acc: 0.8390 - val_loss: 0.7903 - val_acc: 0.7999\n",
            "Epoch 14/50\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "390/390 [==============================] - 119s 305ms/step - loss: 0.6422 - acc: 0.8487 - val_loss: 0.7463 - val_acc: 0.8216\n",
            "Epoch 15/50\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "390/390 [==============================] - 119s 305ms/step - loss: 0.6186 - acc: 0.8544 - val_loss: 0.8145 - val_acc: 0.8049\n",
            "Epoch 16/50\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "390/390 [==============================] - 119s 305ms/step - loss: 0.5928 - acc: 0.8619 - val_loss: 0.7951 - val_acc: 0.8097\n",
            "Epoch 17/50\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "390/390 [==============================] - 119s 305ms/step - loss: 0.5762 - acc: 0.8678 - val_loss: 0.7269 - val_acc: 0.8245\n",
            "Epoch 18/50\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "390/390 [==============================] - 119s 305ms/step - loss: 0.5557 - acc: 0.8737 - val_loss: 0.7976 - val_acc: 0.8053\n",
            "Epoch 19/50\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "390/390 [==============================] - 119s 305ms/step - loss: 0.5374 - acc: 0.8796 - val_loss: 0.7058 - val_acc: 0.8399\n",
            "Epoch 20/50\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "390/390 [==============================] - 119s 306ms/step - loss: 0.5186 - acc: 0.8851 - val_loss: 0.7409 - val_acc: 0.8253\n",
            "Epoch 21/50\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0004065041.\n",
            "390/390 [==============================] - 119s 305ms/step - loss: 0.5080 - acc: 0.8877 - val_loss: 0.6810 - val_acc: 0.8417\n",
            "Epoch 22/50\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.000389661.\n",
            "390/390 [==============================] - 119s 306ms/step - loss: 0.4959 - acc: 0.8928 - val_loss: 0.6677 - val_acc: 0.8514\n",
            "Epoch 23/50\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0003741581.\n",
            "390/390 [==============================] - 119s 306ms/step - loss: 0.4749 - acc: 0.8981 - val_loss: 0.7233 - val_acc: 0.8376\n",
            "Epoch 24/50\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0003598417.\n",
            "390/390 [==============================] - 119s 306ms/step - loss: 0.4640 - acc: 0.9016 - val_loss: 0.6509 - val_acc: 0.8541\n",
            "Epoch 25/50\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0003465804.\n",
            "390/390 [==============================] - 119s 306ms/step - loss: 0.4560 - acc: 0.9038 - val_loss: 0.6601 - val_acc: 0.8539\n",
            "Epoch 26/50\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0003342618.\n",
            "390/390 [==============================] - 119s 306ms/step - loss: 0.4437 - acc: 0.9079 - val_loss: 0.7680 - val_acc: 0.8254\n",
            "Epoch 27/50\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0003227889.\n",
            "390/390 [==============================] - 119s 306ms/step - loss: 0.4350 - acc: 0.9095 - val_loss: 0.6271 - val_acc: 0.8587\n",
            "Epoch 28/50\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0003120774.\n",
            "390/390 [==============================] - 119s 306ms/step - loss: 0.4234 - acc: 0.9147 - val_loss: 0.6838 - val_acc: 0.8514\n",
            "Epoch 29/50\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.000302054.\n",
            "390/390 [==============================] - 119s 305ms/step - loss: 0.4104 - acc: 0.9190 - val_loss: 0.6332 - val_acc: 0.8620\n",
            "Epoch 30/50\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0002926544.\n",
            "390/390 [==============================] - 119s 305ms/step - loss: 0.4012 - acc: 0.9226 - val_loss: 0.6999 - val_acc: 0.8548\n",
            "Epoch 31/50\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0002838221.\n",
            "390/390 [==============================] - 119s 306ms/step - loss: 0.3929 - acc: 0.9229 - val_loss: 0.6897 - val_acc: 0.8562\n",
            "Epoch 32/50\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0002755074.\n",
            "390/390 [==============================] - 119s 305ms/step - loss: 0.3829 - acc: 0.9277 - val_loss: 0.7206 - val_acc: 0.8438\n",
            "Epoch 33/50\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.000267666.\n",
            "390/390 [==============================] - 119s 306ms/step - loss: 0.3742 - acc: 0.9313 - val_loss: 0.7089 - val_acc: 0.8563\n",
            "Epoch 34/50\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0002602585.\n",
            "390/390 [==============================] - 119s 306ms/step - loss: 0.3633 - acc: 0.9333 - val_loss: 0.7232 - val_acc: 0.8453\n",
            "Epoch 35/50\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.00025325.\n",
            "390/390 [==============================] - 119s 306ms/step - loss: 0.3642 - acc: 0.9321 - val_loss: 0.6663 - val_acc: 0.8610\n",
            "Epoch 36/50\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0002466091.\n",
            "390/390 [==============================] - 119s 306ms/step - loss: 0.3519 - acc: 0.9351 - val_loss: 0.7298 - val_acc: 0.8474\n",
            "Epoch 37/50\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0002403076.\n",
            "390/390 [==============================] - 119s 305ms/step - loss: 0.3445 - acc: 0.9386 - val_loss: 0.7246 - val_acc: 0.8414\n",
            "Epoch 38/50\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0002343201.\n",
            "390/390 [==============================] - 119s 305ms/step - loss: 0.3393 - acc: 0.9402 - val_loss: 0.6751 - val_acc: 0.8599\n",
            "Epoch 39/50\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0002286237.\n",
            "390/390 [==============================] - 119s 305ms/step - loss: 0.3353 - acc: 0.9413 - val_loss: 0.6680 - val_acc: 0.8602\n",
            "Epoch 40/50\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0002231977.\n",
            "390/390 [==============================] - 119s 305ms/step - loss: 0.3254 - acc: 0.9445 - val_loss: 0.8181 - val_acc: 0.8364\n",
            "Epoch 41/50\n",
            "\n",
            "Epoch 00041: LearningRateScheduler setting learning rate to 0.0002180233.\n",
            "390/390 [==============================] - 119s 306ms/step - loss: 0.3249 - acc: 0.9450 - val_loss: 0.6852 - val_acc: 0.8630\n",
            "Epoch 42/50\n",
            "\n",
            "Epoch 00042: LearningRateScheduler setting learning rate to 0.0002130833.\n",
            "390/390 [==============================] - 119s 306ms/step - loss: 0.3133 - acc: 0.9480 - val_loss: 0.6811 - val_acc: 0.8625\n",
            "Epoch 43/50\n",
            "\n",
            "Epoch 00043: LearningRateScheduler setting learning rate to 0.0002083623.\n",
            "390/390 [==============================] - 119s 306ms/step - loss: 0.3083 - acc: 0.9501 - val_loss: 0.6830 - val_acc: 0.8639\n",
            "Epoch 44/50\n",
            "\n",
            "Epoch 00044: LearningRateScheduler setting learning rate to 0.0002038459.\n",
            "390/390 [==============================] - 119s 306ms/step - loss: 0.3045 - acc: 0.9500 - val_loss: 0.6459 - val_acc: 0.8735\n",
            "Epoch 45/50\n",
            "\n",
            "Epoch 00045: LearningRateScheduler setting learning rate to 0.0001995211.\n",
            "390/390 [==============================] - 119s 305ms/step - loss: 0.2993 - acc: 0.9532 - val_loss: 0.7066 - val_acc: 0.8535\n",
            "Epoch 46/50\n",
            "\n",
            "Epoch 00046: LearningRateScheduler setting learning rate to 0.0001953761.\n",
            "390/390 [==============================] - 119s 305ms/step - loss: 0.2952 - acc: 0.9540 - val_loss: 0.6516 - val_acc: 0.8727\n",
            "Epoch 47/50\n",
            "\n",
            "Epoch 00047: LearningRateScheduler setting learning rate to 0.0001913998.\n",
            "390/390 [==============================] - 119s 305ms/step - loss: 0.2943 - acc: 0.9536 - val_loss: 0.6411 - val_acc: 0.8707\n",
            "Epoch 48/50\n",
            "\n",
            "Epoch 00048: LearningRateScheduler setting learning rate to 0.0001875821.\n",
            "390/390 [==============================] - 120s 306ms/step - loss: 0.2857 - acc: 0.9554 - val_loss: 0.6505 - val_acc: 0.8737\n",
            "Epoch 49/50\n",
            "\n",
            "Epoch 00049: LearningRateScheduler setting learning rate to 0.0001839137.\n",
            "390/390 [==============================] - 119s 306ms/step - loss: 0.2810 - acc: 0.9583 - val_loss: 0.6567 - val_acc: 0.8767\n",
            "Epoch 50/50\n",
            "\n",
            "Epoch 00050: LearningRateScheduler setting learning rate to 0.000180386.\n",
            "390/390 [==============================] - 119s 306ms/step - loss: 0.2814 - acc: 0.9572 - val_loss: 0.6392 - val_acc: 0.8763\n",
            "Model took 5985.38 seconds to train\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3gAAAFNCAYAAABSRs15AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0khY4EmSJEnSSPwLVEObTW89r9gAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMJ8chctrnSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHPCmZsvEYcA",
        "colab_type": "text"
      },
      "source": [
        "## Repeating the above experiment with batch size =32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ag0N_ToEcB7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "62fbf883-a2b0-4932-c0dc-5e1d369ea3c3"
      },
      "source": [
        "def lr_schedule(epoch, lr):\n",
        "  return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "if version == 2:\n",
        "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
        "else:\n",
        "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "print(model_type)\n",
        "\n",
        "# Prepare model model saving directory.\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(zoom_range=0.0, \n",
        "                             horizontal_flip=False)\n",
        "\n",
        "\n",
        "# train the model\n",
        "start = time.time()\n",
        "# Train the model\n",
        "model_info = model.fit_generator(datagen.flow(x_train, y_train, batch_size = 32),\n",
        "                                 samples_per_epoch = x_train.shape[0], nb_epoch = 50, \n",
        "                                 callbacks=[LearningRateScheduler(lr_schedule, verbose=1)], #Added Rohan's Learning rate scheduler\n",
        "                                 validation_data = (x_test, y_test), verbose=1)\n",
        "\n",
        "                                            \n",
        "end = time.time()\n",
        "print (\"Model took %0.2f seconds to train\"%(end - start))\n",
        "# plot model history\n",
        "plot_model_history(model_info)\n",
        "# compute test accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_13\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_13 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_262 (Conv2D)             (None, 32, 32, 32)   896         input_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_229 (BatchN (None, 32, 32, 32)   128         conv2d_262[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_229 (Activation)     (None, 32, 32, 32)   0           batch_normalization_229[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_147 (Dropout)           (None, 32, 32, 32)   0           activation_229[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_263 (Conv2D)             (None, 32, 32, 32)   1056        dropout_147[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_148 (Dropout)           (None, 32, 32, 32)   0           conv2d_263[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_230 (BatchN (None, 32, 32, 32)   128         dropout_148[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_230 (Activation)     (None, 32, 32, 32)   0           batch_normalization_230[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_149 (Dropout)           (None, 32, 32, 32)   0           activation_230[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_264 (Conv2D)             (None, 32, 32, 32)   9248        dropout_149[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_150 (Dropout)           (None, 32, 32, 32)   0           conv2d_264[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_231 (BatchN (None, 32, 32, 32)   128         dropout_150[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_231 (Activation)     (None, 32, 32, 32)   0           batch_normalization_231[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_151 (Dropout)           (None, 32, 32, 32)   0           activation_231[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_265 (Conv2D)             (None, 32, 32, 128)  4224        dropout_151[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_266 (Conv2D)             (None, 32, 32, 128)  4224        dropout_147[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_152 (Dropout)           (None, 32, 32, 128)  0           conv2d_265[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_82 (Add)                    (None, 32, 32, 128)  0           conv2d_266[0][0]                 \n",
            "                                                                 dropout_152[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_232 (BatchN (None, 32, 32, 128)  512         add_82[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_232 (Activation)     (None, 32, 32, 128)  0           batch_normalization_232[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_153 (Dropout)           (None, 32, 32, 128)  0           activation_232[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_267 (Conv2D)             (None, 32, 32, 32)   4128        dropout_153[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_154 (Dropout)           (None, 32, 32, 32)   0           conv2d_267[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_233 (BatchN (None, 32, 32, 32)   128         dropout_154[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_233 (Activation)     (None, 32, 32, 32)   0           batch_normalization_233[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_155 (Dropout)           (None, 32, 32, 32)   0           activation_233[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_268 (Conv2D)             (None, 32, 32, 32)   9248        dropout_155[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_156 (Dropout)           (None, 32, 32, 32)   0           conv2d_268[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_234 (BatchN (None, 32, 32, 32)   128         dropout_156[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_234 (Activation)     (None, 32, 32, 32)   0           batch_normalization_234[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_157 (Dropout)           (None, 32, 32, 32)   0           activation_234[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_269 (Conv2D)             (None, 32, 32, 128)  4224        dropout_157[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_158 (Dropout)           (None, 32, 32, 128)  0           conv2d_269[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_83 (Add)                    (None, 32, 32, 128)  0           add_82[0][0]                     \n",
            "                                                                 dropout_158[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_235 (BatchN (None, 32, 32, 128)  512         add_83[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_235 (Activation)     (None, 32, 32, 128)  0           batch_normalization_235[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_159 (Dropout)           (None, 32, 32, 128)  0           activation_235[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_270 (Conv2D)             (None, 16, 16, 128)  16512       dropout_159[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_160 (Dropout)           (None, 16, 16, 128)  0           conv2d_270[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_236 (BatchN (None, 16, 16, 128)  512         dropout_160[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_236 (Activation)     (None, 16, 16, 128)  0           batch_normalization_236[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_161 (Dropout)           (None, 16, 16, 128)  0           activation_236[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_271 (Conv2D)             (None, 16, 16, 128)  147584      dropout_161[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_162 (Dropout)           (None, 16, 16, 128)  0           conv2d_271[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_237 (BatchN (None, 16, 16, 128)  512         dropout_162[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_237 (Activation)     (None, 16, 16, 128)  0           batch_normalization_237[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_163 (Dropout)           (None, 16, 16, 128)  0           activation_237[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_272 (Conv2D)             (None, 16, 16, 256)  33024       dropout_163[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_273 (Conv2D)             (None, 16, 16, 256)  33024       add_83[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_164 (Dropout)           (None, 16, 16, 256)  0           conv2d_272[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_84 (Add)                    (None, 16, 16, 256)  0           conv2d_273[0][0]                 \n",
            "                                                                 dropout_164[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_238 (BatchN (None, 16, 16, 256)  1024        add_84[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_238 (Activation)     (None, 16, 16, 256)  0           batch_normalization_238[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_165 (Dropout)           (None, 16, 16, 256)  0           activation_238[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_274 (Conv2D)             (None, 16, 16, 128)  32896       dropout_165[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_166 (Dropout)           (None, 16, 16, 128)  0           conv2d_274[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_239 (BatchN (None, 16, 16, 128)  512         dropout_166[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_239 (Activation)     (None, 16, 16, 128)  0           batch_normalization_239[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_167 (Dropout)           (None, 16, 16, 128)  0           activation_239[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_275 (Conv2D)             (None, 16, 16, 128)  147584      dropout_167[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_168 (Dropout)           (None, 16, 16, 128)  0           conv2d_275[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_240 (BatchN (None, 16, 16, 128)  512         dropout_168[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_240 (Activation)     (None, 16, 16, 128)  0           batch_normalization_240[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_169 (Dropout)           (None, 16, 16, 128)  0           activation_240[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_276 (Conv2D)             (None, 16, 16, 256)  33024       dropout_169[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_170 (Dropout)           (None, 16, 16, 256)  0           conv2d_276[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_85 (Add)                    (None, 16, 16, 256)  0           add_84[0][0]                     \n",
            "                                                                 dropout_170[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_241 (BatchN (None, 16, 16, 256)  1024        add_85[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_241 (Activation)     (None, 16, 16, 256)  0           batch_normalization_241[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_171 (Dropout)           (None, 16, 16, 256)  0           activation_241[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_277 (Conv2D)             (None, 8, 8, 256)    65792       dropout_171[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_172 (Dropout)           (None, 8, 8, 256)    0           conv2d_277[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_242 (BatchN (None, 8, 8, 256)    1024        dropout_172[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_242 (Activation)     (None, 8, 8, 256)    0           batch_normalization_242[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_173 (Dropout)           (None, 8, 8, 256)    0           activation_242[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_278 (Conv2D)             (None, 8, 8, 256)    590080      dropout_173[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_174 (Dropout)           (None, 8, 8, 256)    0           conv2d_278[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_243 (BatchN (None, 8, 8, 256)    1024        dropout_174[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_243 (Activation)     (None, 8, 8, 256)    0           batch_normalization_243[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_175 (Dropout)           (None, 8, 8, 256)    0           activation_243[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_279 (Conv2D)             (None, 8, 8, 512)    131584      dropout_175[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_280 (Conv2D)             (None, 8, 8, 512)    131584      add_85[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_176 (Dropout)           (None, 8, 8, 512)    0           conv2d_279[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_86 (Add)                    (None, 8, 8, 512)    0           conv2d_280[0][0]                 \n",
            "                                                                 dropout_176[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_244 (BatchN (None, 8, 8, 512)    2048        add_86[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_244 (Activation)     (None, 8, 8, 512)    0           batch_normalization_244[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_177 (Dropout)           (None, 8, 8, 512)    0           activation_244[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_281 (Conv2D)             (None, 8, 8, 256)    131328      dropout_177[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_178 (Dropout)           (None, 8, 8, 256)    0           conv2d_281[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_245 (BatchN (None, 8, 8, 256)    1024        dropout_178[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_245 (Activation)     (None, 8, 8, 256)    0           batch_normalization_245[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_179 (Dropout)           (None, 8, 8, 256)    0           activation_245[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_282 (Conv2D)             (None, 8, 8, 256)    590080      dropout_179[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_180 (Dropout)           (None, 8, 8, 256)    0           conv2d_282[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_246 (BatchN (None, 8, 8, 256)    1024        dropout_180[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_246 (Activation)     (None, 8, 8, 256)    0           batch_normalization_246[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_181 (Dropout)           (None, 8, 8, 256)    0           activation_246[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_283 (Conv2D)             (None, 8, 8, 512)    131584      dropout_181[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_182 (Dropout)           (None, 8, 8, 512)    0           conv2d_283[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_87 (Add)                    (None, 8, 8, 512)    0           add_86[0][0]                     \n",
            "                                                                 dropout_182[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_247 (BatchN (None, 8, 8, 512)    2048        add_87[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_247 (Activation)     (None, 8, 8, 512)    0           batch_normalization_247[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_13 (AveragePo (None, 1, 1, 512)    0           activation_247[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_13 (Flatten)            (None, 512)          0           average_pooling2d_13[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 10)           5130        flatten_13[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 2,272,010\n",
            "Trainable params: 2,265,034\n",
            "Non-trainable params: 6,976\n",
            "__________________________________________________________________________________________________\n",
            "ResNet20v1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., callbacks=[<keras.ca..., validation_data=(array([[[..., verbose=1, steps_per_epoch=1562, epochs=50)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 1.8140 - acc: 0.4660 - val_loss: 1.7710 - val_acc: 0.4418\n",
            "Epoch 2/50\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "1562/1562 [==============================] - 150s 96ms/step - loss: 1.3952 - acc: 0.5801 - val_loss: 1.5243 - val_acc: 0.5423\n",
            "Epoch 3/50\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "1562/1562 [==============================] - 149s 96ms/step - loss: 1.2541 - acc: 0.6245 - val_loss: 1.3189 - val_acc: 0.5989\n",
            "Epoch 4/50\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "1562/1562 [==============================] - 149s 96ms/step - loss: 1.1609 - acc: 0.6571 - val_loss: 1.3099 - val_acc: 0.6147\n",
            "Epoch 5/50\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "1562/1562 [==============================] - 149s 95ms/step - loss: 1.0879 - acc: 0.6832 - val_loss: 1.1284 - val_acc: 0.6660\n",
            "Epoch 6/50\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "1562/1562 [==============================] - 149s 95ms/step - loss: 1.0262 - acc: 0.7033 - val_loss: 1.0213 - val_acc: 0.7112\n",
            "Epoch 7/50\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "1562/1562 [==============================] - 149s 95ms/step - loss: 0.9706 - acc: 0.7219 - val_loss: 0.9619 - val_acc: 0.7271\n",
            "Epoch 8/50\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "1562/1562 [==============================] - 148s 95ms/step - loss: 0.9160 - acc: 0.7398 - val_loss: 0.9014 - val_acc: 0.7428\n",
            "Epoch 9/50\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "1562/1562 [==============================] - 148s 95ms/step - loss: 0.8727 - acc: 0.7586 - val_loss: 0.9273 - val_acc: 0.7404\n",
            "Epoch 10/50\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "1562/1562 [==============================] - 149s 95ms/step - loss: 0.8346 - acc: 0.7691 - val_loss: 0.9290 - val_acc: 0.7437\n",
            "Epoch 11/50\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "1562/1562 [==============================] - 149s 95ms/step - loss: 0.8068 - acc: 0.7807 - val_loss: 0.8498 - val_acc: 0.7684\n",
            "Epoch 12/50\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "1562/1562 [==============================] - 149s 95ms/step - loss: 0.7780 - acc: 0.7896 - val_loss: 1.1110 - val_acc: 0.6770\n",
            "Epoch 13/50\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            " 882/1562 [===============>..............] - ETA: 1:01 - loss: 0.7496 - acc: 0.7979"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNKcmHEwEevt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}