# EIP4 Assignment-4

## Completed and saved annotations (assignment 4a)

## Acheived 88% accuracy for ResNet38V2 with 32 filters and 4.45 million parameters in 48th epoch.
### Performed GRAD-CAM activation.
## Got 88% acuracy with batch size =32,64 and 128. Observed that batch size has regularizing effect. LOwer the batch size , higher the regularization. Found this paper (https://openreview.net/pdf?id=B1Yy1BxCZ) titled; "Don't Decay the Learning Rate, Increase the Batch Size"
### My savel model got disappeared after my colab got disconnected. I couldn't do GradCam




## Model
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 32, 32, 3)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 32, 32, 32)   896         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 32, 32, 32)   128         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 32, 32, 32)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 32, 32, 32)   0           activation_1[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 32, 32, 32)   1056        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 32, 32, 32)   128         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 32, 32, 32)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 32, 32, 32)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 32, 32, 32)   9248        dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 32, 32, 32)   128         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 32, 32, 32)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 32, 32, 32)   0           activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 32, 32, 128)  4224        dropout_1[0][0]                  
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 32, 32, 128)  4224        dropout_3[0][0]                  
__________________________________________________________________________________________________
add_1 (Add)                     (None, 32, 32, 128)  0           conv2d_5[0][0]                   
                                                                 conv2d_4[0][0]                   
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 32, 32, 128)  512         add_1[0][0]                      
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 32, 32, 128)  0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 32, 32, 128)  0           activation_4[0][0]               
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 32, 32)   4128        dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 32, 32, 32)   0           activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 32, 32, 32)   9248        dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 32, 32, 32)   128         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 32, 32, 32)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 32, 32, 32)   0           activation_6[0][0]               
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 32, 32, 128)  4224        dropout_6[0][0]                  
__________________________________________________________________________________________________
add_2 (Add)                     (None, 32, 32, 128)  0           add_1[0][0]                      
                                                                 conv2d_8[0][0]                   
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 32, 32, 128)  512         add_2[0][0]                      
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 32, 32, 128)  0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 32, 32, 128)  0           activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 32, 32, 32)   4128        dropout_7[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 32, 32, 32)   128         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 32, 32, 32)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 32, 32, 32)   0           activation_8[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 32, 32, 32)   9248        dropout_8[0][0]                  
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 32, 32, 32)   128         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 32, 32, 32)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 32, 32, 32)   0           activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 32, 32, 128)  4224        dropout_9[0][0]                  
__________________________________________________________________________________________________
add_3 (Add)                     (None, 32, 32, 128)  0           add_2[0][0]                      
                                                                 conv2d_11[0][0]                  
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 32, 32, 128)  512         add_3[0][0]                      
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 32, 32, 128)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 32, 32, 128)  0           activation_10[0][0]              
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 32, 32, 32)   4128        dropout_10[0][0]                 
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 32, 32, 32)   128         conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 32, 32, 32)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 32, 32, 32)   0           activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 32, 32, 32)   9248        dropout_11[0][0]                 
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 32, 32, 32)   128         conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 32, 32, 32)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 32, 32, 32)   0           activation_12[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 32, 32, 128)  4224        dropout_12[0][0]                 
__________________________________________________________________________________________________
add_4 (Add)                     (None, 32, 32, 128)  0           add_3[0][0]                      
                                                                 conv2d_14[0][0]                  
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 32, 32, 128)  512         add_4[0][0]                      
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 32, 32, 128)  0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_13 (Dropout)            (None, 32, 32, 128)  0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 16, 16, 128)  16512       dropout_13[0][0]                 
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 16, 16, 128)  512         conv2d_15[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 16, 16, 128)  0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
dropout_14 (Dropout)            (None, 16, 16, 128)  0           activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 16, 16, 128)  147584      dropout_14[0][0]                 
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 16, 16, 128)  512         conv2d_16[0][0]                  
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 16, 16, 128)  0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
dropout_15 (Dropout)            (None, 16, 16, 128)  0           activation_15[0][0]              
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 16, 16, 256)  33024       add_4[0][0]                      
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 16, 16, 256)  33024       dropout_15[0][0]                 
__________________________________________________________________________________________________
add_5 (Add)                     (None, 16, 16, 256)  0           conv2d_18[0][0]                  
                                                                 conv2d_17[0][0]                  
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 16, 16, 256)  1024        add_5[0][0]                      
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 16, 16, 256)  0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
dropout_16 (Dropout)            (None, 16, 16, 256)  0           activation_16[0][0]              
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 16, 16, 128)  32896       dropout_16[0][0]                 
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 16, 16, 128)  512         conv2d_19[0][0]                  
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 16, 16, 128)  0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
dropout_17 (Dropout)            (None, 16, 16, 128)  0           activation_17[0][0]              
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 16, 16, 128)  147584      dropout_17[0][0]                 
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 16, 16, 128)  512         conv2d_20[0][0]                  
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 16, 16, 128)  0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
dropout_18 (Dropout)            (None, 16, 16, 128)  0           activation_18[0][0]              
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 16, 16, 256)  33024       dropout_18[0][0]                 
__________________________________________________________________________________________________
add_6 (Add)                     (None, 16, 16, 256)  0           add_5[0][0]                      
                                                                 conv2d_21[0][0]                  
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 16, 16, 256)  1024        add_6[0][0]                      
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 16, 16, 256)  0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
dropout_19 (Dropout)            (None, 16, 16, 256)  0           activation_19[0][0]              
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 16, 16, 128)  32896       dropout_19[0][0]                 
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 16, 16, 128)  512         conv2d_22[0][0]                  
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 16, 16, 128)  0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
dropout_20 (Dropout)            (None, 16, 16, 128)  0           activation_20[0][0]              
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 16, 16, 128)  147584      dropout_20[0][0]                 
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 16, 16, 128)  512         conv2d_23[0][0]                  
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 16, 16, 128)  0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
dropout_21 (Dropout)            (None, 16, 16, 128)  0           activation_21[0][0]              
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 16, 16, 256)  33024       dropout_21[0][0]                 
__________________________________________________________________________________________________
add_7 (Add)                     (None, 16, 16, 256)  0           add_6[0][0]                      
                                                                 conv2d_24[0][0]                  
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 16, 16, 256)  1024        add_7[0][0]                      
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 16, 16, 256)  0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
dropout_22 (Dropout)            (None, 16, 16, 256)  0           activation_22[0][0]              
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 16, 16, 128)  32896       dropout_22[0][0]                 
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 16, 16, 128)  512         conv2d_25[0][0]                  
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 16, 16, 128)  0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
dropout_23 (Dropout)            (None, 16, 16, 128)  0           activation_23[0][0]              
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 16, 16, 128)  147584      dropout_23[0][0]                 
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 16, 16, 128)  512         conv2d_26[0][0]                  
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 16, 16, 128)  0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
dropout_24 (Dropout)            (None, 16, 16, 128)  0           activation_24[0][0]              
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 16, 16, 256)  33024       dropout_24[0][0]                 
__________________________________________________________________________________________________
add_8 (Add)                     (None, 16, 16, 256)  0           add_7[0][0]                      
                                                                 conv2d_27[0][0]                  
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 16, 16, 256)  1024        add_8[0][0]                      
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 16, 16, 256)  0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
dropout_25 (Dropout)            (None, 16, 16, 256)  0           activation_25[0][0]              
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 8, 8, 256)    65792       dropout_25[0][0]                 
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 8, 8, 256)    1024        conv2d_28[0][0]                  
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 8, 8, 256)    0           batch_normalization_26[0][0]     
__________________________________________________________________________________________________
dropout_26 (Dropout)            (None, 8, 8, 256)    0           activation_26[0][0]              
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 8, 8, 256)    590080      dropout_26[0][0]                 
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 8, 8, 256)    1024        conv2d_29[0][0]                  
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 8, 8, 256)    0           batch_normalization_27[0][0]     
__________________________________________________________________________________________________
dropout_27 (Dropout)            (None, 8, 8, 256)    0           activation_27[0][0]              
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 8, 8, 512)    131584      add_8[0][0]                      
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 8, 8, 512)    131584      dropout_27[0][0]                 
__________________________________________________________________________________________________
add_9 (Add)                     (None, 8, 8, 512)    0           conv2d_31[0][0]                  
                                                                 conv2d_30[0][0]                  
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 8, 8, 512)    2048        add_9[0][0]                      
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 8, 8, 512)    0           batch_normalization_28[0][0]     
__________________________________________________________________________________________________
dropout_28 (Dropout)            (None, 8, 8, 512)    0           activation_28[0][0]              
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 8, 8, 256)    131328      dropout_28[0][0]                 
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 8, 8, 256)    1024        conv2d_32[0][0]                  
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 8, 8, 256)    0           batch_normalization_29[0][0]     
__________________________________________________________________________________________________
dropout_29 (Dropout)            (None, 8, 8, 256)    0           activation_29[0][0]              
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 8, 8, 256)    590080      dropout_29[0][0]                 
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 8, 8, 256)    1024        conv2d_33[0][0]                  
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 8, 8, 256)    0           batch_normalization_30[0][0]     
__________________________________________________________________________________________________
dropout_30 (Dropout)            (None, 8, 8, 256)    0           activation_30[0][0]              
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 8, 8, 512)    131584      dropout_30[0][0]                 
__________________________________________________________________________________________________
add_10 (Add)                    (None, 8, 8, 512)    0           add_9[0][0]                      
                                                                 conv2d_34[0][0]                  
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 8, 8, 512)    2048        add_10[0][0]                     
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 8, 8, 512)    0           batch_normalization_31[0][0]     
__________________________________________________________________________________________________
dropout_31 (Dropout)            (None, 8, 8, 512)    0           activation_31[0][0]              
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 8, 8, 256)    131328      dropout_31[0][0]                 
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 8, 8, 256)    1024        conv2d_35[0][0]                  
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 8, 8, 256)    0           batch_normalization_32[0][0]     
__________________________________________________________________________________________________
dropout_32 (Dropout)            (None, 8, 8, 256)    0           activation_32[0][0]              
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 8, 8, 256)    590080      dropout_32[0][0]                 
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 8, 8, 256)    1024        conv2d_36[0][0]                  
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 8, 8, 256)    0           batch_normalization_33[0][0]     
__________________________________________________________________________________________________
dropout_33 (Dropout)            (None, 8, 8, 256)    0           activation_33[0][0]              
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 8, 8, 512)    131584      dropout_33[0][0]                 
__________________________________________________________________________________________________
add_11 (Add)                    (None, 8, 8, 512)    0           add_10[0][0]                     
                                                                 conv2d_37[0][0]                  
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 8, 8, 512)    2048        add_11[0][0]                     
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 8, 8, 512)    0           batch_normalization_34[0][0]     
__________________________________________________________________________________________________
dropout_34 (Dropout)            (None, 8, 8, 512)    0           activation_34[0][0]              
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 8, 8, 256)    131328      dropout_34[0][0]                 
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 8, 8, 256)    1024        conv2d_38[0][0]                  
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 8, 8, 256)    0           batch_normalization_35[0][0]     
__________________________________________________________________________________________________
dropout_35 (Dropout)            (None, 8, 8, 256)    0           activation_35[0][0]              
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 8, 8, 256)    590080      dropout_35[0][0]                 
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 8, 8, 256)    1024        conv2d_39[0][0]                  
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 8, 8, 256)    0           batch_normalization_36[0][0]     
__________________________________________________________________________________________________
dropout_36 (Dropout)            (None, 8, 8, 256)    0           activation_36[0][0]              
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 8, 8, 512)    131584      dropout_36[0][0]                 
__________________________________________________________________________________________________
add_12 (Add)                    (None, 8, 8, 512)    0           add_11[0][0]                     
                                                                 conv2d_40[0][0]                  
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 8, 8, 512)    2048        add_12[0][0]                     
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 8, 8, 512)    0           batch_normalization_37[0][0]     
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 1, 1, 512)    0           activation_37[0][0]              
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 512)          0           average_pooling2d_1[0][0]        
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 10)           5130        flatten_1[0][0]                  
==================================================================================================
Total params: 4,454,026
Trainable params: 4,440,138
Non-trainable params: 13,888
I have made two blocks of convolutins
In each block there are two convolution layers. 2 3X3 conv layers and one 1x1 conv layer followed by max pooling.
But I didn't incorporate learning rate scheduler.
May be if I implemnt that I may get 99.4 test accuracy.


 ## Logs
 Epoch 1/50

Epoch 00001: LearningRateScheduler setting learning rate to 0.003.
390/390 [==============================] - 186s 478ms/step - loss: 2.0623 - acc: 0.4838 - val_loss: 2.0728 - val_acc: 0.4696
Epoch 2/50

Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.
390/390 [==============================] - 172s 440ms/step - loss: 1.3324 - acc: 0.6399 - val_loss: 2.0003 - val_acc: 0.4715
Epoch 3/50

Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.
390/390 [==============================] - 171s 439ms/step - loss: 1.1588 - acc: 0.6883 - val_loss: 1.4409 - val_acc: 0.6026
Epoch 4/50

Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.
390/390 [==============================] - 171s 440ms/step - loss: 1.0373 - acc: 0.7254 - val_loss: 1.2421 - val_acc: 0.6563
Epoch 5/50

Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.
390/390 [==============================] - 171s 440ms/step - loss: 0.9554 - acc: 0.7529 - val_loss: 1.8268 - val_acc: 0.5173
Epoch 6/50

Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.
390/390 [==============================] - 171s 439ms/step - loss: 0.8747 - acc: 0.7808 - val_loss: 1.1624 - val_acc: 0.6898
Epoch 7/50

Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.
390/390 [==============================] - 172s 440ms/step - loss: 0.8168 - acc: 0.7977 - val_loss: 0.9391 - val_acc: 0.7659
Epoch 8/50

Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.
390/390 [==============================] - 172s 440ms/step - loss: 0.7636 - acc: 0.8130 - val_loss: 0.8189 - val_acc: 0.7958
Epoch 9/50

Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.
390/390 [==============================] - 172s 440ms/step - loss: 0.7227 - acc: 0.8254 - val_loss: 0.8419 - val_acc: 0.7871
Epoch 10/50

Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.
390/390 [==============================] - 172s 440ms/step - loss: 0.6831 - acc: 0.8395 - val_loss: 0.8959 - val_acc: 0.7804
Epoch 11/50

Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.
390/390 [==============================] - 172s 440ms/step - loss: 0.6476 - acc: 0.8482 - val_loss: 0.8742 - val_acc: 0.7752
Epoch 12/50

Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.
390/390 [==============================] - 172s 440ms/step - loss: 0.6181 - acc: 0.8578 - val_loss: 0.9813 - val_acc: 0.7586
Epoch 13/50

Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.
390/390 [==============================] - 172s 440ms/step - loss: 0.5873 - acc: 0.8661 - val_loss: 0.7933 - val_acc: 0.8077
Epoch 14/50

Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.
390/390 [==============================] - 171s 440ms/step - loss: 0.5548 - acc: 0.8783 - val_loss: 0.7077 - val_acc: 0.8288
Epoch 15/50

Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.
390/390 [==============================] - 172s 440ms/step - loss: 0.5349 - acc: 0.8840 - val_loss: 0.7103 - val_acc: 0.8302
Epoch 16/50

Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.
390/390 [==============================] - 172s 440ms/step - loss: 0.5153 - acc: 0.8907 - val_loss: 0.8331 - val_acc: 0.7919
Epoch 17/50

Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.
390/390 [==============================] - 172s 440ms/step - loss: 0.4887 - acc: 0.8983 - val_loss: 0.7828 - val_acc: 0.8175
Epoch 18/50

Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.
390/390 [==============================] - 172s 440ms/step - loss: 0.4692 - acc: 0.9045 - val_loss: 0.6765 - val_acc: 0.8443
Epoch 19/50

Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.
390/390 [==============================] - 172s 441ms/step - loss: 0.4509 - acc: 0.9100 - val_loss: 0.7073 - val_acc: 0.8427
Epoch 20/50

Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.
390/390 [==============================] - 172s 440ms/step - loss: 0.4376 - acc: 0.9135 - val_loss: 0.6701 - val_acc: 0.8434
Epoch 21/50

Epoch 00021: LearningRateScheduler setting learning rate to 0.0004065041.
390/390 [==============================] - 172s 440ms/step - loss: 0.4149 - acc: 0.9217 - val_loss: 0.6444 - val_acc: 0.8578
Epoch 22/50

Epoch 00022: LearningRateScheduler setting learning rate to 0.000389661.
390/390 [==============================] - 172s 440ms/step - loss: 0.3980 - acc: 0.9265 - val_loss: 0.6892 - val_acc: 0.8449
Epoch 23/50

Epoch 00023: LearningRateScheduler setting learning rate to 0.0003741581.
390/390 [==============================] - 172s 440ms/step - loss: 0.3858 - acc: 0.9310 - val_loss: 0.6714 - val_acc: 0.8534
Epoch 24/50

Epoch 00024: LearningRateScheduler setting learning rate to 0.0003598417.
390/390 [==============================] - 172s 440ms/step - loss: 0.3756 - acc: 0.9334 - val_loss: 0.6136 - val_acc: 0.8642
Epoch 25/50

Epoch 00025: LearningRateScheduler setting learning rate to 0.0003465804.
390/390 [==============================] - 172s 440ms/step - loss: 0.3634 - acc: 0.9369 - val_loss: 0.6106 - val_acc: 0.8734
Epoch 26/50

Epoch 00026: LearningRateScheduler setting learning rate to 0.0003342618.
390/390 [==============================] - 172s 440ms/step - loss: 0.3534 - acc: 0.9403 - val_loss: 0.6696 - val_acc: 0.8605
Epoch 27/50

Epoch 00027: LearningRateScheduler setting learning rate to 0.0003227889.
390/390 [==============================] - 172s 440ms/step - loss: 0.3443 - acc: 0.9424 - val_loss: 0.6670 - val_acc: 0.8643
Epoch 28/50

Epoch 00028: LearningRateScheduler setting learning rate to 0.0003120774.
390/390 [==============================] - 172s 440ms/step - loss: 0.3289 - acc: 0.9472 - val_loss: 0.6801 - val_acc: 0.8591
Epoch 29/50

Epoch 00029: LearningRateScheduler setting learning rate to 0.000302054.
390/390 [==============================] - 172s 440ms/step - loss: 0.3197 - acc: 0.9504 - val_loss: 0.6649 - val_acc: 0.8646
Epoch 30/50

Epoch 00030: LearningRateScheduler setting learning rate to 0.0002926544.
390/390 [==============================] - 172s 440ms/step - loss: 0.3107 - acc: 0.9538 - val_loss: 0.7022 - val_acc: 0.8582
Epoch 31/50

Epoch 00031: LearningRateScheduler setting learning rate to 0.0002838221.
390/390 [==============================] - 172s 441ms/step - loss: 0.3032 - acc: 0.9561 - val_loss: 0.6556 - val_acc: 0.8656
Epoch 32/50

Epoch 00032: LearningRateScheduler setting learning rate to 0.0002755074.
390/390 [==============================] - 172s 440ms/step - loss: 0.2956 - acc: 0.9574 - val_loss: 0.7817 - val_acc: 0.8466
Epoch 33/50

Epoch 00033: LearningRateScheduler setting learning rate to 0.000267666.
390/390 [==============================] - 172s 440ms/step - loss: 0.2917 - acc: 0.9591 - val_loss: 0.8164 - val_acc: 0.8312
Epoch 34/50

Epoch 00034: LearningRateScheduler setting learning rate to 0.0002602585.
390/390 [==============================] - 172s 440ms/step - loss: 0.2855 - acc: 0.9599 - val_loss: 0.6886 - val_acc: 0.8653
Epoch 35/50

Epoch 00035: LearningRateScheduler setting learning rate to 0.00025325.
390/390 [==============================] - 172s 440ms/step - loss: 0.2797 - acc: 0.9618 - val_loss: 0.6556 - val_acc: 0.8673
Epoch 36/50

Epoch 00036: LearningRateScheduler setting learning rate to 0.0002466091.
390/390 [==============================] - 172s 441ms/step - loss: 0.2732 - acc: 0.9642 - val_loss: 0.6526 - val_acc: 0.8744
Epoch 37/50

Epoch 00037: LearningRateScheduler setting learning rate to 0.0002403076.
390/390 [==============================] - 172s 440ms/step - loss: 0.2658 - acc: 0.9672 - val_loss: 0.7056 - val_acc: 0.8573
Epoch 38/50

Epoch 00038: LearningRateScheduler setting learning rate to 0.0002343201.
390/390 [==============================] - 172s 440ms/step - loss: 0.2602 - acc: 0.9680 - val_loss: 0.6603 - val_acc: 0.8714
Epoch 39/50

Epoch 00039: LearningRateScheduler setting learning rate to 0.0002286237.
390/390 [==============================] - 172s 440ms/step - loss: 0.2575 - acc: 0.9685 - val_loss: 0.6532 - val_acc: 0.8763
Epoch 40/50

Epoch 00040: LearningRateScheduler setting learning rate to 0.0002231977.
390/390 [==============================] - 172s 440ms/step - loss: 0.2479 - acc: 0.9711 - val_loss: 0.6377 - val_acc: 0.8781
Epoch 41/50

Epoch 00041: LearningRateScheduler setting learning rate to 0.0002180233.
390/390 [==============================] - 172s 440ms/step - loss: 0.2466 - acc: 0.9710 - val_loss: 0.6800 - val_acc: 0.8704
Epoch 42/50

Epoch 00042: LearningRateScheduler setting learning rate to 0.0002130833.
390/390 [==============================] - 172s 440ms/step - loss: 0.2433 - acc: 0.9716 - val_loss: 0.6873 - val_acc: 0.8644
Epoch 43/50

Epoch 00043: LearningRateScheduler setting learning rate to 0.0002083623.
390/390 [==============================] - 172s 440ms/step - loss: 0.2368 - acc: 0.9740 - val_loss: 0.6629 - val_acc: 0.8720
Epoch 44/50

Epoch 00044: LearningRateScheduler setting learning rate to 0.0002038459.
390/390 [==============================] - 172s 441ms/step - loss: 0.2311 - acc: 0.9761 - val_loss: 0.6527 - val_acc: 0.8728
Epoch 45/50

Epoch 00045: LearningRateScheduler setting learning rate to 0.0001995211.
390/390 [==============================] - 172s 440ms/step - loss: 0.2330 - acc: 0.9743 - val_loss: 0.7218 - val_acc: 0.8686
Epoch 46/50

Epoch 00046: LearningRateScheduler setting learning rate to 0.0001953761.
390/390 [==============================] - 172s 440ms/step - loss: 0.2301 - acc: 0.9755 - val_loss: 0.6681 - val_acc: 0.8709
Epoch 47/50

Epoch 00047: LearningRateScheduler setting learning rate to 0.0001913998.
390/390 [==============================] - 172s 440ms/step - loss: 0.2217 - acc: 0.9785 - val_loss: 0.6989 - val_acc: 0.8730
Epoch 48/50

Epoch 00048: LearningRateScheduler setting learning rate to 0.0001875821.
390/390 [==============================] - 172s 441ms/step - loss: 0.2217 - acc: 0.9773 - val_loss: 0.6273 - val_acc: 0.8816
Epoch 49/50

Epoch 00049: LearningRateScheduler setting learning rate to 0.0001839137.
390/390 [==============================] - 172s 440ms/step - loss: 0.2218 - acc: 0.9773 - val_loss: 0.6662 - val_acc: 0.8759
Epoch 50/50

Epoch 00050: LearningRateScheduler setting learning rate to 0.000180386.
390/390 [==============================] - 172s 441ms/step - loss: 0.2140 - acc: 0.9800 - val_loss: 0.6876 - val_acc: 0.8687
Model took 8604.61 seconds to train


